I"º<p><strong>Trong trang n√†y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Gi·ªõi thi·ªáu</a>
    <ul>
      <li><a href="#nhac-lai-hai-mo-hinh-tuyen-tinh">Nh·∫Øc l·∫°i hai m√¥ h√¨nh tuy·∫øn t√≠nh</a></li>
      <li><a href="#mot-vi-du-nho">M·ªôt v√≠ d·ª• nh·ªè</a></li>
      <li><a href="#mo-hinh-logistic-regression">M√¥ h√¨nh Logistic Regression</a></li>
      <li><a href="#sigmoid-function">Sigmoid function</a></li>
    </ul>
  </li>
  <li><a href="#-ham-mat-mat-va-phuong-phap-toi-uu">2. H√†m m·∫•t m√°t v√† ph∆∞∆°ng ph√°p t·ªëi ∆∞u</a>
    <ul>
      <li><a href="#xay-dung-ham-mat-mat">X√¢y d·ª±ng h√†m m·∫•t m√°t</a></li>
      <li><a href="#toi-uu-ham-mat-mat">T·ªëi ∆∞u h√†m m·∫•t m√°t</a></li>
      <li><a href="#cong-thuc-cap-nhat-cho-logistic-sigmoid-regression">C√¥ng th·ª©c c·∫≠p nh·∫≠t cho logistic sigmoid regression</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-voi-python">3. V√≠ d·ª• v·ªõi Python</a>
    <ul>
      <li><a href="#vi-du-voi-du-lieu--chieu">V√≠ d·ª• v·ªõi d·ªØ li·ªáu 1 chi·ªÅu</a></li>
      <li><a href="#cac-ham-can-thiet-cho-logistic-sigmoid-regression">C√°c h√†m c·∫ßn thi·∫øt cho logistic sigmoid regression</a></li>
      <li><a href="#vi-du-voi-du-lieu--chieu-1">V√≠ d·ª• v·ªõi d·ªØ li·ªáu 2 chi·ªÅu</a></li>
    </ul>
  </li>
  <li><a href="#-mot-vai-tinh-chat-cua-logistic-regression">4. M·ªôt v√†i t√≠nh ch·∫•t c·ªßa Logistic Regression</a>
    <ul>
      <li><a href="#logistic-regression-thuc-ra-duoc-su-dung-nhieu-trong-cac-bai-toan-classification">Logistic Regression th·ª±c ra ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu trong c√°c b√†i to√°n Classification.</a></li>
      <li><a href="#boundary-tao-boi-logistic-regression-co-dang-tuyen-tinh">Boundary t·∫°o b·ªüi Logistic Regression c√≥ d·∫°ng tuy·∫øn t√≠nh</a></li>
    </ul>
  </li>
  <li><a href="#-thao-luan">5. Th·∫£o lu·∫≠n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. T√†i li·ªáu tham kh·∫£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>

<p><a name="nhac-lai-hai-mo-hinh-tuyen-tinh"></a></p>

<h3 id="nh·∫Øc-l·∫°i-hai-m√¥-h√¨nh-tuy·∫øn-t√≠nh">Nh·∫Øc l·∫°i hai m√¥ h√¨nh tuy·∫øn t√≠nh</h3>
<p>Hai m√¥ h√¨nh tuy·∫øn t√≠nh (linear models) <a href="/2016/12/28/linearregression/">Linear Regression</a> v√† <a href="/2017/01/21/perceptron/">Perceptron Learning Algorithm</a> (PLA) ch√∫ng ta ƒë√£ bi·∫øt ƒë·ªÅu c√≥ chung m·ªôt d·∫°ng:
\[
y = f(\mathbf{w}^T\mathbf{x})
\]</p>

<p>trong ƒë√≥ \(f()\) ƒë∆∞·ª£c g·ªçi l√† <em>activation function</em>, v√† \(\mathbf{x}\) ƒë∆∞·ª£c hi·ªÉu l√† d·ªØ li·ªáu m·ªü r·ªông v·ªõi \(x_0 = 1\) ƒë∆∞·ª£c th√™m v√†o ƒë·ªÉ thu·∫≠n ti·ªán cho vi·ªác t√≠nh to√°n. V·ªõi linear regression th√¨ \(f(s) = s\), v·ªõi PLA th√¨ \(f(s) = \text{sgn}(s)\). Trong linear regression, t√≠ch v√¥ h∆∞·ªõng \(\mathbf{w}^T\mathbf{x}\) ƒë∆∞·ª£c tr·ª±c ti·∫øp s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n output \(y\), lo·∫°i n√†y ph√π h·ª£p n·∫øu ch√∫ng ta c·∫ßn d·ª± ƒëo√°n m·ªôt gi√° tr·ªã th·ª±c c·ªßa ƒë·∫ßu ra kh√¥ng b·ªã ch·∫∑n tr√™n v√† d∆∞·ªõi. Trong PLA, ƒë·∫ßu ra ch·ªâ nh·∫≠n m·ªôt trong hai gi√° tr·ªã \(1\) ho·∫∑c \(-1 \), ph√π h·ª£p v·ªõi c√°c b√†i to√°n <em>binary classification</em>.</p>

<p>Trong b√†i n√†y, t√¥i s·∫Ω gi·ªõi thi·ªáu m√¥ h√¨nh th·ª© ba v·ªõi m·ªôt activation kh√°c, ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c b√†i to√°n <em>flexible</em> h∆°n. Trong d·∫°ng n√†y, ƒë·∫ßu ra c√≥ th·ªÉ ƒë∆∞·ª£c th·ªÉ hi·ªán d∆∞·ªõi d·∫°ng x√°c su·∫•t (probability). V√≠ d·ª•: x√°c su·∫•t thi ƒë·ªó n·∫øu bi·∫øt th·ªùi gian √¥n thi, x√°c su·∫•t ng√†y mai c√≥ m∆∞a d·ª±a tr√™n nh·ªØng th√¥ng tin ƒëo ƒë∆∞·ª£c trong ng√†y h√¥m nay,‚Ä¶ M√¥ h√¨nh m·ªõi n√†y c·ªßa ch√∫ng ta c√≥ t√™n l√† <em>logistic regression</em>. M√¥ h√¨nh n√†y gi·ªëng v·ªõi linear regression ·ªü kh√≠a c·∫°nh ƒë·∫ßu ra l√† s·ªë th·ª±c, v√† gi·ªëng v·ªõi PLA ·ªü vi·ªác ƒë·∫ßu ra b·ªã ch·∫∑n (trong ƒëo·∫°n \([0, 1]\)). M·∫∑c d√π trong t√™n c√≥ ch·ª©a t·ª´ <em>regression</em>, logistic regression th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu h∆°n cho c√°c b√†i to√°n classification.</p>

<p><a name="mot-vi-du-nho"></a></p>

<h3 id="m·ªôt-v√≠-d·ª•-nh·ªè">M·ªôt v√≠ d·ª• nh·ªè</h3>
<p>T√¥i xin ƒë∆∞·ª£c s·ª≠ d·ª•ng <a href="https://en.wikipedia.org/wiki/Logistic_regression">m·ªôt v√≠ d·ª• tr√™n Wikipedia</a>:</p>

<blockquote>
  <p>M·ªôt nh√≥m 20 sinh vi√™n d√†nh th·ªùi gian trong kho·∫£ng t·ª´ 0 ƒë·∫øn 6 gi·ªù cho vi·ªác √¥n thi. Th·ªùi gian √¥n thi n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn x√°c su·∫•t sinh vi√™n v∆∞·ª£t qua k·ª≥ thi nh∆∞ th·∫ø n√†o?</p>
</blockquote>

<p>K·∫øt qu·∫£ thu ƒë∆∞·ª£c nh∆∞ sau:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Hours</th>
      <th style="text-align: center">Pass</th>
      <th style="text-align: center">Hours</th>
      <th style="text-align: center">Pass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">.5</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2.75</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">.75</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">3.25</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1.25</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">3.5</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">4.25</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">4.5</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">4.75</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">2.25</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">2.5</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">5.5</td>
      <td style="text-align: center">1</td>
    </tr>
  </tbody>
</table>

<p>M·∫∑c d√π c√≥ m·ªôt ch√∫t <em>b·∫•t c√¥ng</em> khi h·ªçc 3.5 gi·ªù th√¨ tr∆∞·ª£t, c√≤n h·ªçc 1.75 gi·ªù th√¨ l·∫°i ƒë·ªó, nh√¨n chung, h·ªçc c√†ng nhi·ªÅu th√¨ kh·∫£ nƒÉng ƒë·ªó c√†ng cao. PLA kh√¥ng th·ªÉ √°p d·ª•ng ƒë∆∞·ª£c cho b√†i to√°n n√†y v√¨ kh√¥ng th·ªÉ n√≥i m·ªôt ng∆∞·ªùi h·ªçc bao nhi√™u gi·ªù th√¨ 100% tr∆∞·ª£t hay ƒë·ªó, v√† th·ª±c t·∫ø l√† d·ªØ li·ªáu n√†y c≈©ng kh√¥ng <em>linearly separable</em> (ƒëi·ªáu ki·ªán ƒë·ªÉ PLA c√≥ th·ªÉ l√†m vi·ªác). Ch√∫ √Ω r·∫±ng c√°c ƒëi·ªÉm m√†u ƒë·ªè v√† xanh ƒë∆∞·ª£c v·∫Ω ·ªü hai tung ƒë·ªô kh√°c nhau ƒë·ªÉ ti·ªán cho vi·ªác minh h·ªça. C√°c ƒëi·ªÉm n√†y ƒë∆∞·ª£c v·∫Ω d√πng c·∫£ d·ªØ li·ªáu ƒë·∫ßu v√†o \(\mathbf{x}\) v√† ƒë·∫ßu ra \(y). Khi ta n√≥i <em>linearly seperable</em> l√† khi ta ch·ªâ d√πng d·ªØ li·ªáu ƒë·∫ßu v√†o \(\mathbf{x}\).</p>

<p>Ch√∫ng ta bi·ªÉu di·ªÖn c√°c ƒëi·ªÉm n√†y tr√™n ƒë·ªì th·ªã ƒë·ªÉ th·∫•y r√µ h∆°n:</p>

<div class="imgcap">
<img src="\assets\LogisticRegression\ex1.png" align="center" width="800" />
<div class="thecap">H√¨nh 1: V√≠ d·ª• v·ªÅ k·∫øt qu·∫£ thi d·ª±a tr√™n s·ªë gi·ªù √¥n t·∫≠p.</div>
</div>

<p>Nh·∫≠n th·∫•y r·∫±ng c·∫£ linear regression v√† PLA ƒë·ªÅu kh√¥ng ph√π h·ª£p v·ªõi b√†i to√°n n√†y, ch√∫ng ta c·∫ßn m·ªôt m√¥ h√¨nh <em>flexible</em> h∆°n.</p>

<p><a name="mo-hinh-logistic-regression"></a></p>

<h3 id="m√¥-h√¨nh-logistic-regression">M√¥ h√¨nh Logistic Regression</h3>
<p>ƒê·∫ßu ra d·ª± ƒëo√°n c·ªßa:</p>

<ul>
  <li>Linear Regression: 
\[
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}
\]</li>
  <li>PLA:
\[
f(\mathbf{x}) = \text{sgn}(\mathbf{w}^T\mathbf{x})
\]</li>
</ul>

<p>ƒê·∫ßu ra d·ª± ƒëo√°n c·ªßa logistic regression th∆∞·ªùng ƒë∆∞·ª£c vi·∫øt chung d∆∞·ªõi d·∫°ng:
\[
f(\mathbf{x}) = \theta(\mathbf{w}^T\mathbf{x})
\]</p>

<p>Trong ƒë√≥ \(\theta\) ƒë∆∞·ª£c g·ªçi l√† logistic function. M·ªôt s·ªë activation cho m√¥ h√¨nh tuy·∫øn t√≠nh ƒë∆∞·ª£c cho trong h√¨nh d∆∞·ªõi ƒë√¢y:</p>

<div class="imgcap">
<img src="\assets\LogisticRegression\activation.png" align="center" width="800" />
<div class="thecap">H√¨nh 2: C√°c activation function kh√°c nhau.</div>
</div>

<ul>
  <li>ƒê∆∞·ªùng m√†u v√†ng bi·ªÉu di·ªÖn linear regression. ƒê∆∞·ªùng n√†y kh√¥ng b·ªã ch·∫∑n n√™n kh√¥ng ph√π h·ª£p cho b√†i to√°n n√†y. C√≥ m·ªôt <em>trick</em> nh·ªè ƒë·ªÉ ƒë∆∞a n√≥ v·ªÅ d·∫°ng b·ªã ch·∫∑n: <em>c·∫Øt</em> ph·∫ßn nh·ªè h∆°n 0 b·∫±ng c√°ch cho ch√∫ng b·∫±ng 0, <em>c·∫Øt</em> c√°c ph·∫ßn l·ªõn h∆°n 1 b·∫±ng c√°ch cho ch√∫ng b·∫±ng 1. Sau ƒë√≥ l·∫•y ƒëi·ªÉm tr√™n ƒë∆∞·ªùng th·∫≥ng n√†y c√≥ tung ƒë·ªô b·∫±ng 0.5 l√†m ƒëi·ªÉm ph√¢n chia hai <em>class</em>, ƒë√¢y c≈©ng kh√¥ng ph·∫£i l√† m·ªôt l·ª±a ch·ªçn t·ªët. Gi·∫£ s·ª≠ c√≥ th√™m v√†i b·∫°n <em>sinh vi√™n ti√™u bi·ªÉu</em> √¥n t·∫≠p ƒë·∫øn 20 gi·ªù v√†, t·∫•t nhi√™n, thi ƒë·ªó. Khi √°p d·ª•ng m√¥ h√¨nh linear regression nh∆∞ h√¨nh d∆∞·ªõi ƒë√¢y v√† l·∫•y m·ªëc 0.5 ƒë·ªÉ ph√¢n l·ªõp, to√†n b·ªô sinh vi√™n thi tr∆∞·ª£t v·∫´n ƒë∆∞·ª£c d·ª± ƒëo√°n l√† tr∆∞·ª£t, nh∆∞ng r·∫•t nhi·ªÅu sinh vi√™n thi ƒë·ªó c≈©ng ƒë∆∞·ª£c d·ª± ƒëo√°n l√† tr∆∞·ª£t (n·∫øu ta coi ƒëi·ªÉm x m√†u xanh l·ª•c l√† <em>ng∆∞·ª°ng c·ª©ng</em> ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n). R√µ r√†ng ƒë√¢y l√† m·ªôt m√¥ h√¨nh kh√¥ng t·ªët. Anh ch√†ng sinh vi√™n ti√™u bi·ªÉu n√†y ƒë√£ <em>k√©o theo</em> r·∫•t nhi·ªÅu b·∫°n kh√°c b·ªã tr∆∞·ª£t.</li>
</ul>
<div class="imgcap">
<img src="\assets\LogisticRegression\ex1_lr.png" align="center" width="800" />
<div class="thecap">H√¨nh 3: T·∫°i sao Linear Regression kh√¥ng ph√π h·ª£p?</div>
</div>

<ul>
  <li>ƒê∆∞·ªùng m√†u ƒë·ªè (ch·ªâ kh√°c v·ªõi activation function c·ªßa PLA ·ªü ch·ªó  hai class l√† 0 v√† 1 thay v√¨ -1 v√† 1) c≈©ng thu·ªôc d·∫°ng <em>ng∆∞·ª°ng c·ª©ng</em> (hard threshold). PLA kh√¥ng ho·∫°t ƒë·ªông trong b√†i to√°n n√†y v√¨ d·ªØ li·ªáu ƒë√£ cho kh√¥ng <em>linearly separable</em>.</li>
  <li>
    <p>C√°c ƒë∆∞·ªùng m√†u xanh lam v√† xanh l·ª•c ph√π h·ª£p v·ªõi b√†i to√°n c·ªßa ch√∫ng ta h∆°n. Ch√∫ng c√≥ m·ªôt v√†i t√≠nh ch·∫•t quan tr·ªçng sau:</p>

    <ul>
      <li>L√† h√†m s·ªë li√™n t·ª•c nh·∫≠n gi√° tr·ªã th·ª±c, b·ªã ch·∫∑n trong kho·∫£ng \((0, 1)\).</li>
      <li>N·∫øu coi ƒëi·ªÉm c√≥ tung ƒë·ªô l√† 1/2 l√†m ƒëi·ªÉm ph√¢n chia th√¨ c√°c ƒëi·ªÉm c√†ng xa ƒëi·ªÉm n√†y v·ªÅ ph√≠a b√™n tr√°i c√≥ gi√° tr·ªã c√†ng g·∫ßn 0. Ng∆∞·ª£c l·∫°i, c√°c ƒëi·ªÉm c√†ng xa ƒëi·ªÉm n√†y v·ªÅ ph√≠a ph·∫£i c√≥ gi√° tr·ªã c√†ng g·∫ßn 1. ƒêi·ªÅu n√†y <em>kh·ªõp</em> v·ªõi nh·∫≠n x√©t r·∫±ng h·ªçc c√†ng nhi·ªÅu th√¨ x√°c su·∫•t ƒë·ªó c√†ng cao v√† ng∆∞·ª£c l·∫°i.</li>
      <li><em>M∆∞·ª£t</em> (smooth) n√™n c√≥ ƒë·∫°o h√†m m·ªçi n∆°i, c√≥ th·ªÉ ƒë∆∞·ª£c l·ª£i trong vi·ªác t·ªëi ∆∞u.</li>
    </ul>
  </li>
</ul>

<p><a name="sigmoid-function"></a></p>

<h3 id="sigmoid-function">Sigmoid function</h3>

<p>Trong s·ªë c√°c h√†m s·ªë c√≥ 3 t√≠nh ch·∫•t n√≥i tr√™n th√¨ h√†m <em>sigmoid</em>:
\[
f(s) = \frac{1}{1 + e^{-s}} \triangleq \sigma(s)
\]
ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t, v√¨ n√≥ b·ªã ch·∫∑n trong kho·∫£ng \((0, 1)\). Th√™m n·ªØa:
\[
\lim_{s \rightarrow -\infty}\sigma(s) = 0; ~~ \lim_{s \rightarrow +\infty}\sigma(s) = 1 
\]
ƒê·∫∑c bi·ªát h∆°n n·ªØa:
\[
\begin{eqnarray}
\sigma‚Äô(s) &amp;=&amp; \frac{e^{-s}}{(1 + e^{-s})^2} \newline
&amp;=&amp; \frac{1}{1 + e^{-s}} \frac{e^{-s}}{1 + e^{-s}} \newline
&amp;=&amp; \sigma(s)(1 - \sigma(s))
\end{eqnarray}
\]
C√¥ng th·ª©c ƒë·∫°o h√†m ƒë∆°n gi·∫£n th·∫ø n√†y gi√∫p h√†m s·ªë n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i. ·ªû ph·∫ßn sau, t√¥i s·∫Ω l√Ω gi·∫£i vi·ªác <em>ng∆∞·ªùi ta ƒë√£ t√¨m ra h√†m s·ªë ƒë·∫∑c bi·ªát n√†y nh∆∞ th·∫ø n√†o</em>.</p>

<p><a name="tanh-function"></a></p>

<p>Ngo√†i ra, h√†m <em>tanh</em> c≈©ng hay ƒë∆∞·ª£c s·ª≠ d·ª•ng: 
\[
\text{tanh}(s) = \frac{e^{s} - e^{-s}}{e^s + e^{-s}}
\]</p>

<p>H√†m s·ªë n√†y nh·∫≠n gi√° tr·ªã trong kho·∫£ng \((-1, 1)\) nh∆∞ng c√≥ th·ªÉ d·ªÖ d√†ng ƒë∆∞a n√≥ v·ªÅ kho·∫£ng \((0, 1)\). B·∫°n ƒë·ªçc c√≥ th·ªÉ ch·ª©ng minh ƒë∆∞·ª£c:
\[
\text{tanh}(s) = 2\sigma(2s) - 1
\]</p>

<p><a name="-ham-mat-mat-va-phuong-phap-toi-uu"></a></p>

<h2 id="2-h√†m-m·∫•t-m√°t-v√†-ph∆∞∆°ng-ph√°p-t·ªëi-∆∞u">2. H√†m m·∫•t m√°t v√† ph∆∞∆°ng ph√°p t·ªëi ∆∞u</h2>

<p><a name="xay-dung-ham-mat-mat"></a></p>

<h3 id="x√¢y-d·ª±ng-h√†m-m·∫•t-m√°t">X√¢y d·ª±ng h√†m m·∫•t m√°t</h3>

<p>V·ªõi m√¥ h√¨nh nh∆∞ tr√™n (c√°c activation m√†u xanh lam v√† l·ª•c), ta c√≥ th·ªÉ gi·∫£ s·ª≠ r·∫±ng x√°c su·∫•t ƒë·ªÉ m·ªôt ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}\) r∆°i v√†o class 1 l√† \(f(\mathbf{w}^T\mathbf{x})\) v√† r∆°i v√†o class 0 l√† \(1 - f(\mathbf{w}^T\mathbf{x})\). V·ªõi m√¥ h√¨nh ƒë∆∞·ª£c gi·∫£ s·ª≠ nh∆∞ v·∫≠y, v·ªõi c√°c ƒëi·ªÉm d·ªØ li·ªáu training (ƒë√£ bi·∫øt ƒë·∫ßu ra \(y\)), ta c√≥ th·ªÉ vi·∫øt nh∆∞ sau:</p>

<p>\[
\begin{eqnarray}
P(y_i = 1 | \mathbf{x}_i; \mathbf{w}) &amp;=&amp; &amp;f(\mathbf{w}^T\mathbf{x}_i)  ~~(1) \newline
P(y_i = 0 | \mathbf{x}_i; \mathbf{w}) &amp;=&amp; 1 - &amp;f(\mathbf{w}^T\mathbf{x}_i)  ~~(2) \newline
\end{eqnarray}
\]
trong ƒë√≥ \( P(y_i = 1 | \mathbf{x}_i; \mathbf{w})\) ƒë∆∞·ª£c hi·ªÉu l√† x√°c su·∫•t x·∫£y ra s·ª± ki·ªán ƒë·∫ßu ra \(y_i = 1\) khi bi·∫øt tham s·ªë m√¥ h√¨nh \(\mathbf{w}\) v√† d·ªØ li·ªáu ƒë·∫ßu v√†o \(\mathbf{x}_i\). B·∫°n ƒë·ªçc c√≥ th·ªÉ ƒë·ªçc th√™m <a href="https://vi.wikipedia.org/wiki/X√°c_su·∫•t_c√≥_ƒëi·ªÅu_ki·ªán">X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán</a>. M·ª•c ƒë√≠ch c·ªßa ch√∫ng ta l√† t√¨m c√°c h·ªá s·ªë \(\mathbf{w}\) sao cho \(f(\mathbf{w}^T\mathbf{x}_i)\) c√†ng g·∫ßn v·ªõi 1 c√†ng t·ªët v·ªõi c√°c ƒëi·ªÉm d·ªØ li·ªáu thu·ªôc class 1 v√† c√†ng g·∫ßn v·ªõi 0 c√†ng t·ªët v·ªõi nh·ªØng ƒëi·ªÉm thu·ªôc class 0.</p>

<p>K√Ω hi·ªáu \(z_i = f(\mathbf{w}^T\mathbf{x}_i)\) v√† vi·∫øt g·ªôp l·∫°i hai bi·ªÉu th·ª©c b√™n tr√™n ta c√≥:
\[
P(y_i| \mathbf{x}_i; \mathbf{w}) = z_i^{y_i}(1 - z_i)^{1- y_i}
\]</p>

<p>Bi·ªÉu th·ª©c n√†y l√† t∆∞∆°ng ƒë∆∞∆°ng v·ªõi hai bi·ªÉu th·ª©c \((1)\) v√† \((2)\) ·ªü tr√™n v√¨ khi \(y_i=1\), ph·∫ßn th·ª© hai c·ªßa v·∫ø ph·∫£i s·∫Ω tri·ªát ti√™u, khi \(y_i = 0\), ph·∫ßn th·ª© nh·∫•t s·∫Ω b·ªã tri·ªát ti√™u! Ch√∫ng ta mu·ªën m√¥ h√¨nh g·∫ßn v·ªõi d·ªØ li·ªáu ƒë√£ cho nh·∫•t, t·ª©c x√°c su·∫•t n√†y ƒë·∫°t gi√° tr·ªã cao nh·∫•t.</p>

<p>X√©t to√†n b·ªô training set v·ªõi \(\mathbf{X} = [\mathbf{x}_1,\mathbf{x}_2, \dots, \mathbf{x}_N] \in \mathbb{R}^{d \times N}\) v√† \(\mathbf{y} = [y_1, y_2, \dots, y_N]\), ch√∫ng ta c·∫ßn t√¨m \(\mathbf{w}\) ƒë·ªÉ bi·ªÉu th·ª©c sau ƒë√¢y ƒë·∫°t gi√° tr·ªã l·ªõn nh·∫•t:
\[
P(\mathbf{y}|\mathbf{X}; \mathbf{w})
\]
·ªü ƒë√¢y, ta c≈©ng k√Ω hi·ªáu \(\mathbf{X, y}\) nh∆∞ c√°c <a href="https://vi.wikipedia.org/wiki/Bi·∫øn_ng·∫´u_nhi√™n">bi·∫øn ng·∫´u nhi√™n</a> (random variables). N√≥i c√°ch kh√°c:
\[
\mathbf{w} = \arg\max_{\mathbf{w}} P(\mathbf{y}|\mathbf{X}; \mathbf{w})
\]</p>

<p><a name="maximun likelihood estimation"></a></p>

<p>B√†i to√°n t√¨m tham s·ªë ƒë·ªÉ m√¥ h√¨nh g·∫ßn v·ªõi d·ªØ li·ªáu nh·∫•t tr√™n ƒë√¢y c√≥ t√™n g·ªçi chung l√† b√†i to√°n <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"><em>maximum likelihood estimation</em></a> v·ªõi h√†m s·ªë ph√≠a sau \(\arg\max\) ƒë∆∞·ª£c g·ªçi l√† <em>likelihood function</em>. Khi l√†m vi·ªác v·ªõi c√°c b√†i to√°n Machine Learning s·ª≠ d·ª•ng c√°c m√¥ h√¨nh x√°c su·∫•t th·ªëng k√™, ch√∫ng ta s·∫Ω g·∫∑p l·∫°i c√°c b√†i to√°n thu·ªôc d·∫°ng n√†y, ho·∫∑c <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"><em>maximum a posteriori estimation</em></a>, r·∫•t nhi·ªÅu. T√¥i s·∫Ω d√†nh 1 b√†i kh√°c ƒë·ªÉ n√≥i v·ªÅ hai d·∫°ng b√†i to√°n n√†y.</p>

<p>Gi·∫£ s·ª≠ th√™m r·∫±ng c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c sinh ra m·ªôt c√°ch ng·∫´u nhi√™n ƒë·ªôc l·∫≠p v·ªõi nhau (independent), ta c√≥ th·ªÉ vi·∫øt:
\[
\begin{eqnarray}
P(\mathbf{y}|\mathbf{X}; \mathbf{w}) &amp;=&amp; \prod_{i=1}^N P(y_i| \mathbf{x}_i; \mathbf{w}) \newline
&amp;=&amp; \prod_{i=1}^N z_i^{y_i}(1 - z_i)^{1- y_i}
\end{eqnarray}
\]
v·ªõi \(\prod\) l√† k√Ω hi·ªáu c·ªßa t√≠ch. B·∫°n ƒë·ªçc c√≥ th·ªÉ mu·ªën ƒë·ªçc th√™m v·ªÅ <a href="https://vi.wikipedia.org/wiki/ƒê·ªôc_l·∫≠p_th·ªëng_k√™">ƒê·ªôc l·∫≠p th·ªëng k√™</a>.</p>

<p>Tr·ª±c ti·∫øp t·ªëi ∆∞u h√†m s·ªë n√†y theo \(\mathbf{w}\) nh√¨n qua kh√¥ng ƒë∆°n gi·∫£n! H∆°n n·ªØa, khi \(N\) l·ªõn, t√≠ch c·ªßa \(N\) s·ªë nh·ªè h∆°n 1 c√≥ th·ªÉ d·∫´n t·ªõi sai s·ªë trong t√≠nh to√°n (numerial error) v√¨ t√≠ch l√† m·ªôt s·ªë qu√° nh·ªè. M·ªôt ph∆∞∆°ng ph√°p th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë√≥ l√† l·∫•y logarit t·ª± nhi√™n (c∆° s·ªë \(e\)) c·ªßa  <em>likelihood function</em> bi·∫øn ph√©p nh√¢n th√†nh ph√©p c·ªông v√† ƒë·ªÉ tr√°nh vi·ªác s·ªë qu√° nh·ªè. Sau ƒë√≥ l·∫•y ng∆∞·ª£c d·∫•u ƒë·ªÉ ƒë∆∞·ª£c m·ªôt h√†m v√† coi n√≥ l√† h√†m m·∫•t m√°t. L√∫c n√†y b√†i to√°n t√¨m gi√° tr·ªã l·ªõn nh·∫•t (maximum likelihood) tr·ªü th√†nh b√†i to√°n t√¨m gi√° tr·ªã nh·ªè nh·∫•t c·ªßa h√†m m·∫•t m√°t (h√†m n√†y c√≤n ƒë∆∞·ª£c g·ªçi l√† negative log likelihood):
\[
\begin{eqnarray}
J(\mathbf{w}) = -\log P(\mathbf{y}|\mathbf{X}; \mathbf{w}) \newline
= -\sum_{i=1}^N(y_i \log {z}_i + (1-y_i) \log (1 - {z}_i))
\end{eqnarray}
\]
v·ªõi ch√∫ √Ω r·∫±ng \(z_i\) l√† m·ªôt h√†m s·ªë c·ªßa \(\mathbf{w}\). B·∫°n ƒë·ªçc t·∫°m nh·ªõ bi·ªÉu th·ª©c v·∫ø ph·∫£i c√≥ t√™n g·ªçi l√† <em>cross entropy</em>, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëo <em>kho·∫£ng c√°ch</em> gi·ªØa hai ph√¢n ph·ªëi (distributions). Trong b√†i to√°n ƒëang x√©t, m·ªôt ph√¢n ph·ªëi l√† d·ªØ li·ªáu ƒë∆∞·ª£c cho, v·ªõi x√°c su·∫•t ch·ªâ l√† 0 ho·∫∑c 1; ph√¢n ph·ªëi c√≤n l·∫°i ƒë∆∞·ª£c t√≠nh theo m√¥ h√¨nh logistic regression. <em>Kho·∫£ng c√°ch</em> gi·ªØa hai ph√¢n ph·ªëi nh·ªè ƒë·ªìng nghƒ©a v·ªõi vi·ªác (<em>c√≥ v·∫ª hi·ªÉn nhi√™n l√†</em>) hai ph√¢n ph·ªëi ƒë√≥ r·∫•t g·∫ßn nhau. T√≠nh ch·∫•t c·ª• th·ªÉ c·ªßa h√†m s·ªë n√†y s·∫Ω ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong m·ªôt b√†i kh√°c m√† t·∫ßm quan tr·ªçng c·ªßa kho·∫£ng c√°ch gi·ªØa hai ph√¢n ph·ªëi l√† l·ªõn h∆°n.</p>

<p><strong>Ch√∫ √Ω:</strong> Trong machine learning, logarit th·∫≠p ph√¢n √≠t ƒë∆∞·ª£c d√πng, v√¨ v·∫≠y \(\log\) th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ k√Ω hi·ªáu logarit t·ª± nhi√™n.</p>

<p><a name="toi-uu-ham-mat-mat"></a></p>

<h3 id="t·ªëi-∆∞u-h√†m-m·∫•t-m√°t">T·ªëi ∆∞u h√†m m·∫•t m√°t</h3>

<!-- V·ªõi h√†m _sigmoid_ \\(\sigma\\), nh·∫Øc l·∫°i t√≠nh ch·∫•t ƒë·∫∑t bi·ªát ƒë√£ ch·ª©ng minh ·ªü tr√™n: \\(\sigma'(s) = \sigma(s) ( 1 - \sigma(s))\\) v√† \\(z\_i = \sigma(\mathbf{w}^T\mathbf{x}\_i)\\), ta c√≥:

\\[
\frac{\partial z\_i}{\partial \mathbf{w}} = z\_i (1 - z\_i)\mathbf{x}\_i
\\]
v√†:

\\[
\frac{\partial \log z\_i}{\partial \mathbf{w}} = \frac{1}{z\_i}\frac{\partial z\_i}{\partial \mathbf{w}} = (1-z\_i)\mathbf{x}\_i
\\]

\\[
\frac{\partial \log(1 - z\_i)}{\partial \mathbf{w}} = -\frac{1}{1 - z\_i}\frac{\partial z\_i}{\partial \mathbf{w}} = -z\_i\mathbf{x}\_i
\\]
 -->

<p>Ch√∫ng ta l·∫°i s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">Stochastic Gradient Descent</a> (SGD) ·ªü ƒë√¢y (<em>B·∫°n ƒë·ªçc ƒë∆∞·ª£c khuy·∫øn kh√≠ch ƒë·ªçc SGD tr∆∞·ªõc khi ƒë·ªçc ph·∫ßn n√†y</em>) . H√†m m·∫•t m√°t v·ªõi ch·ªâ m·ªôt ƒëi·ªÉm d·ªØ li·ªáu \((\mathbf{x}_i, y_i)\) l√†:
\[
J(\mathbf{w}; \mathbf{x}_i, y_i) = -(y_i \log {z}_i + (1-y_i) \log (1 - {z}_i))
\]</p>

<p>V·ªõi ƒë·∫°o h√†m:
\[
\begin{eqnarray}
\frac{\partial J(\mathbf{w}; \mathbf{x}_i, y_i)}{\partial \mathbf{w}} &amp;=&amp; -(\frac{y_i}{z_i} - \frac{1- y_i}{1 - z_i} ) \frac{\partial z_i}{\partial \mathbf{w}} \newline
&amp;=&amp; \frac{z_i - y_i}{z_i(1 - z_i)} \frac{\partial z_i}{\partial \mathbf{w}} ~~~~~~ (3)
\end{eqnarray}
\]</p>

<p>ƒê·ªÉ cho bi·ªÉu th·ª©c n√†y tr·ªü n√™n <em>g·ªçn</em> v√† <em>ƒë·∫πp</em> h∆°n, ch√∫ng ta s·∫Ω t√¨m h√†m \(z = f(\mathbf{w}^T\mathbf{x})\) sao cho m·∫´u s·ªë b·ªã tri·ªát ti√™u. N·∫øu ƒë·∫∑t \(s = \mathbf{w}^T\mathbf{x}\), ch√∫ng ta s·∫Ω c√≥:
\[
\frac{\partial z_i}{\partial \mathbf{w}} = \frac{\partial z_i}{\partial s} \frac{\partial s}{\partial \mathbf{w}} = \frac{\partial z_i}{\partial s} \mathbf{x}
\]
M·ªôt c√°ch tr·ª±c quan nh·∫•t, ta s·∫Ω t√¨m h√†m s·ªë \(z = f(s)\) sao cho:
\[
\frac{\partial z}{\partial s} = z(1 - z) ~~ (4)
\]
ƒë·ªÉ tri·ªát ti√™u m·∫´u s·ªë trong bi·ªÉu th·ª©c \((3)\). Ch√∫ng ta c√πng kh·ªüi ƒë·ªông m·ªôt ch√∫t v·ªõi ph∆∞∆°ng tr√¨nh vi ph√¢n ƒë∆°n gi·∫£n n√†y. Ph∆∞∆°ng tr√¨nh \((4)\) t∆∞∆°ng ƒë∆∞∆°ng v·ªõi:
\[
\begin{eqnarray}
&amp;\frac{\partial z}{z(1-z)} &amp;=&amp; \partial s \newline
\Leftrightarrow &amp; (\frac{1}{z} + \frac{1}{1 - z})\partial z &amp;=&amp;\partial s \newline
\Leftrightarrow &amp; \log z - \log(1 - z) &amp;=&amp; s \newline
\Leftrightarrow &amp; \log \frac{z}{1 - z} &amp;=&amp; s \newline
\Leftrightarrow &amp; \frac{z}{1 - z} &amp;=&amp; e^s \newline
\Leftrightarrow &amp; z &amp;=&amp; e^s (1 - z) \newline
\Leftrightarrow &amp; z = \frac{e^s}{1 +e^s} &amp;=&amp;\frac{1}{1 + e^{-s}} = \sigma(s)
\end{eqnarray}
\]
ƒê·∫øn ƒë√¢y, t√¥i hy v·ªçng c√°c b·∫°n ƒë√£ hi·ªÉu h√†m s·ªë <em>sigmoid</em> ƒë∆∞·ª£c t·∫°o ra nh∆∞ th·∫ø n√†o.</p>

<p><em>Ch√∫ √Ω: Trong vi·ªác gi·∫£i ph∆∞∆°ng tr√¨nh vi ph√¢n ·ªü tr√™n, t√¥i ƒë√£ b·ªè qua h·∫±ng s·ªë khi l·∫•y nguy√™n h√†m hai v·∫ø. Tuy v·∫≠y, vi·ªác n√†y kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu t·ªõi k·∫øt qu·∫£.</em></p>

<p><a name="cong-thuc-cap-nhat-cho-logistic-sigmoid-regression"></a></p>

<h3 id="c√¥ng-th·ª©c-c·∫≠p-nh·∫≠t-cho-logistic-sigmoid-regression">C√¥ng th·ª©c c·∫≠p nh·∫≠t cho logistic sigmoid regression</h3>
<p>T·ªõi ƒë√¢y, b·∫°n ƒë·ªçc c√≥ th·ªÉ ki·ªÉm tra r·∫±ng:
\[
\frac{\partial J(\mathbf{w}; \mathbf{x}_i, y_i)}{\partial \mathbf{w}} = (z_i - y_i)\mathbf{x}_i
\]
Q√∫a ƒë·∫πp!</p>

<p>V√† c√¥ng th·ª©c c·∫≠p nh·∫≠t (theo thu·∫≠t to√°n <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">SGD</a>) cho logistic regression l√†: 
\[
\mathbf{w} = \mathbf{w} + \eta(y_i - z_i)\mathbf{x}_i
\]
Kh√° ƒë∆°n gi·∫£n! V√†, nh∆∞ th∆∞·ªùng l·ªá, ch√∫ng ta s·∫Ω c√≥ v√†i v√≠ d·ª• v·ªõi Python.</p>

<p><a name="-vi-du-voi-python"></a></p>

<h2 id="3-v√≠-d·ª•-v·ªõi-python">3. V√≠ d·ª• v·ªõi Python</h2>

<p><a name="vi-du-voi-du-lieu--chieu"></a></p>

<h3 id="v√≠-d·ª•-v·ªõi-d·ªØ-li·ªáu-1-chi·ªÅu">V√≠ d·ª• v·ªõi d·ªØ li·ªáu 1 chi·ªÅu</h3>

<p>Quay tr·ªü l·∫°i v·ªõi v√≠ d·ª• n√™u ·ªü ph·∫ßn Gi·ªõi thi·ªáu. Tr∆∞·ªõc ti√™n ta c·∫ßn khai b√°o v√†i th∆∞ vi·ªán v√† d·ªØ li·ªáu:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To support both python 2 and python 3
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">1.50</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="mf">2.00</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">,</span> <span class="mf">2.50</span><span class="p">,</span> 
              <span class="mf">2.75</span><span class="p">,</span> <span class="mf">3.00</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">,</span> <span class="mf">3.50</span><span class="p">,</span> <span class="mf">4.00</span><span class="p">,</span> <span class="mf">4.25</span><span class="p">,</span> <span class="mf">4.50</span><span class="p">,</span> <span class="mf">4.75</span><span class="p">,</span> <span class="mf">5.00</span><span class="p">,</span> <span class="mf">5.50</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># extended data 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="cac-ham-can-thiet-cho-logistic-sigmoid-regression"></a></p>

<h3 id="c√°c-h√†m-c·∫ßn-thi·∫øt-cho-logistic-sigmoid-regression">C√°c h√†m c·∫ßn thi·∫øt cho logistic sigmoid regression</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">logistic_sigmoid_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_count</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_init</span><span class="p">]</span>    
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">check_w_after</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="k">while</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">max_count</span><span class="p">:</span>
        <span class="c1"># mix data 
</span>        <span class="n">mix_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">mix_id</span><span class="p">:</span>
            <span class="n">xi</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">zi</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">xi</span><span class="p">))</span>
            <span class="n">w_new</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">yi</span> <span class="o">-</span> <span class="n">zi</span><span class="p">)</span><span class="o">*</span><span class="n">xi</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># stopping criteria
</span>            <span class="k">if</span> <span class="n">count</span><span class="o">%</span><span class="n">check_w_after</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>                
                <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_new</span> <span class="o">-</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="n">check_w_after</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">w</span>
            <span class="n">w</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>
<span class="n">eta</span> <span class="o">=</span> <span class="p">.</span><span class="mi">05</span> 
<span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">logistic_sigmoid_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-4.092695  ]
 [ 1.55277242]]
</code></pre></div></div>

<p>V·ªõi k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c, ƒë·∫ßu ra \(y\) c√≥ th·ªÉ ƒë∆∞·ª£c d·ª± ƒëo√°n theo c√¥ng th·ª©c: <code class="language-plaintext highlighter-rouge">y = sigmoid(-4.1 + 1.55*x)</code>. V·ªõi d·ªØ li·ªáu trong t·∫≠p training, k·∫øt qu·∫£ l√†:</p>

<!-- 
<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
</pre></td></tr></tbody></table></code></pre></figure>
 -->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 0.03281144  0.04694533  0.06674738  0.09407764  0.13102736  0.17961209
   0.17961209  0.24121129  0.31580406  0.40126557  0.49318368  0.58556493
   0.67229611  0.74866712  0.86263755  0.90117058  0.92977426  0.95055357
   0.96541314  0.98329067]]
</code></pre></div></div>

<p>Bi·ªÉu di·ªÖn k·∫øt qu·∫£ n√†y tr√™n ƒë·ªì th·ªã ta c√≥:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s">'bs'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">w0</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="o">-</span><span class="n">w0</span><span class="o">/</span><span class="n">w1</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">w0</span> <span class="o">+</span> <span class="n">w1</span><span class="o">*</span><span class="n">xx</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="s">'g-'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="s">'y^'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'studying hours'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predicted probability of pass'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="imgcap">
<img src="\assets\LogisticRegression\lg_results.png" align="center" width="600" />
<div class="thecap">H√¨nh 4: D·ªØ li·ªáu v√† h√†m sigmoid t√¨m ƒë∆∞·ª£c.</div>
</div>

<p>N·∫øu nh∆∞ ch·ªâ c√≥ hai output l√† ‚Äòfail‚Äô ho·∫∑c ‚Äòpass‚Äô, ƒëi·ªÉm tr√™n ƒë·ªì th·ªã c·ªßa h√†m sigmoid t∆∞∆°ng ·ª©ng v·ªõi x√°c su·∫•t 0.5 ƒë∆∞·ª£c ch·ªçn l√†m <em>hard threshold</em> (ng∆∞·ª°ng c·ª©ng). Vi·ªác n√†y c√≥ th·ªÉ ch·ª©ng minh kh√° d·ªÖ d√†ng (t√¥i s·∫Ω b√†n ·ªü ph·∫ßn d∆∞·ªõi).</p>

<p><a name="vi-du-voi-du-lieu--chieu-1"></a></p>

<h3 id="v√≠-d·ª•-v·ªõi-d·ªØ-li·ªáu-2-chi·ªÅu">V√≠ d·ª• v·ªõi d·ªØ li·ªáu 2 chi·ªÅu</h3>
<p>Ch√∫ng ta x√©t th√™m m·ªôt v√≠ d·ª• nh·ªè n·ªØa trong kh√¥ng gian hai chi·ªÅu. Gi·∫£ s·ª≠ ch√∫ng ta c√≥ hai class xanh-ƒë·ªè v·ªõi d·ªØ li·ªáu ƒë∆∞·ª£c ph√¢n b·ªë nh∆∞ h√¨nh d∆∞·ªõi.</p>
<div class="imgcap">
<img src="\assets\LogisticRegression\logistic_2d.png" align="center" width="400" />
<div class="thecap">H√¨nh 5: Hai class v·ªõi d·ªØ li·ªáu hai chi·ªÅu.</div>
</div>
<p>V·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o n·∫±m trong kh√¥ng gian hai chi·ªÅu, h√†m sigmoid c√≥ d·∫°ng nh∆∞ th√°c n∆∞·ªõc d∆∞·ªõi ƒë√¢y:</p>
<div class="imgcap">
<img src="http://galaxy.agh.edu.pl/~vlsi/AI/bias/img/plaszczyzna.gif" align="center" width="400" />
<div class="thecap">H√¨nh 6: H√†m sigmoid v·ªõi d·ªØ li·ªáu c√≥ chi·ªÅu l√† 2. (Ngu·ªìn: <a href="http://galaxy.agh.edu.pl/~vlsi/AI/bias/bias_eng.html">Biased and non biased neurons</a>)</div>
</div>

<p>K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c khi √°p d·ª•ng m√¥ h√¨nh logistic regression ƒë∆∞·ª£c minh h·ªça nh∆∞ h√¨nh d∆∞·ªõi v·ªõi m√†u n·ªÅn kh√°c nhau th·ªÉ hi·ªán x√°c su·∫•t ƒëi·ªÉm ƒë√≥ thu·ªôc class ƒë·ªè. ƒê·ªè h∆°n t·ª©c g·∫ßn 1 h∆°n, xanh h∆°n t·ª©c g·∫ßn 0 h∆°n.</p>
<div class="imgcap">
<img src="\assets\LogisticRegression\logistic_2d_2.png" align="center" width="400" />
<div class="thecap">H√¨nh 7: Logistic Regression v·ªõi d·ªØ li·ªáu hai chi·ªÅu.</div>
</div>

<p>N·∫øu ph·∫£i l·ª±a ch·ªçn m·ªôt <em>ng∆∞·ª°ng c·ª©ng</em> (ch·ª© kh√¥ng ch·∫•p nh·∫≠n x√°c su·∫•t) ƒë·ªÉ ph√¢n chia hai class, ch√∫ng ta quan s√°t th·∫•y ƒë∆∞·ªùng th·∫≥ng n·∫±m n·∫±m trong khu v·ª±c xanh l·ª•c l√† m·ªôt l·ª±a ch·ªçn h·ª£p l√Ω. T√¥i s·∫Ω ch·ª©ng minh ·ªü ph·∫ßn d∆∞·ªõi r·∫±ng, ƒë∆∞·ªùng ph√¢n chia gi·ªØa hai class t√¨m ƒë∆∞·ª£c b·ªüi logistic regression c√≥ d·∫°ng m·ªôt ƒë∆∞·ªùng ph·∫≥ng, t·ª©c v·∫´n l√† linear.</p>

<p><a name="-mot-vai-tinh-chat-cua-logistic-regression"></a></p>

<h2 id="4-m·ªôt-v√†i-t√≠nh-ch·∫•t-c·ªßa-logistic-regression">4. M·ªôt v√†i t√≠nh ch·∫•t c·ªßa Logistic Regression</h2>

<p><a name="logistic-regression-thuc-ra-duoc-su-dung-nhieu-trong-cac-bai-toan-classification"></a></p>

<h3 id="logistic-regression-th·ª±c-ra-ƒë∆∞·ª£c-s·ª≠-d·ª•ng-nhi·ªÅu-trong-c√°c-b√†i-to√°n-classification">Logistic Regression th·ª±c ra ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu trong c√°c b√†i to√°n Classification.</h3>
<p>M·∫∑c d√π c√≥ t√™n l√† Regression, t·ª©c m·ªôt m√¥ h√¨nh cho fitting, Logistic Regression l·∫°i ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu trong c√°c b√†i to√°n Classification. Sau khi t√¨m ƒë∆∞·ª£c m√¥ h√¨nh, vi·ªác x√°c ƒë·ªãnh class \(y\) cho m·ªôt ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}\) ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng vi·ªác so s√°nh hai bi·ªÉu th·ª©c x√°c su·∫•t:
\[
P(y = 1| \mathbf{x}; \mathbf{w}); ~~ P(y = 0| \mathbf{x}; \mathbf{w}) 
\]
N·∫øu bi·ªÉu th·ª©c th·ª© nh·∫•t l·ªõn h∆°n th√¨ ta k·∫øt lu·∫≠n ƒëi·ªÉm d·ªØ li·ªáu thu·ªôc class 1, ng∆∞·ª£c l·∫°i th√¨ n√≥ thu·ªôc class 0. V√¨ t·ªïng hai bi·ªÉu th·ª©c n√†y lu√¥n b·∫±ng 1 n√™n m·ªôt c√°ch g·ªçn h∆°n, ta ch·ªâ c·∫ßn x√°c ƒë·ªãnh xem \(P(y = 1| \mathbf{x}; \mathbf{w})\) l·ªõn h∆°n 0.5 hay kh√¥ng. N·∫øu c√≥, class 1. N·∫øu kh√¥ng, class 0.</p>

<p><a name="boundary-tao-boi-logistic-regression-co-dang-tuyen-tinh"></a></p>

<h3 id="boundary-t·∫°o-b·ªüi-logistic-regression-c√≥-d·∫°ng-tuy·∫øn-t√≠nh">Boundary t·∫°o b·ªüi Logistic Regression c√≥ d·∫°ng tuy·∫øn t√≠nh</h3>
<p>Th·∫≠t v·∫≠y, theo l·∫≠p lu·∫≠n ·ªü ph·∫ßn tr√™n th√¨ ch√∫ng ta c·∫ßn ki·ªÉm tra:</p>

<p>\[
\begin{eqnarray}
P(y = 1| \mathbf{x}; \mathbf{w}) &amp;&gt;&amp; 0.5 \newline
\Leftrightarrow \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x}}} &amp;&gt;&amp; 0.5 \newline
\Leftrightarrow e^{-\mathbf{w}^T\mathbf{x}} &amp;&lt;&amp; 1 \newline
\Leftrightarrow \mathbf{w}^T\mathbf{x} &amp;&gt;&amp; 0
\end{eqnarray}
\]</p>

<p>N√≥i c√°ch kh√°c, boundary gi·ªØa hai class l√† ƒë∆∞·ªùng c√≥ ph∆∞∆°ng tr√¨nh \(\mathbf{w}^T\mathbf{x}\). ƒê√¢y ch√≠nh l√† ph∆∞∆°ng tr√¨nh c·ªßa m·ªôt si√™u m·∫∑t ph·∫≥ng. V·∫≠y Logistic Regression t·∫°o ra boundary c√≥ d·∫°ng tuy·∫øn t√≠nh.</p>

<p><a name="-thao-luan"></a></p>

<h2 id="5-th·∫£o-lu·∫≠n">5. Th·∫£o lu·∫≠n</h2>

<ul>
  <li>M·ªôt ƒëi·ªÉm c·ªông cho Logistic Regression so v·ªõi PLA l√† n√≥ kh√¥ng c·∫ßn c√≥ gi·∫£ thi·∫øt d·ªØ li·ªáu hai class l√† linearly separable. Tuy nhi√™n, boundary t√¨m ƒë∆∞·ª£c v·∫´n c√≥ d·∫°ng tuy·∫øn t√≠nh. V·∫≠y n√™n m√¥ h√¨nh n√†y ch·ªâ ph√π h·ª£p v·ªõi lo·∫°i d·ªØ li·ªáu m√† hai class l√† g·∫ßn v·ªõi linearly separable. M·ªôt ki·ªÉu d·ªØ li·ªáu m√† Logistic Regression kh√¥ng l√†m vi·ªác ƒë∆∞·ª£c l√† d·ªØ li·ªáu m√†
m·ªôt class ch·ª©a c√°c ƒëi·ªÉm n·∫±m trong 1 v√≤ng tr√≤n, class kia ch·ª©a c√°c ƒëi·ªÉm b√™n ngo√†i ƒë∆∞·ªùng tr√≤n ƒë√≥. Ki·ªÉu d·ªØ li·ªáu n√†y ƒë∆∞·ª£c g·ªçi l√† phi tuy·∫øn (non-linear). Sau m·ªôt v√†i b√†i n·ªØa, t√¥i s·∫Ω gi·ªõi thi·ªáu v·ªõi c√°c b·∫°n c√°c m√¥ h√¨nh kh√°c ph√π h·ª£p h∆°n v·ªõi lo·∫°i d·ªØ li·ªáu n√†y h∆°n.</li>
  <li>M·ªôt h·∫°n ch·∫ø n·ªØa c·ªßa Logistic Regression l√† n√≥ y√™u c·∫ßu c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c t·∫°o ra m·ªôt c√°ch <em>ƒë·ªôc l·∫≠p</em> v·ªõi nhau. Tr√™n th·ª±c t·∫ø, c√°c ƒëi·ªÉm d·ªØ li·ªáu c√≥ th·ªÉ b·ªã <em>·∫£nh h∆∞·ªüng</em> b·ªüi nhau. V√≠ d·ª•: c√≥ m·ªôt nh√≥m √¥n t·∫≠p v·ªõi nhau trong 4 gi·ªù, c·∫£ nh√≥m ƒë·ªÅu thi ƒë·ªó (gi·∫£ s·ª≠ c√°c b·∫°n n√†y h·ªçc r·∫•t t·∫≠p trung), nh∆∞ng c√≥ m·ªôt sinh vi√™n h·ªçc m·ªôt m√¨nh c≈©ng trong 4 gi·ªù th√¨ x√°c su·∫•t thi ƒë·ªó th·∫•p h∆°n. M·∫∑c d√π v·∫≠y, ƒë·ªÉ cho ƒë∆°n gi·∫£n, khi x√¢y d·ª±ng m√¥ h√¨nh, ng∆∞·ªùi ta v·∫´n th∆∞·ªùng gi·∫£ s·ª≠ c√°c ƒëi·ªÉm d·ªØ li·ªáu l√† ƒë·ªôc l·∫≠p v·ªõi nhau.</li>
  <li>Khi bi·ªÉu di·ªÖn theo Neural Networks, Linear Regression, PLA, v√† Logistic Regression c√≥ d·∫°ng nh∆∞ sau:</li>
</ul>
<div class="imgcap">
<img src="\assets\LogisticRegression\3models.png" align="center" width="800" />
<div class="thecap">H√¨nh 8: Bi·ªÉu di·ªÖn Linear Regression, PLA, v√† Logistic Regression theo Neural network.</div>
</div>

<ul>
  <li>
    <p>N·∫øu h√†m m·∫•t m√°t c·ªßa Logistic Regression ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng:
\[
J(\mathbf{w}) = \sum_{i=1}^N (y_i - z_i)^2
\]
th√¨ kh√≥ khƒÉn g√¨ s·∫Ω x·∫£y ra? C√°c b·∫°n h√£y coi ƒë√¢y nh∆∞ m·ªôt b√†i t·∫≠p nh·ªè.</p>
  </li>
  <li>
    <p>Source code cho c√°c v√≠ d·ª• trong b√†i n√†y c√≥ th·ªÉ <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/LogisticRegression/LogisticRegression_post.ipynb">t√¨m th·∫•y ·ªü ƒë√¢y</a>.</p>
  </li>
</ul>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-t√†i-li·ªáu-tham-kh·∫£o">6. T√†i li·ªáu tham kh·∫£o</h2>

<p>[1] Cox, David R. ‚ÄúThe regression analysis of binary sequences.‚Äù Journal of the Royal Statistical Society. Series B (Methodological) (1958): 215-242.</p>

<p>[2] Cramer, Jan Salomon. ‚ÄúThe origins of logistic regression.‚Äù (2002).</p>

<p>[3] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. (<a href="http://work.caltech.edu/telecourse.html">link to course</a>)</p>

<p>[4] Bishop, Christopher M. ‚ÄúPattern recognition and Machine Learning.‚Äù, Springer  (2006). (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[5] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley &amp; Sons, 2012.</p>

<p>[6] Andrer Ng. CS229 Lecture notes. <a href="https://datajobs.com/data-science-repo/Generalized-Linear-Models-[Andrew-Ng].pdf">Part II: Classification and logistic regression</a></p>

<p>[7] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. <a href="https://statweb.stanford.edu/~tibs/">The Elements of Statistical Learning</a>.</p>
:ET