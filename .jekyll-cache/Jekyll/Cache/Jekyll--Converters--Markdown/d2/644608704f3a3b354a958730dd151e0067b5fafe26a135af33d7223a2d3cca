I"wæ<p><strong>Trong trang n√†y:</strong></p>

<!-- MarkdownTOC -->

<ul>
  <li><a href="#-gioi-thieu">1. Gi·ªõi thi·ªáu</a></li>
  <li><a href="#-linear-discriminant-analysis-cho-bai-toan-voi--classes">2. Linear Discriminant Analysis cho b√†i to√°n v·ªõi 2 classes</a>
    <ul>
      <li><a href="#-y-tuong-co-ban">2.1. √ù t∆∞·ªüng c∆° b·∫£n</a></li>
      <li><a href="#-xay-dung-ham-muc-tieu">2.2. X√¢y d·ª±ng h√†m m·ª•c ti√™u</a></li>
      <li><a href="#-nghiem-cua-bai-toan-toi-uu">2.3. Nghi·ªám c·ªßa b√†i to√°n t·ªëi ∆∞u</a></li>
    </ul>
  </li>
  <li><a href="#-linear-discriminant-analysis-cho-multi-class-classification-problems">3. Linear Discriminant Analysis cho multi-class classification problems</a>
    <ul>
      <li><a href="#-xay-dung-ham-mat-mat">3.1. X√¢y d·ª±ng h√†m m·∫•t m√°t</a>
        <ul>
          <li><a href="#-within-class-nho">3.1.1. Within-class nh·ªè</a></li>
          <li><a href="#-between-class-lon">3.1.2. Between-class l·ªõn</a></li>
        </ul>
      </li>
      <li><a href="#-ham-mat-mat-cho-multi-class-lda">3.2. H√†m m·∫•t m√°t cho multi-class LDA</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-tren-python">4. V√≠ d·ª• tr√™n Python</a>
    <ul>
      <li><a href="#-lda-voi--classes">4.1. LDA v·ªõi 2 classes</a></li>
    </ul>
  </li>
  <li><a href="#thao-luan">5.Th·∫£o lu·∫≠n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. T√†i li·ªáu tham kh·∫£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>
<p>Trong hai b√†i vi·∫øt tr∆∞·ªõc, t√¥i ƒë√£ gi·ªõi thi·ªáu v·ªÅ thu·∫≠t to√°n gi·∫£m chi·ªÅu d·ªØ li·ªáu ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i nh·∫•t - Principle Component Analysis (PCA). Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, PCA l√† m·ªôt ph∆∞∆°ng ph√°p thu·ªôc lo·∫°i <a href="/2016/12/27/categories/#unsupervised-learning-hoc-khong-giam-sat">unsupervised learning</a>, t·ª©c l√† n√≥ ch·ªâ s·ª≠ d·ª•ng c√°c vector m√¥ t·∫£ d·ªØ li·ªáu m√† kh√¥ng d√πng t·ªõi labels, n·∫øu c√≥, c·ªßa d·ªØ li·ªáu. Trong b√†i to√°n classification, d·∫°ng ƒëi·ªÉn h√¨nh nh·∫•t c·ªßa <a href="/2016/12/27/categories/#supervised-learning-hoc-co-giam-sat">supervised learning</a>, vi·ªác s·ª≠ d·ª•ng labels s·∫Ω mang l·∫°i k·∫øt qu·∫£ ph√¢n lo·∫°i t·ªët h∆°n.</p>

<p>Nh·∫Øc l·∫°i m·ªôt l·∫ßn n·ªØa, <a href="http://machinelearningcoban.com/2017/06/15/pca/#-principal-component-analysis">PCA l√† ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu sao cho l∆∞·ª£ng th√¥ng tin v·ªÅ d·ªØ li·ªáu, th·ªÉ hi·ªán ·ªü t·ªïng ph∆∞∆°ng sai, ƒë∆∞·ª£c gi·ªØ l·∫°i l√† nhi·ªÅu nh·∫•t</a>. Tuy nhi√™n, trong nhi·ªÅu tr∆∞·ªùng h·ª£p, ta kh√¥ng c·∫ßn gi·ªØ l·∫°i l∆∞·ª£ng th√¥ng tin l·ªõn nh·∫•t m√† ch·ªâ c·∫ßn gi·ªØ l·∫°i th√¥ng tin c·∫ßn thi·∫øt cho ri√™ng b√†i to√°n. X√©t v√≠ d·ª• v·ªÅ b√†i to√°n ph√¢n l·ªõp v·ªõi 2 classes ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 1.</p>

<hr />

<div class="imgcap">
<img src="/assets/29_lda/lda.png" align="center" width="600" />
</div>

<div class="thecap" style="text-align: justify">H√¨nh 1: Chi·∫øu d·ªØ li·ªáu l√™n c√°c ƒë∆∞·ªùng th·∫≥ng kh√°c nhau. C√≥ hai l·ªõp d·ªØ li·ªáu minh ho·∫° b·ªüi c√°c ƒëi·ªÉm m√†u xanh v√† ƒë·ªè. D·ªØ li·ªáu ƒë∆∞·ª£c gi·∫£m s·ªë chi·ªÅu v·ªÅ 1 b·∫±ng c√°ch chi·∫øu ch√∫ng l√™n c√°c ƒë∆∞·ªùng th·∫≥ng kh√°c nhau \(d_1\) v√† \(d_2\). Trong hai c√°ch chi·ªÅu n√†y, ph∆∞∆°ng c·ªßa \(d_1\) g·∫ßn gi·ªëng v·ªõi ph∆∞∆°ng c·ªßa th√†nh ph·∫ßn ch√≠nh th·ª© nh·∫•t c·ªßa d·ªØ li·ªáu, ph∆∞∆°ng c·ªßa \(d_2\) g·∫ßn v·ªõi th√†nh ph·∫ßn ph·ª• c·ªßa d·ªØ li·ªáu n·∫øu d√πng PCA. Khi chi·∫øu l√™n \(d_1\), c√°c ƒëi·ªÉm m√†u ƒë·ªè v√† xanh b·ªã ch·ªìng l·∫•n l√™n nhau, khi·∫øn cho vi·ªác ph√¢n lo·∫°i d·ªØ li·ªáu l√† kh√¥ng kh·∫£ thi tr√™n ƒë∆∞·ªùng th·∫≥ng n√†y. Ng∆∞·ª£c l·∫°i, khi ƒë∆∞·ª£c chi·∫øu l√™n \(d_2\), d·ªØ li·ªáu c·ªßa hai class ƒë∆∞·ª£c chia th√†nh c√°c c·ª•m t∆∞∆°ng ·ª©ng t√°ch bi·ªát nhau, khi·∫øn cho vi·ªác classification tr·ªü n√™n ƒë∆°n gi·∫£n h∆°n v√† hi·ªáu qu·∫£ h∆°n. C√°c ƒë∆∞·ªùng cong h√¨nh chu√¥ng th·ªÉ hi·ªán x·∫•p x·ªâ ph√¢n b·ªë x√°c su·∫•t c·ªßa d·ªØ li·ªáu h√¨nh chi·∫øu trong m·ªói class.</div>
<hr />

<p>Trong H√¨nh 1, ta gi·∫£ s·ª≠ r·∫±ng d·ªØ li·ªáu ƒë∆∞·ª£c chi·∫øu l√™n 1 ƒë∆∞·ªùng th·∫≥ng v√† m·ªói ƒëi·ªÉm ƒë∆∞·ª£c ƒë·∫°i di·ªán b·ªüi h√¨nh chi·∫øu c·ªßa n√≥ l√™n ƒë∆∞·ªùng th·∫≥ng kia. Nh∆∞ v·∫≠y, t·ª´ d·ªØ li·ªáu nhi·ªÅu chi·ªÅu, ta ƒë√£ gi·∫£m n√≥ v·ªÅ 1 chi·ªÅu. C√¢u h·ªèi ƒë·∫∑t ra l√†, ƒë∆∞·ªùng th·∫≥ng c·∫ßn c√≥ ph∆∞∆°ng nh∆∞ th·∫ø n√†o ƒë·ªÉ h√¨nh chi·∫øu c·ªßa d·ªØ li·ªáu tr√™n ƒë∆∞·ªùng th·∫≥ng n√†y <em>gi√∫p √≠ch cho vi·ªác classification nh·∫•t</em>? Vi·ªác classification ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ ƒë∆∞·ª£c hi·ªÉu l√† vi·ªác t√¨m ra m·ªôt ng∆∞·ª°ng gi√∫p ph√¢n t√°ch hai class m·ªôt c√°ch ƒë∆°n gi·∫£n v√† ƒë·∫°t k·∫øt qu·∫£ t·ªët nh·∫•t.</p>

<p>X√©t hai ƒë∆∞·ªùng th·∫±ng \(d_1\) v√† \(d_2\). Trong ƒë√≥ ph∆∞∆°ng c·ªßa \(d_1\) g·∫ßn v·ªõi ph∆∞∆°ng c·ªßa th√†nh ph·∫ßn ch√≠nh n·∫øu l√†m PCA, ph∆∞∆°ng c·ªßa \(d_2\) g·∫ßn v·ªõi ph∆∞∆°ng c·ªßa th√†nh ph·∫ßn ph·ª• t√¨m ƒë∆∞·ª£c b·∫±ng PCA. N·∫øu ra l√†m gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng PCA, ta s·∫Ω thu ƒë∆∞·ª£c d·ªØ li·ªáu g·∫ßn v·ªõi c√°c ƒëi·ªÉm ƒë∆∞·ª£c chi·∫øu l√™n \(d_1\). L√∫c n√†y vi·ªác ph√¢n t√°ch hai class tr·ªü n√™n ph·ª©c t·∫°p v√¨ c√°c ƒëi·ªÉm ƒë·∫°i di·ªán cho hai classes ch·ªìng l·∫•n l√™n nhau. Ng∆∞·ª£c l·∫°i, n·∫øu ta chi·∫øu d·ªØ li·ªáu l√™n ƒë∆∞·ªùng th·∫≥ng g·∫ßn v·ªõi th√†nh ph·∫ßn ph·ª• t√¨m ƒë∆∞·ª£c b·ªüi PCA, t·ª©c \(d_2\), c√°c ƒëi·ªÉm h√¨nh chi·∫øu n·∫±m ho√†n to√†n v·ªÅ hai ph√≠a kh√°c nhau c·ªßa ƒëi·ªÉm m√†u l·ª•c tr√™n ƒë∆∞·ªùng th·∫≥ng n√†y. V·ªõi b√†i to√°n classification, vi·ªác chi·∫øu d·ªØ li·ªáu l√™n \(d_2\) v√¨ v·∫≠y s·∫Ω mang l·∫°i hi·ªáu qu·∫£ h∆°n. Vi·ªác ph√¢n lo·∫°i m·ªôt ƒëi·ªÉm d·ªØ li·ªáu m·ªõi s·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh nhanh ch√≥ng b·∫±ng c√°ch so s√°nh h√¨nh chi·∫øu c·ªßa n√≥ l√™n \(d_2\) v·ªõi ƒëi·ªÉm m√†u xanh l·ª•c n√†y.</p>

<p>Qua v√≠ d·ª• tr√™n ta th·∫•y, <strong>kh√¥ng ph·∫£i vi·ªác gi·ªØ l·∫°i th√¥ng tin nhi·ªÅu nh·∫•t s·∫Ω lu√¥n mang l·∫°i k·∫øt qu·∫£ t·ªët nh·∫•t.</strong> Ch√∫ √Ω r·∫±ng k·∫øt qu·∫£ c·ªßa ph√¢n t√≠ch tr√™n ƒë√¢y kh√¥ng c√≥ nghƒ©a l√† th√†nh ph·∫ßn ph·ª• mang l·∫°i hi·ªáu qu·∫£ t·ªët h∆°n th√†nh ph·∫ßn ch√≠nh, n√≥ ch·ªâ l√† m·ªôt tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát. Vi·ªác chi·∫øu d·ªØ li·ªáu l√™n ƒë∆∞·ªùng th·∫≥ng n√†o c·∫ßn nhi·ªÅu ph√¢n t√≠ch c·ª• th·ªÉ h∆°n n·ªØa. C≈©ng xin n√≥i th√™m, hai ƒë∆∞·ªùng th·∫±ng \(d_1\) v√† \(d_2\) tr√™n ƒë√¢y kh√¥ng vu√¥ng g√≥c v·ªõi nhau, t√¥i ch·ªâ ch·ªçn ra hai h∆∞·ªõng g·∫ßn v·ªõi c√°c th√†nh ph·∫ßn ch√≠nh v√† ph·ª• c·ªßa d·ªØ li·ªáu ƒë·ªÉ minh ho·∫°. N·∫øu b·∫°n c·∫ßn ƒë·ªçc th√™m v·ªÅ th√†nh ph·∫ßn ch√≠nh/ph·ª•, b·∫°n s·∫Ω th·∫•y <a href="/2017/06/15/pca/">B√†i 27</a> v√† <a href="/2017/06/21/pca2/">B√†i 28</a> v·ªÅ Principal Component Analysis (Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh) c√≥ √≠ch.</p>

<p>Linear Discriminant Analysis (LDA) ƒë∆∞·ª£c ra ƒë·ªùi nh·∫±m gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y. LDA l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu cho b√†i to√°n classification. LDA c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu (dimensionality reduction), v√† c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† m·ªôt ph∆∞∆°ng ph√°p ph√¢n l·ªõp (classification), v√† c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªìng th·ªùi cho c·∫£ hai, t·ª©c gi·∫£m chi·ªÅu d·ªØ li·ªáu sao cho vi·ªác ph√¢n l·ªõp hi·ªáu qu·∫£ nh·∫•t. S·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu m·ªõi l√† nh·ªè h∆°n ho·∫∑c b·∫±ng \(C-1\) trong ƒë√≥ \(C\) l√† s·ªë l∆∞·ª£ng classes. T·ª´ ‚ÄòDiscriminant‚Äô ƒë∆∞·ª£c hi·ªÉu l√† <em>nh·ªØng th√¥ng tin ƒë·∫∑c tr∆∞ng cho m·ªói class, khi·∫øn n√≥ kh√¥ng b·ªã l·∫´n v·ªõi c√°c classes kh√°c</em>. T·ª´ ‚ÄòLinear‚Äô ƒë∆∞·ª£c d√πng v√¨ c√°ch gi·∫£m chi·ªÅu d·ªØ li·ªáu ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi m·ªôt ma tr·∫≠n chi·∫øu (projection matrix), l√† m·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh (linear transform).</p>

<p>Trong M·ª•c 2 d∆∞·ªõi ƒë√¢y, t√¥i s·∫Ω tr√¨nh b√†y v·ªÅ tr∆∞·ªùng h·ª£p binary classification, t·ª©c c√≥ 2 classes. M·ª•c 3 s·∫Ω t·ªïng qu√°t l√™n cho tr∆∞·ªùng h·ª£p v·ªõi nhi·ªÅu classes h∆°n 2. M·ª•c 4 s·∫Ω c√≥ c√°c v√≠ d·ª• v√† code Python cho LDA.</p>

<p><a name="-linear-discriminant-analysis-cho-bai-toan-voi--classes"></a></p>

<h2 id="2-linear-discriminant-analysis-cho-b√†i-to√°n-v·ªõi-2-classes">2. Linear Discriminant Analysis cho b√†i to√°n v·ªõi 2 classes</h2>
<p><a name="-y-tuong-co-ban"></a></p>

<h3 id="21-√Ω-t∆∞·ªüng-c∆°-b·∫£n">2.1. √ù t∆∞·ªüng c∆° b·∫£n</h3>
<p>M·ªçi ph∆∞∆°ng ph√°p classification ƒë·ªÅu ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu v·ªõi b√†i to√°n binary classification, v√† LDA c≈©ng kh√¥ng ph·∫£i ngo·∫°i l·ªá.</p>

<p>Quay l·∫°i v·ªõi Hinh 1, c√°c ƒë∆∞·ªùng h√¨nh chu√¥ng th·ªÉ hi·ªán ƒë·ªì th·ªã c·ªßa c√°c h√†m m·∫≠t ƒë·ªô x√°c su·∫•t (probability density function - pdf) c·ªßa d·ªØ li·ªáu ƒë∆∞·ª£c chi·∫øu xu·ªëng theo t·ª´ng class. <em>Ph√¢n ph·ªëi chu·∫©n ·ªü ƒë√¢y ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ l√† m·ªôt ƒë·∫°i di·ªán, d·ªØ li·ªáu kh√¥ng nh·∫•t thi·∫øt lu√¥n ph·∫£i tu√¢n theo ph√¢n ph·ªëi chu·∫©n.</em></p>

<p>ƒê·ªô r·ªông c·ªßa m·ªói ƒë∆∞·ªùng h√¨nh chu√¥ng th·ªÉ hi·ªán ƒë·ªô l·ªách chu·∫©n c·ªßa d·ªØ li·ªáu. D·ªØ li·ªáu c√†ng t·∫≠p trung th√¨ ƒë·ªô l·ªách chu·∫©n c√†ng nh·ªè, c√†ng ph√¢n t√°n th√¨ ƒë·ªô l·ªách chu·∫©n c√†ng cao. Khi ƒë∆∞·ª£c chi·∫øu l√™n \(d_1\), d·ªØ li·ªáu c·ªßa hai classes b·ªã ph√¢n t√°n qu√° nhi·ªÅu, khi·∫øn cho ch√∫ng b·ªã tr·ªôn l·∫´n v√†o nhau. Khi ƒë∆∞·ª£c chi·∫øu l√™n \(d_2\), m·ªói classes ƒë·ªÅu c√≥ ƒë·ªô l·ªách chu·∫©n nh·ªè, khi·∫øn cho d·ªØ li·ªáu trong t·ª´ng class t·∫≠p trung h∆°n, d·∫´n ƒë·∫øn k·∫øt qu·∫£ t·ªët h∆°n.</p>

<p>Tuy nhi√™n, vi·ªác ƒë·ªô l·ªách chu·∫©n nh·ªè trong m·ªói class ch∆∞a ƒë·ªß ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô <em>Discriminant</em> c·ªßa d·ªØ li·ªáu. X√©t c√°c v√≠ d·ª• trong H√¨nh 2.</p>

<hr />

<div class="imgcap">
<img src="/assets/29_lda/lda4.png" align="center" width="800" />
</div>

<div class="thecap" style="text-align: justify">H√¨nh 2: Kho·∫£ng c√°ch gi·ªØa c√°c k·ª≥ v·ªçng v√† t·ªïng c√°c ph∆∞∆°ng sai ·∫£nh h∆∞·ªüng t·ªõi ƒë·ªô <em>discriminant</em> c·ªßa d·ªØ li·ªáu. a) Kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng l√† l·ªõn nh∆∞ng ph∆∞∆°ng sai trong m·ªói class c≈©ng l·ªõn, khi·∫øn cho hai ph√¢n ph·ªëi ch·ªìng l·∫•n l√™n nhau (ph·∫ßn m√†u x√°m). b) Ph∆∞∆°ng sai cho m·ªói class l√† r·∫•t nh·ªè nh∆∞ng hai k·ª≥ v·ªçng qu√° g·∫ßn nhau, khi·∫øn kh√≥ ph√¢n bi·ªát 2 class. c) Khi ph∆∞∆°ng sai ƒë·ªß nh·ªè v√† kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng ƒë·ªß l·ªõn, ta th·∫•y r·∫±ng d·ªØ li·ªáu <em>discriminant</em> h∆°n.</div>
<hr />

<p>H√¨nh 2a) gi·ªëng v·ªõi d·ªØ li·ªáu khi chi·∫øu l√™n \(d_1\) ·ªü H√¨nh 1. C·∫£ hai class ƒë·ªÅu qu√° ph√¢n t√°n khi·∫øn cho t·ªâ l·ªá ch·ªìng l·∫•n (ph·∫ßn di·ªán t√≠ch m√†u x√°m) l√† l·ªõn, t·ª©c d·ªØ li·ªáu ch∆∞a th·ª±c s·ª± <em>discriminative</em>.</p>

<p>H√¨nh 2b) l√† tr∆∞·ªùng h·ª£p khi ƒë·ªô l·ªách chu·∫©n c·ªßa hai class ƒë·ªÅu nh·ªè, t·ª©c d·ªØ li·ªáu t·∫≠p trung h∆°n. Tuy nhi√™n, v·∫•n ƒë·ªÅ v·ªõi tr∆∞·ªùng h·ª£p n√†y l√† kho·∫£ng c√°ch gi·ªØa hai class, ƒë∆∞·ª£c ƒëo b·∫±ng kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng \(m_1\) v√† \(m_2\), l√† qu√° nh·ªè, khi·∫øn cho ph·∫ßn ch·ªìng l·∫•n c≈©ng chi·∫øm m√¥t t·ªâ l·ªá l·ªõn, v√† t·∫•t nhi√™n, c≈©ng kh√¥ng t·ªët cho classification.</p>

<p>H√¨nh 2c) l√† tr∆∞·ªùng h·ª£p khi hai ƒë·ªô l·ªách chu·∫©n l√† nh·ªè v√† kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng l√† l·ªõn, ph·∫ßn ch·ªëng l·∫•n nh·ªè kh√¥ng ƒë√°ng k·ªÉ.</p>

<p>C√≥ th·ªÉ b·∫°n ƒëang t·ª± h·ªèi, ƒë·ªô l·ªách chu·∫©n v√† kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng ƒë·∫°i di·ªán cho c√°c ti√™u ch√≠ g√¨:</p>

<ul>
  <li>
    <p>Nh∆∞ ƒë√£ n√≥i, ƒë·ªô l·ªách chu·∫©n nh·ªè th·ªÉ hi·ªán vi·ªác d·ªØ li·ªáu √≠t ph√¢n t√°n. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† d·ªØ li·ªáu trong m·ªói class c√≥ xu h∆∞·ªõng gi·ªëng nhau. Hai ph∆∞∆°ng sai \(s_1^2, s_2^2\) c√≤n ƒë∆∞·ª£c g·ªçi l√† c√°c <strong>within-class variances</strong>.</p>
  </li>
  <li>
    <p>Kho·∫£ng c√°ch gi·ªØa c√°c k·ª≥ v·ªçng l√† l·ªõn ch·ª©ng t·ªè r·∫±ng hai classes n·∫±m xa nhau, t·ª©c d·ªØ li·ªáu gi·ªØa c√°c classes l√† kh√°c nhau nhi·ªÅu. B√¨nh ph∆∞∆°ng kho·∫£ng c√°ch gi·ªØa hai k·ª≥ v·ªçng \((m_1 - m_2)^2\) c√≤n ƒë∆∞·ª£c g·ªçi l√† <strong>between-class variance</strong>.</p>
  </li>
</ul>

<p>Hai classes ƒë∆∞·ª£c g·ªçi l√† <em>discriminative</em> n·∫øu hai class ƒë√≥ c√°ch xa nhau (between-class variance l·ªõn) v√† d·ªØ li·ªáu trong m·ªói class c√≥ xu h∆∞·ªõng gi·ªëng nhau (within-class variance nh·ªè). Linear Discriminant Analysis l√† thu·∫≠t to√°n ƒëi t√¨m m·ªôt ph√©p chi·∫øu sao cho t·ªâ l·ªá gi·ªØa <em>between-class variance</em> v√† <em>within-class variance</em> l·ªõn nh·∫•t c√≥ th·ªÉ.</p>

<p><a name="-xay-dung-ham-muc-tieu"></a></p>

<h3 id="22-x√¢y-d·ª±ng-h√†m-m·ª•c-ti√™u">2.2. X√¢y d·ª±ng h√†m m·ª•c ti√™u</h3>
<p>Gi·∫£ s·ª≠ r·∫±ng c√≥ \(N\) ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\) trong ƒë√≥ \(N_1 &lt; N\) ƒëi·ªÉm ƒë·∫ßu ti√™n thu·ªôc class th·ª© nh·∫•t, \(N_2 = N - N_1\) ƒëi·ªÉm cu·ªëi c√πng thu·ªôc class th·ª© hai. K√Ω hi·ªáu \(\mathcal{C}_1 = \{n | 1 \leq n \leq N_1\}\) l√† t·∫≠p h·ª£p c√°c ch·ªâ s·ªë c·ªßa c√°c ƒëi·ªÉm thu·ªôc class 1 v√† \(\mathcal{C}_2 = \{m| N_1 + 1 \leq m \leq N\})\) l√† t·∫≠p h·ª£p c√°c ch·ªâ s·ªë c·ªßa c√°c ƒëi·ªÉm thu·ªôc class 2. Ph√©p chi·∫øu d·ªØ li·ªáu xu·ªëng 1 ƒë∆∞·ªùng th·∫≥ng c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ t·∫£ b·∫±ng m·ªôt vector h·ªá s·ªë \(\mathbf{w}\), gi√° tr·ªã t∆∞∆°ng ·ª©ng c·ªßa m·ªói ƒëi·ªÉm d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c cho b·ªüi:
\[
y_n = \mathbf{w}^T\mathbf{x}_n, 1 \leq n \leq N
\]</p>

<p>Vector k·ª≥ v·ªçng c·ªßa m·ªói class:
\[
\mathbf{m}_k = \frac{1}{N_k}\sum_{n \in \mathcal{C}_k}\mathbf{x}_n,~~~ k = 1, 2 ~~~~ (1)
\]</p>

<p>Khi ƒë√≥:
\[
m_1 - m_2 = \frac{1}{N_1}\sum_{i \in \mathcal{C}_1}y_i - \frac{1}{N_2}\sum_{j \in \mathcal{C}_2}y_j =  \mathbf{w}^T(\mathbf{m}_1 - \mathbf{m}_2) ~~~~ (2)
\]</p>

<p>C√°c <strong>within-class variances</strong> ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:
\[
s_k^2 = \sum_{n \in \mathcal{C}_k} (y_n - m_k)^2, ~~ k = 1, 2 ~~~~ (3)
\]</p>

<p><em>Ch√∫ √Ω r·∫±ng c√°c within-class variances ·ªü ƒë√¢y kh√¥ng ƒë∆∞·ª£c l·∫•y trung b√¨nh nh∆∞ variance th√¥ng th∆∞·ªùng. ƒêi·ªÅu n√†y ƒë∆∞·ª£c l√Ω gi·∫£i l√† t·∫ßm quan tr·ªçng c·ªßa m·ªói within-class variance n√™n t·ªâ l·ªá thu·∫≠n v·ªõi s·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu trong class ƒë√≥, t·ª©c within-class variance b·∫±ng variance nh√¢n v·ªõi s·ªë ƒëi·ªÉm trong class ƒë√≥. Th·∫ø n√™n ta kh√¥ng chia trung b√¨nh n·ªØa.</em></p>

<p>LDA l√† thu·∫≠t to√°n ƒëi t√¨m gi√° tr·ªã l·ªõn nh·∫•t c·ªßa h√†m m·ª•c ti√™u:
\[
J(\mathbf{w}) = \frac{(m_1 - m_2)^2}{s_1^2 + s_2^2} ~~~~~~~~~ (4)
\]</p>

<p>Ti·∫øp theo, ch√∫ng ta s·∫Ω ƒëi t√¨m bi·ªÉu th·ª©c ph·ª• thu·ªôc gi·ªØa t·ª≠ s·ªë v√† m·∫´u s·ªë trong v·∫ø ph·∫£i c·ªßa \((4)\) v√†o \(\mathbf{w}\).</p>

<p>V·ªõi t·ª≠ s·ªë:
\[
\begin{eqnarray}
(m_1 - m_2)^2 = \mathbf{w}^T \underbrace{(\mathbf{m}_1 - \mathbf{m}_2)(\mathbf{m}_1 - \mathbf{m}_2)^T}_{\mathbf{S}_B} \mathbf{w} = \mathbf{w}^T\mathbf{S}_B \mathbf{w} ~~~~ (5)
\end{eqnarray}
\]
\(\mathbf{S}_B\) c√≤n ƒë∆∞·ª£c g·ªçi l√† <strong>between-class covariance matrix</strong>. ƒê√¢y l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng.</p>

<p>V·ªõi m·∫´u s·ªë:</p>

<p>\[
\begin{eqnarray}
s_1^2 + s_2^2 &amp;=&amp; \sum_{k=1}^2 \sum_{n \in \mathcal{C}_k} \left(\mathbf{w}^T(\mathbf{x}_n - \mathbf{m}_k)\right)^2 \<br />
&amp;=&amp;\mathbf{w}^T \underbrace{\sum_{k=1}^2 \sum_{n \in \mathcal{C}_k} (\mathbf{x}_n - \mathbf{m}_k)(\mathbf{x}_n - \mathbf{m}_k)^T}_{\mathbf{S}_W} \mathbf{w} = \mathbf{w}^T\mathbf{S}_W \mathbf{w}~~~~~(6)
\end{eqnarray}
\]
\(\mathbf{S}_W\) c√≤n ƒë∆∞·ª£c g·ªçi l√† <strong>within-class covariance matrix</strong>. ƒê√¢y c≈©ng l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng v√¨ n√≥ l√† t·ªïng c·ªßa hai ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng.</p>

<p>Trong \((5)\) v√† \((6)\), ta ƒë√£ s·ª≠ d·ª•ng ƒë·∫≥ng th·ª©c:
\[
(\mathbf{a}^T\mathbf{b})^2 = (\mathbf{a}^T\mathbf{b})(\mathbf{a}^T\mathbf{b}) = \mathbf{a}^T\mathbf{b}\mathbf{b}^T\mathbf{a}
\]
v·ªõi \(\mathbf{a}, \mathbf{b}\) l√† hai vectors c√πng chi·ªÅu b·∫•t k·ª≥.</p>

<!-- Nh∆∞ v·∫≠y, h√†m m·ª•c ti√™u c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l·∫°i th√†nh:  -->
<p>Nh∆∞ v·∫≠y, b√†i to√°n t·ªëi ∆∞u cho LDA tr·ªü th√†nh:
\[
\mathbf{w}  = \arg\max_{\mathbf{w}}\frac{\mathbf{w}^T\mathbf{S}_B \mathbf{w}}{\mathbf{w}^T\mathbf{S}_W\mathbf{w}} ~~~~~~~~ (7)
\]</p>

<p><a name="-nghiem-cua-bai-toan-toi-uu"></a></p>

<h3 id="23-nghi·ªám-c·ªßa-b√†i-to√°n-t·ªëi-∆∞u">2.3. Nghi·ªám c·ªßa b√†i to√°n t·ªëi ∆∞u</h3>
<p>Nghi·ªám \(\mathbf{w}\) c·ªßa \((7)\) s·∫Ω l√† nghi·ªám c·ªßa ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m h√†m m·ª•c ti√™u b·∫±ng 0. S·ª≠ d·ª•ng <a href="http://127.0.0.1:4000/math/#chain-rules">chain rule</a> cho <a href="http://127.0.0.1:4000/math/#-dao-ham-cua-ham-nhieu-bien">ƒë·∫°o h√†m h√†m nhi·ªÅu bi·∫øn</a> v√† c√¥ng th·ª©c \(\nabla_{\mathbf{w}}\mathbf{w} \mathbf{A}\mathbf{w} = 2\mathbf{Aw}\) n·∫øu \(\mathbf{A}\) l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng, ta c√≥:</p>

<p>\[
\begin{eqnarray}
\nabla_{\mathbf{w}} J(\mathbf{w}) &amp;=&amp; \frac{1}{(\mathbf{w}^T\mathbf{S}_{W}\mathbf{w})^2} \left(
2\mathbf{S}_B \mathbf{w} (\mathbf{w}^T\mathbf{S}_{W}\mathbf{w}) - 2\mathbf{w}^T\mathbf{S}_{B}\mathbf{w}^T\mathbf{S}_W \mathbf{w}
\right) = \mathbf{0}&amp; (8)\<br />
\Leftrightarrow \mathbf{S}_B\mathbf{w} &amp;=&amp; \frac{\mathbf{w}^T\mathbf{S}_B \mathbf{w}}{\mathbf{w}^T\mathbf{S}_W\mathbf{w}}\mathbf{S}_W\mathbf{w}&amp; (9) \<br />
\mathbf{S}_W^{-1}\mathbf{S}_B \mathbf{w} &amp;=&amp; J(\mathbf{w})\mathbf{w} &amp; (10)
\end{eqnarray}
\]</p>

<hr />

<p><strong>L∆∞u √Ω:</strong> Trong \((10)\), ta ƒë√£ gi·∫£ s·ª≠ r·∫±ng ma tr·∫≠n \(\mathbf{S}_W\) l√† kh·∫£ ngh·ªãch. ƒêi·ªÅu n√†y kh√¥ng lu√¥n lu√¥n ƒë√∫ng, nh∆∞ng c√≥ m·ªôt <em>trick</em> nh·ªè l√† ta c√≥ th·ªÉ x·∫•p x·ªâ \(\mathbf{S}_W\) b·ªüi \( \bar{\mathbf{S}}_W \approx \mathbf{S}_W + \lambda\mathbf{I}\) v·ªõi \(\lambda\) l√† m·ªôt s·ªë th·ª±c d∆∞∆°ng nh·ªè. Ma tr·∫≠n m·ªõi n√†y l√† kh·∫£ ngh·ªãch v√¨ tr·ªã ri√™ng nh·ªè nh·∫•t c·ªßa n√≥ b·∫±ng v·ªõi tr·ªã ri√™ng nh·ªè nh·∫•t c·ªßa \(\mathbf{S}_W\) c·ªông v·ªõi \(\lambda\) t·ª©c kh√¥ng nh·ªè h∆°n \(\lambda &gt; 0\). ƒêi·ªÅu n√†y ƒë∆∞·ª£c suy ra t·ª´ vi·ªác \(\mathbf{S}_W\) l√† m·ªôt ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng. T·ª´ ƒë√≥ suy ra \(\bar{\mathbf{S}}_W\) l√† m·ªôt ma tr·∫≠n x√°c ƒë·ªãnh d∆∞∆°ng v√¨ m·ªçi tr·ªã ri√™ng c·ªßa n√≥ l√† th·ª±c d∆∞∆°ng, v√† v√¨ th·∫ø, n√≥ kh·∫£ ngh·ªãch. Khi t√≠nh to√°n, ta c√≥ th·ªÉ s·ª≠ d·ª•ng ngh·ªãch ƒë·∫£o c·ªßa \(\bar{\mathbf{S}}_W\).</p>

<p>K·ªπ thu·∫≠t n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng r·∫•t nhi·ªÅu khi ta c·∫ßn s·ª≠ d·ª•ng ngh·ªãch ƒë·∫£o c·ªßa m·ªôt ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng v√† ch∆∞a bi·∫øt n√≥ c√≥ th·ª±c s·ª± l√† x√°c ƒë·ªãnh d∆∞∆°ng hay kh√¥ng.</p>
<hr />

<p>Quay tr·ªü l·∫°i v·ªõi \((10)\), v√¨ \(J(\mathbf{w})\) l√† m·ªôt s·ªë v√¥ h∆∞·ªõng, ta suy ra \(\mathbf{w}\) ph·∫£i l√† m·ªôt vector ri√™ng c·ªßa \(\mathbf{S}_W^{-1}\mathbf{S}_B\) ·ª©ng v·ªõi m·ªôt tr·ªã ri√™ng n√†o ƒë√≥. H∆°n n·ªØa, gi√° tr·ªã c·ªßa tr·ªã ri√™ng n√†y b·∫±ng v·ªõi \(J(\mathbf{w})\). V·∫≠y, ƒë·ªÉ h√†m m·ª•c ti√™u l√† l·ªõn nh·∫•t th√¨ \(J(\mathbf{w})\) ch√≠nh l√† tr·ªã ri√™ng l·ªõn nh·∫•t c·ªßa \(\mathbf{S}_W^{-1}\mathbf{S}_B\). D·∫•u b·∫±ng x·∫£y ra khi \(\mathbf{w}\) l√† vector ri√™ng ·ª©ng v·ªõi tr·ªã ri√™ng l·ªõn nh·∫•t ƒë√≥. B·∫°n ƒë·ªçc c√≥ th·ªÉ hi·ªÉu ph·∫ßn n√†y h∆°n khi xem c√°ch l·∫≠p tr√¨nh tr√™n Python ·ªü M·ª•c 4.</p>

<p>T·ª´ c√≥ th·ªÉ th·∫•y ngay r·∫±ng n·∫øu \(\mathbf{w}\) l√† nghi·ªám c·ªßa \((7)\) th√¨ \(k\mathbf{w}\) c≈©ng l√† nghi·ªám v·ªõi \(k\) l√† s·ªë th·ª±c kh√°c kh√¥ng b·∫•t k·ª≥. V·∫≠y ta c√≥ th·ªÉ ch·ªçn \(\mathbf{w}\) sao cho \((\mathbf{m}_1 - \mathbf{m}_2)^T\mathbf{w} = J(\mathbf{w}) = L =\) tr·ªã ri√™ng l·ªõn nh·∫•t c·ªßa \(\mathbf{S}_W^{-1}\mathbf{S}_B\) . Khi ƒë√≥, thay ƒë·ªãnh nghƒ©a c·ªßa \(\mathbf{S}_B\) ·ªü \((5)\) v√†o \((10)\) ta c√≥:</p>

<p>\[
L \mathbf{w} = \mathbf{S}_{W}^{-1}(\mathbf{m}_1 - \mathbf{m}_2)\underbrace{(\mathbf{m}_1 - \mathbf{m}_2)^T\mathbf{w}}_L =  L\mathbf{S}_{W}^{-1}(\mathbf{m}_1 - \mathbf{m}_2)
\]
ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ta c√≥ th·ªÉ ch·ªçn:
\[
\mathbf{w} = \alpha\mathbf{S}_{W}^{-1}(\mathbf{m}_1 - \mathbf{m}_2) ~~~~ (11)
\]
v·ªõi \(\alpha \neq 0\) b·∫•t k·ª≥.</p>

<p>Bi·ªÉu th·ª©c \((11)\) c√≤n ƒë∆∞·ª£c bi·∫øt nh∆∞ l√† <em>Fisher‚Äôs linear discriminant</em>, ƒë∆∞·ª£c ƒë·∫∑t theo t√™n nh√† khoa h·ªçc <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald  Fisher</a>.</p>

<p><a name="-linear-discriminant-analysis-cho-multi-class-classification-problems"></a></p>

<h2 id="3-linear-discriminant-analysis-cho-multi-class-classification-problems">3. Linear Discriminant Analysis cho multi-class classification problems</h2>

<p><a name="-xay-dung-ham-mat-mat"></a></p>

<h3 id="31-x√¢y-d·ª±ng-h√†m-m·∫•t-m√°t">3.1. X√¢y d·ª±ng h√†m m·∫•t m√°t</h3>
<p>Trong m·ª•c n√†y, ch√∫ng ta s·∫Ω xem x√©t tr∆∞·ªùng h·ª£p t·ªïng qu√°t khi c√≥ nhi·ªÅu h∆°n 2 classes. Gi·∫£ s·ª≠ r·∫±ng chi·ªÅu c·ªßa d·ªØ li·ªáu \(D\) l·ªõn h∆°n s·ªë l∆∞·ª£ng classes \(C\).</p>

<p>Gi·∫£ s·ª≠ r·∫±ng chi·ªÅu m√† ch√∫ng ta mu·ªën gi·∫£m v·ªÅ l√† \(D‚Äô &lt; D\) v√† d·ªØ li·ªáu m·ªõi ·ª©ng v·ªõi m·ªói ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}\) l√†:
\[
\mathbf{y} = \mathbf{W}^T\mathbf{x}
\]
v·ªõi \(\mathbf{W} \in \mathbb{R}^{D\times D‚Äô}\).</p>

<p><em>Ch√∫ √Ω r·∫±ng LDA ·ªü ƒë√¢y kh√¥ng s·ª≠ d·ª•ng bias.</em></p>

<p>M·ªôt v√†i k√Ω hi·ªáu:</p>
<ul>
  <li>
    <p>\(\mathbf{X}_k, \mathbf{Y}_k = \mathbf{W}^T\mathbf{X}_k\) l·∫ßn l∆∞·ª£t l√† ma tr·∫≠n d·ªØ li·ªáu c·ªßa class \(k\) trong kh√¥ng gian ban ƒë·∫ßu v√† kh√¥ng gian m·ªõi v·ªõi s·ªë chi·ªÅu nh·ªè h∆°n.</p>
  </li>
  <li>
    <p>\(\mathbf{m}_k = \frac{1}{N_k}\sum_{n \in \mathcal{C}_k}\mathbf{x}_k \in \mathbb{R}^{D}\) l√† vector k·ª≥ v·ªçng c·ªßa class \(k\) trong kh√¥ng gian ban ƒë·∫ßu.</p>
  </li>
  <li>
    <p>\(\mathbf{e}_k = \frac{1}{N_k}\sum_{n \in \mathcal{C}_k} \mathbf{y}_n = \mathbf{W}^T\mathbf{m}_k \in \mathbb{R}^{D‚Äô}\) l√† vector k·ª≥ v·ªçng c·ªßa class \(k\) trong kh√¥ng gian m·ªõi.</p>
  </li>
  <li>
    <p>\(\mathbf{m}\) l√† vector k·ª≥ v·ªçng c·ªßa to√†n b·ªô d·ªØ li·ªáu trong kh√¥ng gian ban ƒë·∫ßu v√† \(\mathbf{e}\) l√† vector k·ª≥ v·ªçng trong kh√¥ng gian m·ªõi.</p>
  </li>
</ul>

<!-- L√∫c n√†y, **within-class covariance matrix** ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:
\\[
\mathbf{S}\_W = \sum\_{k = 1}^C \mathbf{S}\_k ~~~ (12)
\\]
trong ƒë√≥:
\\[
\begin{eqnarray}
    \mathbf{S}\_k &=& \sum_{k \in \mathcal{C}\_k} (\mathbf{x}\_n - \mathbf{m}\_k )(\mathbf{x}\_n - \mathbf{m}\_k )^T & (13)\\\
    \mathbf{m}\_k &=& \frac{1}{N_k} \sum_{n \in \mathcal{C}\_k}\mathbf{x}\_n, ~~ k = 1, 2, \dots, C &(14)
\end{eqnarray}
\\]

v·ªõi \\(\mathcal{C}\_k\\) l√† t·∫≠p h·ª£p c√°c ch·ªâ s·ªë t∆∞∆°ng ·ª©ng v·ªõi d·ªØ li·ªáu thu·ªôc l·ªõp th·ª© \\(k\\) v√† \\(N_k\\) l√† s·ªë ph·∫ßn t·ª≠ c·ªßa \\(\mathcal{C}_k\\).

Bi·ªÉu th·ª©c t·ªïng qu√° c·ªßa **between-class covariance matrix** c√≥ kh√°c ƒëi m·ªôt ch√∫t. Tr∆∞·ªõc ti√™n ta x√©t ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa to√†n b·ªô d·ªØ li·ªáu:
\\[
\mathbf{S}\_T \sum_{n=1}^N (\mathbf{x}\_n - \mathbf{m}) (\mathbf{x}\_n - \mathbf{m})^T ~~~ (13)
\\]
v·ªõi \\(\mathbf{m}\\) l√† vector k·ª≥ v·ªçng c·ªßa to√†n b·ªô d·ªØ li·ªáu:
\\[
\mathbf{m} = \frac{1}{N} \sum_{n=1}^N\mathbf{x}\_n = \frac{1}{N}\sum\_{k=1}^CN_k\mathbf{m}\_k ~~~ (14)
\\]
v√† \\(N = \sum_{k=1}^C N_k\\) l√† t·ªïng to√†n b·ªô s·ªë ƒëi·ªÉm d·ªØ li·ªáu trong t·∫≠p training.

-------------- -->

<p>M·ªôt trong nh·ªØng c√°ch x√¢y d·ª±ng h√†m m·ª•c ti√™u cho multi-class LDA ƒë∆∞·ª£c minh h·ªça trong H√¨nh 3.</p>
<hr />

<div>
<table width="100%" style="border: 0px solid white">

    <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/29_lda/multi_LDA.png" />
         </td>
        <td width="40%" style="border: 0px solid white" align="justify">
        H√¨nh 3: LDA cho multi-class classification problem. M·ª•c ƒë√≠ch c≈©ng l√† s·ª± kh√°c nhau gi·ªØa c√°c th√†nh ph·∫ßn trong 1 class (within-class) l√† nh·ªè v√† s·ª± kh√°c nhau gi·ªØa c√°c classes l√† l·ªõn. C√°c ƒëi·ªÉm d·ªØ li·ªáu c√≥ m√†u kh√°c nhau th·ªÉ hi·ªán c√°c class kh√°c nhau.

        </td>
    </tr>
</table>
</div>
<hr />

<p>ƒê·ªô ph√¢n t√°n c·ªßa m·ªôt t·∫≠p h·ª£p d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c coi nh∆∞ t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm t·ªõi vector k·ª≥ v·ªçng c·ªßa ch√∫ng. N·∫øu t·∫•t c·∫£ c√°c ƒëi·ªÉm ƒë·ªÅu g·∫ßn vector k·ª≥ v·ªçng c·ªßa ch√∫ng th√¨ ƒë·ªô ph√¢n t√°n c·ªßa t·∫≠p d·ªØ li·ªáu ƒë√≥ ƒë∆∞·ª£c coi l√† nh·ªè. Ng∆∞·ª£c l·∫°i, n·∫øu t·ªïng n√†y l√† l·ªõn, t·ª©c trung b√¨nh c√°c ƒëi·ªÉm ƒë·ªÅu xa trung t√¢m, t·∫≠p h·ª£p n√†y c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† c√≥ ƒë·ªô ph√¢n t√°n cao.</p>

<p>D·ª±a v√†o nh·∫≠n x√©t n√†y, ta c√≥ th·ªÉ x√¢y d·ª±ng c√°c ƒë·∫°i l∆∞·ª£ng:</p>

<p><a name="-within-class-nho"></a></p>

<h4 id="311-within-class-nh·ªè">3.1.1. Within-class nh·ªè</h4>
<p>Within-class variance c·ªßa class \(k\) c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh nh∆∞ sau:
\[
\begin{eqnarray}
\sigma_k^2 &amp;=&amp; \sum_{n \in \mathcal{C}_k} ||\mathbf{y}_n -\mathbf{e}_k||_F^2 = ||\mathbf{Y}_k - \mathbf{E}_k||_2^2 &amp; (15) \<br />
&amp;=&amp; ||\mathbf{W}^T (\mathbf{X}_k - \mathbf{M}_k) ||_F^2 &amp; (16)\<br />
&amp;=&amp; \text{trace}\left(\mathbf{W}^T (\mathbf{X}_k - \mathbf{M}_k)(\mathbf{X}_k - \mathbf{M}_k)^T \mathbf{W}\right)
\end{eqnarray}
\]
V·ªõi \(\mathbf{E}_k\) m·ªôt ma tr·∫≠n c√≥ c√°c c·ªôt gi·ªëng h·ªát nhau v√† b·∫±ng v·ªõi vector k·ª≥ v·ªçng \(\mathbf{e}_k\). C√≥ th·ªÉ nh·∫≠n th·∫•y \( \mathbf{E}_k = \mathbf{W}^T\mathbf{M}_k\) v·ªõi \(\mathbf{M}_k\) l√† ma tr·∫≠n c√≥ c√°c c·ªôt gi·ªëng h·ªát nhau v√† b·∫±ng v·ªõi vector k·ª≥ v·ªçng \(\mathbf{m}_k\) trong kh√¥ng gian ban ƒë·∫ßu.</p>

<p>V·∫≠y ƒë·∫°i l∆∞·ª£ng ƒëo within-class trong multi-class LDA c√≥ th·ªÉ ƒë∆∞·ª£c ƒëo b·∫±ng:
\[
\begin{eqnarray}
s_W = \sum_{k = 1}^C \sigma_k^2 &amp;=&amp; \sum_{k=1}^C \text{trace}\left(\mathbf{W}^T (\mathbf{X}_k - \mathbf{M}_k)(\mathbf{X}_k - \mathbf{M}_k)^T \mathbf{W}\right)&amp; (17)\<br />
&amp;=&amp; \text{trace}\left( \mathbf{W}^T\mathbf{S}_W \mathbf{W}\right) &amp; (18)
\end{eqnarray}
\]
v·ªõi:
\[
\mathbf{S}_W = \sum_{k=1}^C ||\mathbf{X}_k- \mathbf{M}_k ||_F^2 = \sum_{k=1}^C \sum_{n \in \mathcal{C}_k} (\mathbf{x}_n - \mathbf{m}_k)(\mathbf{x}_n - \mathbf{m}_k)^T ~~~~~(19)
\]
v√† n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† <strong>within-class covariance matrix c·ªßa multi-class LDA</strong>. Ma tr·∫≠n \(\mathbf{S}_B\) n√†y l√† m·ªôt ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng theo ƒë·ªãnh nghƒ©a.</p>

<p><a name="-between-class-lon"></a></p>

<h4 id="312-between-class-l·ªõn">3.1.2. Between-class l·ªõn</h4>
<p>Vi·ªác betwwen-class l·ªõn, nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c n·∫øu t·∫•t c·∫£ c√°c ƒëi·ªÉm trong kh√¥ng gian m·ªõi ƒë·ªÅu xa vector k·ª≥ v·ªçng chung \(\mathbf{e}\). Vi·ªác n√†y c≈©ng c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c n·∫øu c√°c vector k·ª≥ v·ªçng c·ªßa m·ªói class xa c√°c vector k·ª≥ v·ªçng chung (trong kh√¥ng gian m·ªõi). V·∫≠y ta c√≥ th·ªÉ ƒë·ªãnh nghƒ©a ƒë·∫°i l∆∞·ª£ng between-class nh∆∞ sau:
\[
s_B = \sum_{k=1}^C N_k ||\mathbf{e}_k - \mathbf{e} ||_F^2 = \sum_{k=1}^C ||\mathbf{E}_k - \mathbf{E} ||_F^2~~~~~ (20)
\]
Ta l·∫•y \(N_k\) l√†m tr·ªçng s·ªë v√¨ c√≥ th·ªÉ c√≥ nh·ªØng class c√≥ nhi·ªÅu ph·∫ßn t·ª≠ so v·ªõi c√°c classes c√≤n l·∫°i.</p>

<p>Ch√∫ √Ω r·∫±ng ma tr·∫≠n \(\mathbf{E}\) c√≥ th·ªÉ c√≥ s·ªë c·ªôt <em>linh ƒë·ªông</em>, ph·ª• thu·ªôc v√†o s·ªë c·ªôt c·ªßa ma tr·∫≠n \(\mathbf{E}_k\) m√† n√≥ ƒëi c√πng (v√† b·∫±ng \(N_k\)).</p>

<p>L·∫≠p lu·∫≠n t∆∞∆°ng t·ª± nh∆∞ \((17), (18)\), b·∫°n ƒë·ªçc c√≥ th·ªÉ ch·ª©ng minh ƒë∆∞·ª£c:
\[
s_B = \text{trace} \left(\mathbf{W}^T \mathbf{S}_B \mathbf{W} \right) ~~~ (21)
\]</p>

<p>v·ªõi:
\[
\mathbf{S}_B = \sum_{k = 1}^C (\mathbf{M}_k - \mathbf{M})(\mathbf{M}_k - \mathbf{M})^T = \sum_{k=1}^C N_k (\mathbf{m}_k - \mathbf{m})(\mathbf{m}_k - \mathbf{m})^T ~~~~~ (22)
\]</p>

<p>v√† s·ªë c·ªôt c·ªßa ma tr·∫≠n \(\mathbf{M}\) c≈©ng <em>linh ƒë·ªông</em> theo s·ªë c·ªôt c·ªßa \(\mathbf{M}_k\). Ma tr·∫≠n n√†y l√† t·ªïng c·ªßa c√°c ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng, n√™n n√≥ l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng.</p>

<p><a name="-ham-mat-mat-cho-multi-class-lda"></a></p>

<h3 id="32-h√†m-m·∫•t-m√°t-cho-multi-class-lda">3.2. H√†m m·∫•t m√°t cho multi-class LDA</h3>
<p>V·ªõi c√°ch ƒë·ªãnh nghƒ©a v√† √Ω t∆∞·ªüng v·ªÅ within-class nh·ªè v√† between-class l·ªõn nh∆∞ tr√™n, ta c√≥ th·ªÉ x√¢y d·ª±ng b√†i to√°n t·ªëi ∆∞u:</p>

<p>\[
  \mathbf{W} = \arg\max_{\mathbf{W}}J(\mathbf{W}) =  \arg\max_{\mathbf{W}} \frac{\text{trace}(\mathbf{W}^T\mathbf{S}_B\mathbf{W})}{\text{trace}(\mathbf{W}^T\mathbf{S}_W\mathbf{W})}
\]</p>

<p>Nghi·ªám c≈©ng ƒë∆∞·ª£c t√¨m b·∫±ng c√°ch gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m h√†m m·ª•c ti√™u b·∫±ng 0. Nh·∫Øc l·∫°i v·ªÅ ƒë·∫°o h√†m c·ªßa h√†m \(\text{trace}\) theo ma tr·∫≠n:
\[
  \nabla_{\mathbf{W}} \text{trace}(\mathbf{W}^T\mathbf{A} \mathbf{W}) = 2\mathbf{A}\mathbf{W}
\]
v·ªõi \(\mathbf{A} \in \mathbb{R}^{D \times D}\) l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng. (Xem trang 597 <a href="https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf">c·ªßa t√†i li·ªáu n√†y</a>).</p>

<p>V·ªõi c√°ch t√≠nh t∆∞∆°ng t·ª± nh∆∞ \((8) - (10)\), ta c√≥:
\[
\begin{eqnarray}
  \nabla_{\mathbf{W}} J(\mathbf{W}) &amp;=&amp; \frac{2 \left( \mathbf{S}_B\mathbf{W} \text{trace}(\mathbf{W}^T\mathbf{S}_W\mathbf{W}) - \text{trace}(\mathbf{W}^T\mathbf{S}_B\mathbf{W})\mathbf{S}_W \mathbf{W} \right)}{\left(\text{trace}(\mathbf{W}^T\mathbf{S}_W\mathbf{W})\right)^2}  = \mathbf{0} &amp; (23)\<br />
  \Leftrightarrow \mathbf{S}_W^{-1}\mathbf{S}_B\mathbf{W}&amp;=&amp; J \mathbf{W} &amp; (24)
\end{eqnarray}
\]</p>

<p>T·ª´ ƒë√≥ suy ra m·ªói c·ªôt c·ªßa \(\mathbf{W}\) l√† m·ªôt vector ri√™ng c·ªßa \(\mathbf{S}_W^{-1} \mathbf{S}_B\) ·ª©ng v·ªõi tr·ªã ri√™ng l·ªõn nh·∫•t c·ªßa ma tr·∫≠n n√†y.</p>

<p>Nh·∫≠n th·∫•y r·∫±ng c√°c c·ªôt c·ªßa \(\mathbf{W}\) c·∫ßn ph·∫£i ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh. V√¨ n·∫øu kh√¥ng, d·ªØ li·ªáu trong kh√¥ng gian m·ªõi \(\mathbf{y} = \mathbf{W}^T\mathbf{x}\) s·∫Ω ph·ª• thu·ªôc tuy·∫øn t√≠nh v√† c√≥ th·ªÉ ti·∫øp t·ª•c ƒë∆∞·ª£c gi·∫£m s·ªë chi·ªÅu m√† kh√¥ng ·∫£nh h∆∞·ªüng g√¨.</p>

<p>V·∫≠y c√°c c·ªôt c·ªßa \(\mathbf{W}\) l√† c√°c vector ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh ·ª©ng v·ªõi tr·ªã ri√™ng cao nh·∫•t c·ªßa \(\mathbf{S}_W^{-1} \mathbf{S}_B\). C√¢u h·ªèi ƒë·∫∑t ra l√†: C√≥ nhi·ªÅu nh·∫•t bao nhi√™u vector ri√™ng ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh ·ª©ng v·ªõi tr·ªã ri√™ng l·ªõn nh·∫•t c·ªßa \(\mathbf{S}_W^{-1} \mathbf{S}_B\)?</p>

<p>S·ªë l∆∞·ª£ng l·ªõn nh·∫•t c√°c vector ri√™ng ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh ·ª©ng v·ªõi 1 tr·ªã ri√™ng ch√≠nh l√† rank c·ªßa kh√¥ng gian ri√™ng ·ª©ng v·ªõi tr·ªã ri√™ng ƒë√≥, v√† kh√¥ng th·ªÉ l·ªõn h∆°n rank c·ªßa ma tr·∫≠n.</p>

<p>Ta c√≥ m·ªôt b·ªï ƒë·ªÅ quan tr·ªçng:</p>

<hr />

<p><strong>B·ªï ƒë·ªÅ:</strong>
\[
\text{rank}(\mathbf{S}_B) \leq C - 1 ~~~~~~ (23)
\]</p>

<p><strong>Ch·ª©ng minh:</strong> (<em>Tuy nhi√™n, vi·ªác ch·ª©ng minh n√†y kh√¥ng th·ª±c s·ª± quan tr·ªçng, ch·ªâ ph√π h·ª£p v·ªõi nh·ªØng b·∫°n mu·ªën hi·ªÉu s√¢u</em>)</p>

<p>Vi·∫øt l·∫°i \((22)\) d∆∞·ªõi d·∫°ng:
\[
\mathbf{S}_B = \mathbf{P}\mathbf{P}^T
\]
v·ªõi \(\mathbf{P} \in {R}^{D \times C}\) m√† c·ªôt th·ª© \(k\) cu·∫£ n√≥ l√†:
\[
\mathbf{p}_k = \sqrt{N_k} (\mathbf{m}_k - \mathbf{m})
\]</p>

<p>Th√™m n·ªØa, c·ªôt cu·ªëi c√πng l√† m·ªôt t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c c·ªôt c√≤n l·∫°i. L√Ω do l√†:
\[
\mathbf{m}_C - \mathbf{m} = \mathbf{m}_C - \frac{\sum_{k=1}^C N_k \mathbf{m}_k}{N} = \sum_{k=1}^{C-1} \frac{N_k}{N} (\mathbf{m}_k - \mathbf{m})
\]</p>

<p>Nh∆∞ v·∫≠y ma tr·∫≠n \(\mathbf{P}\) c√≥ nhi·ªÅu nh·∫•t \(C-1\) c·ªôt ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh, v·∫≠y n√™n rank c·ªßa n√≥ kh√¥ng v∆∞·ª£t qu√° \(C -1\).</p>

<p>Cu·ªëi c√πng, \(\mathbf{S}_B\) l√† t√≠ch c·ªßa hai ma tr·∫≠n v·ªõi rank kh√¥ng qu√° \(C-1\), n√™n \(\text{rank}(\mathbf{S}_B)\) kh√¥ng v∆∞·ª£t qu√° \(C-1\). \(~~~~~~\square\).</p>

<p>N·∫øu b·∫°n c·∫ßn √¥n l·∫°i v√†i ki·∫øn th·ª©c v·ªÅ rank:</p>

<ul>
  <li>
    <p>H·∫°ng (rank) c·ªßa m·ªôt ma tr·∫≠n, kh√¥ng nh·∫•t thi·∫øt vu√¥ng, l√† s·ªë l∆∞·ª£ng l·ªõn nh·∫•t c√°c c·ªôt ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh c·ªßa ma tr·∫≠n ƒë√≥. V·∫≠y n√™n rank c·ªßa m·ªôt ma tr·∫≠n kh√¥ng th·ªÉ l·ªõn h∆°n s·ªë c·ªôt c·ªßa ma tr·∫≠n ƒë√≥.</p>
  </li>
  <li>
    <p>\(\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A}^T)\). V·∫≠y n√™n s·ªë l∆∞·ª£ng l·ªõn nh·∫•t c√°c c·ªôt ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh c≈©ng ch√≠nh b·∫±ng s·ªë l∆∞·ª£ng l·ªõn nh·∫•t c√°c h√†ng ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh.</p>
  </li>
  <li>
    <p>\(\text{rank}(\mathbf{AB}) \leq \min \left\{\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}) \right\}\) v·ªõi \(\mathbf{A}, \mathbf{B}\) l√† hai ma tr·∫≠n b·∫•t k·ª≥ c√≥ th·ªÉ nh√¢n v·ªõi nhau ƒë∆∞·ª£c.</p>
  </li>
  <li>
    <p>\(\text{rank}(\mathbf{A} + \mathbf{B}) \leq \text{rank}(\mathbf{A}) + \text{rank}(\mathbf{B})\) v·ªõi \(\mathbf{A}, \mathbf{B}\) l√† hai ma tr·∫≠n c√πng chi·ªÅu b·∫•t k·ª≥.</p>
  </li>
</ul>

<hr />

<p>T·ª´ ƒë√≥ ra c√≥ \(\text{rank}\left(\mathbf{S}_W^{-1} \mathbf{S}_B\right) \leq \text{rank}\mathbf{S}_B \leq C - 1\).</p>

<p>V·∫≠y s·ªë chi·ªÅu c·ªßa kh√¥ng gian m·ªõi l√† m·ªôt s·ªë kh√¥ng l·ªõn h∆°n \(C-1\). Xem ra s·ªë chi·ªÅu theo multi-class LDA ƒë√£ ƒë∆∞·ª£c gi·∫£m ƒëi r·∫•t nhi·ªÅu. Nh∆∞ng ch·∫•t l∆∞·ª£ng c·ªßa d·ªØ li·ªáu m·ªõi nh∆∞ th·∫ø n√†o, ch√∫ng ta c·∫ßn l√†m m·ªôt v√†i th√≠ nghi·ªám.</p>

<p>T√≥m l·∫°i, nghi·ªám c·ªßa b√†i to√°n multi-class LDA l√† c√°c vector ri√™ng ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh ·ª©ng v·ªõi tr·ªã ri√™ng cao nh·∫•t c·ªßa \(\mathbf{S}_W^{-1} \mathbf{S}_B\).</p>

<p><strong>L∆∞u √Ω:</strong> C√≥ nhi·ªÅu c√°ch kh√°c nhau ƒë·ªÉ x√¢y d·ª±ng h√†m m·ª•c ti√™u cho multi-class LDA d·ª±a tr√™n vi·ªác ƒë·ªãnh nghƒ©a within-class variance nh·ªè v√† between-class variance l·ªõn. Ch√∫ng ta ƒëang s·ª≠ d·ª•ng h√†m \(\text{trace}\) ƒë·ªÉ ƒëong ƒë·∫øm hai ƒë·∫°i l∆∞·ª£ng n√†y. C√≥ nhi·ªÅu c√°ch kh√°c n·ªØa, v√≠ d·ª• nh∆∞ s·ª≠ d·ª•ng ƒë·ªãnh th·ª©c. Tuy nhi√™n, c√≥ m·ªôt ƒëi·ªÉm chung gi·ªØa c√°c c√°ch ti·∫øp c·∫≠n n√†y l√† chi·ªÅu c·ªßa kh√¥ng gian m·ªõi s·∫Ω kh√¥ng v∆∞·ª£t qu√° \(C-1\).</p>

<p><a name="-vi-du-tren-python"></a></p>

<h2 id="4-v√≠-d·ª•-tr√™n-python">4. V√≠ d·ª• tr√™n Python</h2>

<p><a name="-lda-voi--classes"></a></p>

<h3 id="41-lda-v·ªõi-2-classes">4.1. LDA v·ªõi 2 classes</h3>

<p>T·∫°o d·ªØ li·ªáu gi·∫£:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To support both python 2 and python 3
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>
<span class="c1"># list of points
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.backends.backend_pdf</span> <span class="kn">import</span> <span class="n">PdfPages</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">cov0</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">N0</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">N1</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">N0</span> <span class="o">+</span> <span class="n">N1</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov0</span><span class="p">,</span> <span class="n">N0</span><span class="p">)</span> <span class="c1"># each row is a data point
</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">N1</span><span class="p">)</span>
</code></pre></div></div>

<p>C√°c ƒëi·ªÉm cho 2 classes n√†y ƒë∆∞·ª£c minh ho·∫° b·ªüi c√°c ƒëi·ªÉm m√†u lam v√† ƒë·ªè tr√™n H√¨nh 4.</p>

<p>Ti·∫øp theo, ch√∫ng ta ƒëi t√≠nh c√°c within-class v√† between-class covariance matrices:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Build S_B
</span><span class="n">m0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X0</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">m0</span> <span class="o">-</span> <span class="n">m1</span><span class="p">)</span>
<span class="n">S_B</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># Build S_W
</span><span class="n">A</span> <span class="o">=</span> <span class="n">X0</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">m0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N0</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">X1</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N1</span><span class="p">))</span>

<span class="n">S_W</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>
<p>Nghi·ªám c·ªßa b√†i to√°n l√† vector ri√™ng ·ª©ng v·ªõi tr·ªã ri√™ng l·ªõn nh·∫•t c·ªßa <code class="language-plaintext highlighter-rouge">np.linalg.inv(S_W).dot(S_B)</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eig</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S_W</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">S_B</span><span class="p">))</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 0.75091074 -0.66040371]
</code></pre></div></div>

<p>ƒê∆∞·ªùng th·∫≥ng c√≥ ph∆∞∆°ng <code class="language-plaintext highlighter-rouge">w</code> ƒë∆∞·ª£c minh ho·∫° b·ªüi ƒë∆∞·ªùng m√†u l·ª•c tr√™n H√¨nh 4. Ta th·∫•y r·∫±ng nghi·ªám n√†y h·ª£p l√Ω v·ªõi d·ªØ li·ªáu c√≥ ƒë∆∞·ª£c.</p>
<hr />

<div>
<table width="100%" style="border: 0px solid white">

    <tr>
        <td width="50%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/29_lda/res.png" />
         </td>
         <td width="40%" style="border: 0px solid white" align="justify">
         H√¨nh 4: V√≠ d·ª• minh ho·∫° v·ªÅ LDA trong kh√¥ng gian 2 chi·ªÅu. ƒê∆∞·ªùng th·∫≥ng m√†u l·ª•c l√† ƒë∆∞·ªùng th·∫≥ng m√† d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chi·∫øu l√™n. Ta c√≥ th·ªÉ th·∫•y r·∫±ng, n·∫øu chi·∫øu l√™n ƒë∆∞·ª£c th·∫≥ng n√†y, d·ªØ li·ªáu c·ªßa hai classes s·∫Ω n·∫±m v·ªÅ hai ph√≠a c·ªßa m·ªôt ƒëi·ªÉm tr√™n ƒë∆∞·ªùng th·∫≥ng ƒë√≥. 
         </td>
      </tr>
</table>
</div>
<hr />

<p>ƒê·ªÉ ki·ªÉm ch·ª©ng ƒë·ªô ch√≠nh x√°c c·ªßa nghi·ªám t√¨m ƒë∆∞·ª£c, ta c√πng so s√°nh n√≥ v·ªõi nghi·ªám t√¨m ƒë∆∞·ª£c b·ªüi th∆∞ vi·ªán <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">N1</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">coef_</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">coef_</span><span class="p">))</span> <span class="c1"># normalize
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    [[ 0.75091074 -0.66040371]]
</code></pre></div></div>

<p>Ta th·∫•y r·∫±ng nghi·ªám t√¨m theo c√¥ng th·ª©c v√† nghi·ªám t√¨m theo th∆∞ vi·ªán l√† nh∆∞ nhau. Nh∆∞ v·∫≠y vi·ªác ph√¢n t√≠ch ·ªü M·ª•c 2 l√† ho√†n to√†n ch√≠nh x√°c.</p>

<!-- ### Iris Classification v·ªõi PCA v√† LDA
Ti·∫øp theo, ch√∫ng ta c√πng l√†m m·ªôt th√≠ nghi·ªám nh·ªè tr√™n [Iris dataset](/2017/01/08/knn/#bo-co-so-du-lieu-iris-iris-flower-dataset) v·ªõi 3 classes, m·ªói classes c√≥ 50 d·ªØ li·ªáu t·ªïng c·ªông, m·ªói ƒëi·ªÉm d·ªØ li·ªáu c√≥ s·ªë chi·ªÅu l√† 4.

Ch√∫ng ta s·∫Ω l·∫•y 100 ƒëi·ªÉm ra l√†m d·ªØ li·ªáu training, 50 ƒëi·ªÉm c√≤n l·∫°i l√†m d·ªØ li·ªáu test. Ta s·∫Ω d√πng 2 c√°ch kh√°c nhau, PCA v√† LDA, ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu tr∆∞·ªõc khi ƒë∆∞a d·ªØ li·ªáu m·ªõi v√†o m√¥t b·ªô ph√¢n l·ªõp [Kernel SVM](/2017/04/22/kernelsmv/).

V√¨ c√≥ 3 classes n√™n ta ch·ªçn s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu m·ªõi l√† 2 cho c√°c PCA v√† LDA.
 -->

<p>M·ªôt v√≠ d·ª• kh√°c so s√°nh PCA v√† LDA c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y t·∫°i ƒë√¢y: <a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html">Comparison of LDA and PCA 2D projection of Iris dataset</a>.</p>

<p><a name="thao-luan"></a></p>

<h2 id="5th·∫£o-lu·∫≠n">5.Th·∫£o lu·∫≠n</h2>

<ul>
  <li>
    <p>LDA l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu c√≥ s·ª≠ d·ª•ng th√¥ng tin v·ªÅ label c·ªßa d·ªØ li·ªáu. LDA l√† m·ªôt thu·∫≠t to√°n supervised.</p>
  </li>
  <li>
    <p>√ù t∆∞·ªüng c∆° b·∫£n c·ªßa LDA l√† t√¨m m·ªôt kh√¥ng gian m·ªõi v·ªõi s·ªë chi·ªÅu nh·ªè h∆°n kh√¥ng gian ban ƒë·∫ßu sao cho h√¨nh chi·∫øu c·ªßa c√°c ƒëi·ªÉm trong c√πng 1 class l√™n kh√¥ng gian m·ªõi n√†y l√† g·∫ßn nhau trong khi h√¨nh chi·∫øu c·ªßa c√°c ƒëi·ªÉm c·ªßa c√°c classes kh√°c nhau l√† kh√°c nhau.</p>
  </li>
  <li>
    <p>Trong PCA, s·ªë chi·ªÅu c·ªßa kh√¥ng gian m·ªõi c√≥ th·ªÉ l√† b·∫•t k·ª≥ s·ªë n√†o kh√¥ng l·ªõn h∆°n s·ªë chi·ªÅu v√† s·ªë ƒëi·ªÉm c·ªßa d·ªØ li·ªáu. Trong LDA, v·ªõi b√†i to√°n c√≥ \(C\) classes, s·ªë chi·ªÅu c·ªßa kh√¥ng gian m·ªõi ch·ªâ c√≥ th·ªÉ kh√¥ng v∆∞·ª£t qu√° \(C-1\).</p>
  </li>
  <li>
    <p>LDA c√≥ gi·∫£ s·ª≠ ng·∫ßm r·∫±ng d·ªØ li·ªáu c·ªßa c√°c classes ƒë·ªÅu tu√¢n theo ph√¢n ph·ªëi chu·∫©n v√† c√°c ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa c√°c classes l√† g·∫ßn nhau.</p>
  </li>
  <li>
    <p>V·ªõi b√†i to√°n c√≥ 2 classes, t·ª´ H√¨nh 1 ta c√≥ th·ªÉ th·∫•y r·∫±ng hai classes l√† linearly separable n·∫øu v√† ch·ªâ n·∫øu t·ªìn t·∫°i m·ªôt ƒë∆∞·ªùng th·∫≥ng v√† 1 ƒëi·ªÉm tr√™n ƒë∆∞·ªùng th·∫≥ng ƒë√≥ (ƒëi·ªÉm m√πa l·ª•c) sao cho: d·ªØ li·ªáu h√¨nh chi·∫øu tr√™n ƒë∆∞·ªùng th·∫≥ng c·ªßa hai classes n·∫±m v·ªÅ hai ph√≠a kh√°c nhau c·ªßa ƒëi·ªÉm ƒë√≥.</p>
  </li>
  <li>
    <p>LDA ho·∫°t ƒë·ªông r·∫•t t·ªët n·∫øu c√°c classes l√† linearly separable, tuy nhi√™n, ch·∫•t l∆∞·ª£ng m√¥ h√¨nh gi·∫£m ƒëi r√µ r·ªát n·∫øu c√°c classes l√† kh√¥ng linearly separable. ƒêi·ªÅu n√†y d·ªÖ hi·ªÉu v√¨ khi ƒë√≥, chi·∫øu d·ªØ li·ªáu l√™n ph∆∞∆°ng n√†o th√¨ c≈©ng b·ªã ch·ªìng l·∫ßn, v√† vi·ªác t√°ch bi·ªát kh√¥ng th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c nh∆∞ ·ªü kh√¥ng gian ban ƒë·∫ßu.</p>
  </li>
  <li>
    <p>M·∫∑c d√π c√≥ h·∫°n ch·∫ø, √Ω t∆∞·ªüng v·ªÅ <em>small within-class</em> v√† <em>large between-class</em> ƒë∆∞·ª£c s·ª≠ d·ª•ng r·∫•t nhi·ªÅu trong c√°c m√¥ h√¨nh classification kh√°c. V√≠ d·ª• <a href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/conf/iccv11/FDDL_ICCV_final.pdf">Fisher discrimination dictionary learning for sparse representation - ICCV 2011</a>.</p>
  </li>
</ul>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-t√†i-li·ªáu-tham-kh·∫£o">6. T√†i li·ªáu tham kh·∫£o</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear discriminant analysis</a>.</p>

<p>[2] Bishop, Christopher M. ‚ÄúPattern recognition and Machine Learning.‚Äù, Springer (2006). Chapter 4. (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[3] <a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html">Comparison of LDA and PCA 2D projection of Iris dataset</a></p>
:ET