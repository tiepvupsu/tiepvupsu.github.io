I"`§<div class="imgcap">
 <img src="http://sebastianruder.com/content/images/2016/09/contours_evaluation_optimizers.gif" align="center" width="400" />
 <div class="thecap"> Tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a cÃ¡c thuáº­t toÃ¡n GD khÃ¡c nhau. (Nguá»“n <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent"></a> An overview of gradient descent optimization algorithms). </div>
</div>

<p>Trong <a href="/2017/01/12/gradientdescent/">pháº§n 1</a> cá»§a Gradient Descent (GD), tÃ´i Ä‘Ã£ giá»›i thiá»‡u vá»›i báº¡n Ä‘á»c vá» thuáº­t toÃ¡n Gradient Descent. TÃ´i xin nháº¯c láº¡i ráº±ng nghiá»‡m cuá»‘i cÃ¹ng cá»§a Gradient Descent phá»¥ thuá»™c ráº¥t nhiá»u vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o vÃ  learning rate. Trong bÃ i nÃ y, tÃ´i xin Ä‘á» cáº­p má»™t vÃ i phÆ°Æ¡ng phÃ¡p thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ kháº¯c phá»¥c nhá»¯ng háº¡n cháº¿ cá»§a GD. Äá»“ng thá»i, cÃ¡c thuáº­t toÃ¡n biáº¿n thá»ƒ cá»§a GD thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh Deep Learning cÅ©ng sáº½ Ä‘Æ°á»£c tá»•ng há»£p.</p>

<p><strong>Trong trang nÃ y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-cac-thuat-toan-toi-uu-gradient-descent">1. CÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u Gradient Descent</a>
    <ul>
      <li><a href="#-momentum">1.1 Momentum</a>
        <ul>
          <li><a href="#nhac-lai-thuat-toan-gradient-descent">Nháº¯c láº¡i thuáº­t toÃ¡n Gradient Descent</a></li>
          <li><a href="#gradient-duoi-goc-nhin-vat-ly">Gradient dÆ°á»›i gÃ³c nhÃ¬n váº­t lÃ½</a></li>
          <li><a href="#gradient-descent-voi-momentum">Gradient Descent vá»›i Momentum</a></li>
          <li><a href="#mot-vi-du-nho">Má»™t vÃ­ dá»¥ nhá»</a></li>
        </ul>
      </li>
      <li><a href="#-nesterov-accelerated-gradient-nag">1.2. Nesterov accelerated gradient (NAG)</a>
        <ul>
          <li><a href="#y-tuong-chinh">Ã tÆ°á»Ÿng chÃ­nh</a></li>
          <li><a href="#cong-thuc-cap-nhat">CÃ´ng thá»©c cáº­p nháº­t</a></li>
          <li><a href="#vi-du-minh-hoa">VÃ­ dá»¥ minh há»a</a></li>
        </ul>
      </li>
      <li><a href="#-cac-thuat-toan-khac">1.3. CÃ¡c thuáº­t toÃ¡n khÃ¡c</a></li>
    </ul>
  </li>
  <li><a href="#-bien-the-cua-gradient-descent">2. Biáº¿n thá»ƒ cá»§a Gradient Descent</a>
    <ul>
      <li><a href="#-batch-gradient-descent">2.1. Batch Gradient Descent</a></li>
      <li><a href="#-stochastic-gradient-descent">2.2. Stochastic Gradient Descent.</a>
        <ul>
          <li><a href="#vi-du-voi-bai-toan-linear-regression">VÃ­ dá»¥ vá»›i bÃ i toÃ¡n Linear Regression</a></li>
        </ul>
      </li>
      <li><a href="#-mini-batch-gradient-descent">2.3. Mini-batch Gradient Descent</a></li>
    </ul>
  </li>
  <li><a href="#-stopping-criteria-dieu-kien-dung">3. Stopping Criteria (Ä‘iá»u kiá»‡n dá»«ng)</a></li>
  <li><a href="#-mot-phuong-phap-toi-uu-don-gian-khac-newtons-method">4. Má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u Ä‘Æ¡n giáº£n khÃ¡c: Newtonâ€™s method</a>
    <ul>
      <li><a href="#newtons-method-cho-giai-phuong-trinh-\\fx--\\">Newtonâ€™s method cho giáº£i phÆ°Æ¡ng trÃ¬nh \(f(x) = 0\)</a></li>
      <li><a href="#newtons-method-trong-bai-toan-tim-local-minimun">Newtonâ€™s method trong bÃ i toÃ¡n tÃ¬m local minimun</a></li>
      <li><a href="#han-che-cua-newtons-method">Háº¡n cháº¿ cá»§a Newtonâ€™s method</a></li>
    </ul>
  </li>
  <li><a href="#-ket-luan">5. Káº¿t luáº­n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. TÃ i liá»‡u tham kháº£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-cac-thuat-toan-toi-uu-gradient-descent"></a></p>

<h2 id="1-cÃ¡c-thuáº­t-toÃ¡n-tá»‘i-Æ°u-gradient-descent">1. CÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u Gradient Descent</h2>

<p><a name="-momentum"></a></p>

<h3 id="11-momentum">1.1 Momentum</h3>
<p><a name="nhac-lai-thuat-toan-gradient-descent"></a></p>

<h4 id="nháº¯c-láº¡i-thuáº­t-toÃ¡n-gradient-descent">Nháº¯c láº¡i thuáº­t toÃ¡n Gradient Descent</h4>
<p>DÃ nh cho cÃ¡c báº¡n chÆ°a Ä‘á»c <a href="/2017/01/12/gradientdescent/">pháº§n 1</a> cá»§a Gradient Descent. Äá»ƒ giáº£i bÃ i toÃ¡n tÃ¬m Ä‘iá»ƒm <em>global optimal</em> cá»§a hÃ m máº¥t mÃ¡t \(J(\theta)\) (HÃ m máº¥t mÃ¡t cÅ©ng thÆ°á»ng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  \(J()\) vá»›i \(\theta\) lÃ  táº­p há»£p cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh), tÃ´i xin nháº¯c láº¡i thuáº­t toÃ¡n GD:</p>

<hr />
<p><strong>Thuáº­t toÃ¡n Gradient Descent:</strong></p>

<ol>
  <li>Dá»± Ä‘oÃ¡n má»™t Ä‘iá»ƒm khá»Ÿi táº¡o \(\theta = \theta_0\).</li>
  <li>Cáº­p nháº­t \(\theta\) Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ cháº¥p nháº­n Ä‘Æ°á»£c: 
\[
\theta = \theta - \eta \nabla_{\theta}J(\theta)
\]</li>
</ol>

<p><a name="voi-\\\nabla\thetaj\theta\\-la-dao-ham-cua-ham-mat-mat-tai-\\\theta\\"></a></p>

<p>vá»›i \(\nabla_{\theta}J(\theta)\) lÃ  Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t táº¡i \(\theta\).</p>

<hr />

<p><a name="gradient-duoi-goc-nhin-vat-ly"></a></p>

<h4 id="gradient-dÆ°á»›i-gÃ³c-nhÃ¬n-váº­t-lÃ½">Gradient dÆ°á»›i gÃ³c nhÃ¬n váº­t lÃ½</h4>

<p>Thuáº­t toÃ¡n GD thÆ°á»ng Ä‘Æ°á»£c vÃ­ vá»›i tÃ¡c dá»¥ng cá»§a trá»ng lá»±c lÃªn má»™t hÃ²n bi Ä‘áº·t trÃªn má»™t máº·t cÃ³ dáº¡ng nhÆ° hÃ¬nh má»™t thung lÅ©ng giá»‘ng nhÆ° hÃ¬nh 1a) dÆ°á»›i Ä‘Ã¢y. Báº¥t ká»ƒ ta Ä‘áº·t hÃ²n bi á»Ÿ A hay B thÃ¬ cuá»‘i cÃ¹ng hÃ²n bi cÅ©ng sáº½ lÄƒn xuá»‘ng vÃ  káº¿t thÃºc á»Ÿ vá»‹ trÃ­ C.</p>

<div class="imgcap">
 <img src="/assets/GD/momentum.png" align="center" width="800" />
 <div class="thecap"> HÃ¬nh 1: So sÃ¡nh Gradient Descent vá»›i cÃ¡c hiá»‡n tÆ°á»£ng váº­t lÃ½ </div>
</div>

<p>Tuy nhiÃªn, náº¿u nhÆ° bá» máº·t cÃ³ hai Ä‘Ã¡y thung lÅ©ng nhÆ° HÃ¬nh 1b) thÃ¬ tÃ¹y vÃ o viá»‡c Ä‘áº·t bi á»Ÿ A hay B, vá»‹ trÃ­ cuá»‘i cÃ¹ng cá»§a bi sáº½ á»Ÿ C hoáº·c D. Äiá»ƒm D lÃ  má»™t Ä‘iá»ƒm local minimum chÃºng ta khÃ´ng mong muá»‘n.</p>

<p>Náº¿u suy nghÄ© má»™t cÃ¡ch váº­t lÃ½ hÆ¡n, váº«n trong HÃ¬nh 1b), náº¿u váº­n tá»‘c ban Ä‘áº§u cá»§a bi khi á»Ÿ Ä‘iá»ƒm B Ä‘á»§ lá»›n, khi bi lÄƒn Ä‘áº¿n Ä‘iá»ƒm D, theo <em>Ä‘Ã </em>, bi cÃ³ thá»ƒ tiáº¿p tá»¥c di chuyá»ƒn lÃªn dá»‘c phÃ­a bÃªn trÃ¡i cá»§a D. VÃ  náº¿u giáº£ sá»­ váº­n tá»‘c ban Ä‘áº§u lá»›n hÆ¡n ná»¯a, bi cÃ³ thá»ƒ vÆ°á»£t dá»‘c tá»›i Ä‘iá»ƒm E rá»“i lÄƒn xuá»‘ng C nhÆ° trong HÃ¬nh 1c). ÄÃ¢y chÃ­nh lÃ  Ä‘iá»u chÃºng ta mong muá»‘n. Báº¡n Ä‘á»c cÃ³ thá»ƒ Ä‘áº·t cÃ¢u há»i ráº±ng liá»‡u bi lÄƒn tá»« A tá»›i C cÃ³ theo <em>Ä‘Ã </em> lÄƒn tá»›i E rá»“i tá»›i D khÃ´ng. Xin tráº£ lá»i ráº±ng Ä‘iá»u nÃ y khÃ³ xáº£y ra hÆ¡n vÃ¬ náº¿u so vá»›i dá»‘c DE thÃ¬ dá»‘c CE cao hÆ¡n nhiá»u.</p>

<p>Dá»±a trÃªn hiá»‡n tÆ°á»£ng nÃ y, má»™t thuáº­t toÃ¡n Ä‘Æ°á»£c ra Ä‘á»i nháº±m kháº¯c phá»¥c viá»‡c nghiá»‡m cá»§a GD rÆ¡i vÃ o má»™t Ä‘iá»ƒm local minimum khÃ´ng mong muá»‘n. Thuáº­t toÃ¡n Ä‘Ã³ cÃ³ tÃªn lÃ  Momentum (tá»©c <em>theo Ä‘Ã </em> trong tiáº¿ng Viá»‡t).</p>

<p><a name="gradient-descent-voi-momentum"></a></p>

<h4 id="gradient-descent-vá»›i-momentum">Gradient Descent vá»›i Momentum</h4>
<p>Äá»ƒ biá»ƒu diá»…n <em>momentum</em> báº±ng toÃ¡n há»c thÃ¬ chÃºng ta pháº£i lÃ m tháº¿ nÃ o?</p>

<p>Trong GD, chÃºng ta cáº§n tÃ­nh lÆ°á»£ng thay Ä‘á»•i á»Ÿ thá»i Ä‘iá»ƒm \(t\) Ä‘á»ƒ cáº­p nháº­t vá»‹ trÃ­ má»›i cho nghiá»‡m (tá»©c <em>hÃ²n bi</em>). Náº¿u chÃºng ta coi Ä‘áº¡i lÆ°á»£ng nÃ y nhÆ° váº­n tá»‘c \(v_t\) trong váº­t lÃ½, vá»‹ trÃ­ má»›i cá»§a <em>hÃ²n bi</em> sáº½ lÃ  \(\theta_{t+1} = \theta_{t} - v_t\). Dáº¥u trá»« thá»ƒ hiá»‡n viá»‡c pháº£i di chuyá»ƒn ngÆ°á»£c vá»›i Ä‘áº¡o hÃ m. CÃ´ng viá»‡c cá»§a chÃºng ta bÃ¢y giá» lÃ  tÃ­nh Ä‘áº¡i lÆ°á»£ng \(v_t\) sao cho nÃ³ vá»«a mang thÃ´ng tin cá»§a <em>Ä‘á»™ dá»‘c</em> (tá»©c Ä‘áº¡o hÃ m), vá»«a mang thÃ´ng tin cá»§a <em>Ä‘Ã </em>, tá»©c váº­n tá»‘c trÆ°á»›c Ä‘Ã³ \(v_{t-1}\) (chÃºng ta coi nhÆ° váº­n tá»‘c ban Ä‘áº§u \(v_0=0\)). Má»™t cÃ¡ch Ä‘Æ¡n giáº£n nháº¥t, ta cÃ³ thá»ƒ cá»™ng (cÃ³ trá»ng sá»‘) hai Ä‘áº¡i lÆ°á»£ng nÃ y láº¡i:
\[
v_{t}= \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta)
\]</p>

<p>Trong Ä‘Ã³ \(\gamma\) thÆ°á»ng Ä‘Æ°á»£c chá»n lÃ  má»™t giÃ¡ trá»‹ khoáº£ng 0.9, \(v_t\) lÃ  váº­n tá»‘c táº¡i thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³, \( \nabla_{\theta}J(\theta)\) chÃ­nh lÃ  Ä‘á»™ dá»‘c cá»§a Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³. 
Sau Ä‘Ã³ vá»‹ trÃ­ má»›i cá»§a <em>hÃ²n bi</em> Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh nhÆ° sau:
\[
\theta = \theta - v_t
\]</p>

<p>Thuáº­t toÃ¡n Ä‘Æ¡n giáº£n nÃ y tá» ra ráº¥t hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿ (trong khÃ´ng gian nhiá»u chiá»u, cÃ¡ch tÃ­nh toÃ¡n cÅ©ng hoÃ n tÃ²an tÆ°Æ¡ng tá»±). DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ trong khÃ´ng gian má»™t chiá»u.</p>

<p><a name="mot-vi-du-nho"></a></p>

<h4 id="má»™t-vÃ­-dá»¥-nhá»">Má»™t vÃ­ dá»¥ nhá»</h4>
<p>ChÃºng ta xem xÃ©t má»™t hÃ m Ä‘Æ¡n giáº£n cÃ³ hai Ä‘iá»ƒm local minimum, trong Ä‘Ã³ 1 Ä‘iá»ƒm lÃ  global minimum:
\[
f(x) = x^2 + 10\sin(x)
\]
CÃ³ Ä‘áº¡o hÃ m lÃ : \(fâ€™(x) = 2x + 10\cos(x)\). HÃ¬nh 2 dÆ°á»›i Ä‘Ã¢y thá»ƒ hiá»‡n sá»± khÃ¡c nhau giá»¯a thuáº­t toÃ¡n GD vÃ  thuáº­t toÃ¡n GD vá»›i Momentum:</p>

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/GD/nomomentum1d.gif" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/GD/momentum1d.gif" />
        </td>
    </tr>
</table> 
<div class="thecap"> HÃ¬nh 2: Minh há»a thuáº­t toÃ¡n GD vá»›i Momentum. </div>
</div>

<p>HÃ¬nh bÃªn trÃ¡i lÃ  Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m khi khÃ´ng sá»­ dá»¥ng Momentum, thuáº­t toÃ¡n há»™i tá»¥ sau chá»‰ 5 vÃ²ng láº·p nhÆ°ng nghiá»‡m tÃ¬m Ä‘Æ°á»£c lÃ  nghiá»‡m local minimun.</p>

<p>HÃ¬nh bÃªn pháº£i lÃ  Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m khi cÃ³ sá»­ dá»¥ng Momentum, <em>hÃ²n bi</em> Ä‘Ã£ cÃ³ thá»ƒ vÆ°á»£t dá»‘c tá»›i khu vá»±c gáº§n Ä‘iá»ƒm global minimun, sau Ä‘Ã³ dao Ä‘á»™ng xung quanh Ä‘iá»ƒm nÃ y, giáº£m tá»‘c rá»“i cuá»‘i cÃ¹ng tá»›i Ä‘Ã­ch. Máº·c dÃ¹ máº¥t nhiá»u vÃ²ng láº·p hÆ¡n, GD vá»›i Momentum cho chÃºng ta nghiá»‡m chÃ­nh xÃ¡c hÆ¡n. Quan sÃ¡t Ä‘Æ°á»ng Ä‘i cá»§a <em>hÃ²n bi</em> trong trÆ°á»ng há»£p nÃ y, chÃºng ta tháº¥y ráº±ng Ä‘iá»u nÃ y giá»‘ng vá»›i váº­t lÃ½ hÆ¡n!</p>

<p>Náº¿u biáº¿t trÆ°á»›c Ä‘iá»ƒm <em>Ä‘áº·t bi</em> ban Ä‘áº§u <code class="language-plaintext highlighter-rouge">theta</code>, Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t táº¡i má»™t Ä‘iá»ƒm báº¥t ká»³ <code class="language-plaintext highlighter-rouge">grad(theta)</code>, lÆ°á»£ng thÃ´ng tin lÆ°u trá»¯ tá»« váº­n tá»‘c trÆ°á»›c Ä‘Ã³ <code class="language-plaintext highlighter-rouge">gamma</code> vÃ  learning rate <code class="language-plaintext highlighter-rouge">eta</code>, chÃºng ta cÃ³ thá»ƒ viáº¿t hÃ m sá»‘ <code class="language-plaintext highlighter-rouge">GD_momentum</code> trong Python nhÆ° sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check convergence
</span><span class="k">def</span> <span class="nf">has_converged</span><span class="p">(</span><span class="n">theta_new</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">theta_new</span><span class="p">))</span><span class="o">/</span>
                            <span class="nb">len</span><span class="p">(</span><span class="n">theta_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-3</span>

<span class="k">def</span> <span class="nf">GD_momentum</span><span class="p">(</span><span class="n">theta_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="c1"># Suppose we want to store history of theta
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta_init</span><span class="p">]</span>
    <span class="n">v_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta_init</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">v_new</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">*</span><span class="n">v_old</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">theta_new</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">v_new</span>
        <span class="k">if</span> <span class="n">has_converged</span><span class="p">(</span><span class="n">theta_new</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
            <span class="k">break</span> 
        <span class="n">theta</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta_new</span><span class="p">)</span>
        <span class="n">v_old</span> <span class="o">=</span> <span class="n">v_new</span>
    <span class="k">return</span> <span class="n">theta</span> 
    <span class="c1"># this variable includes all points in the path
</span>    <span class="c1"># if you just want the final answer, 
</span>    <span class="c1"># use `return theta[-1]`
</span></code></pre></div></div>

<p><a name="-nesterov-accelerated-gradient-nag"></a></p>

<h3 id="12-nesterov-accelerated-gradient-nag">1.2. Nesterov accelerated gradient (NAG)</h3>

<p>Momentum giÃºp <em>hÃ²n bi</em> vÆ°á»£t qua Ä‘Æ°á»£c <em>dá»‘c locaminimum</em>, tuy nhiÃªn, cÃ³ má»™t háº¡n cháº¿ chÃºng ta cÃ³ thá»ƒ tháº¥y trong vÃ­ dá»¥ trÃªn: Khi tá»›i gáº§n <em>Ä‘Ã­ch</em>, momemtum váº«n máº¥t khÃ¡ nhiá»u thá»i gian trÆ°á»›c khi dá»«ng láº¡i. LÃ½ do láº¡i cÅ©ng chÃ­nh lÃ  vÃ¬ cÃ³ <em>Ä‘Ã </em>. CÃ³ má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c tiáº¿p tá»¥c giÃºp kháº¯c phá»¥c Ä‘iá»u nÃ y, phÆ°Æ¡ng phÃ¡p Ä‘Ã³ mang tÃªn Nesterov accelerated gradient (NAG), giÃºp cho thuáº­t toÃ¡n há»™i tá»¥ nhanh hÆ¡n.</p>

<p><a name="y-tuong-chinh"></a></p>

<h4 id="Ã½-tÆ°á»Ÿng-chÃ­nh">Ã tÆ°á»Ÿng chÃ­nh</h4>

<p>Ã tÆ°á»Ÿng cÆ¡ báº£n lÃ  <em>dá»± Ä‘oÃ¡n hÆ°á»›ng Ä‘i trong tÆ°Æ¡ng lai</em>, tá»©c nhÃ¬n trÆ°á»›c má»™t bÆ°á»›c! Cá»¥ thá»ƒ, náº¿u sá»­ dá»¥ng sá»‘ háº¡ng <em>momentum</em> \(\gamma v_{t-1}\) Ä‘á»ƒ cáº­p nháº­t thÃ¬ ta cÃ³ thá»ƒ <em>xáº¥p xá»‰</em> Ä‘Æ°á»£c vá»‹ trÃ­ tiáº¿p theo cá»§a hÃ²n bi lÃ  \(\theta - \gamma v_{t-1}\) (chÃºng ta khÃ´ng Ä‘Ã­nh kÃ¨m pháº§n gradient á»Ÿ Ä‘Ã¢y vÃ¬ sáº½ sá»­ dá»¥ng nÃ³ trong bÆ°á»›c cuá»‘i cÃ¹ng). Váº­y, thay vÃ¬ sá»­ dá»¥ng gradient cá»§a Ä‘iá»ƒm hiá»‡n táº¡i, NAG <em>Ä‘i trÆ°á»›c má»™t bÆ°á»›c</em>, sá»­ dá»¥ng gradient cá»§a Ä‘iá»ƒm tiáº¿p theo. Theo dÃµi hÃ¬nh dÆ°á»›i Ä‘Ã¢y:</p>

<div class="imgcap">
 <img src="/assets/GD/nesterov.jpeg" align="center" width="800" />
 <div class="thecap"> Ã tÆ°á»Ÿng cá»§a Nesterov accelerated gradient. (Nguá»“n: <a href="http://cs231n.github.io/neural-networks-3/">CS231n Stanford: Convolutional Neural Networks for Visual Recognition</a>) </div>
</div>

<ul>
  <li>
    <p>Vá»›i momentum thÃ´ng thÆ°á»ng: <em>lÆ°á»£ng thay Ä‘á»•i</em> lÃ  tá»•ng cá»§a hai vector: momentum vector vÃ  gradient á»Ÿ thá»i Ä‘iá»ƒm hiá»‡n táº¡i.</p>
  </li>
  <li>
    <p>Vá»›i Nesterove momentum: <em>lÆ°á»£ng thay Ä‘á»•i</em> lÃ  tá»•ng cá»§a hai vector: momentum vector vÃ  gradient á»Ÿ thá»i Ä‘iá»ƒm Ä‘Æ°á»£c xáº¥p xá»‰ lÃ  Ä‘iá»ƒm tiáº¿p theo.</p>
  </li>
</ul>

<p><a name="cong-thuc-cap-nhat"></a></p>

<h4 id="cÃ´ng-thá»©c-cáº­p-nháº­t">CÃ´ng thá»©c cáº­p nháº­t</h4>

<p>CÃ´ng thá»©c cáº­p nháº­t cá»§a NAG Ä‘Æ°á»£c cho nhÆ° sau:</p>

<p>\[
\begin{eqnarray}
v_{t} &amp;=&amp; \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta - \gamma v_{t-1}) \<br />
\theta &amp;=&amp; \theta -  v_{t}
\end{eqnarray}
\]</p>

<p>Äá»ƒ Ã½ má»™t chÃºt cÃ¡c báº¡n sáº½ tháº¥y Ä‘iá»ƒm Ä‘Æ°á»£c tÃ­nh Ä‘áº¡o hÃ m Ä‘Ã£ thay Ä‘á»•i.</p>

<p><a name="vi-du-minh-hoa"></a></p>

<h4 id="vÃ­-dá»¥-minh-há»a">VÃ­ dá»¥ minh há»a</h4>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ so sÃ¡nh Momentum vÃ  NAG cho bÃ i toÃ¡n Linear Regression:</p>

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/GD/LR_momentum_contours.gif" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/GD/LR_NAG_contours.gif" />
        </td>
    </tr>
</table> 
<div class="thecap"> Minh há»a thuáº­t toÃ¡n GD vá»›i Momentum vÃ  NAG. </div>
</div>

<p>HÃ¬nh bÃªn trÃ¡i lÃ  Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m vá»›i phÆ°Æ¡ng phÃ¡p Momentum. nghiá»‡m Ä‘i khÃ¡ lÃ  <em>zigzag</em> vÃ  máº¥t nhiá»u vÃ²ng láº·p hÆ¡n. HÃ¬nh bÃªn pháº£i lÃ  Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m vá»›i phÆ°Æ¡ng phÃ¡p NAG, nghiá»‡m há»™i tá»¥ nhanh hÆ¡n, vÃ  Ä‘Æ°á»ng Ä‘i Ã­t <em>zigzag</em> hÆ¡n.</p>

<p>(Source code cho <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/LR%20Momentum.ipynb">hÃ¬nh bÃªn trÃ¡i</a> vÃ  <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/LR%20NAG.ipynb"> hÃ¬nh bÃªn pháº£i</a>).</p>

<p><a name="-cac-thuat-toan-khac"></a></p>

<h3 id="13-cÃ¡c-thuáº­t-toÃ¡n-khÃ¡c">1.3. CÃ¡c thuáº­t toÃ¡n khÃ¡c</h3>
<p>NgoÃ i hai thuáº­t toÃ¡n trÃªn, cÃ³ ráº¥t nhiá»u thuáº­t toÃ¡n nÃ¢ng cao khÃ¡c Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿, Ä‘áº·c biá»‡t lÃ  cÃ¡c bÃ i toÃ¡n Deep Learning. CÃ³ thá»ƒ nÃªu má»™t vÃ i tá»« khÃ³a nhÆ° Adagrad, Adam, RMSprop,â€¦ TÃ´i sáº½ khÃ´ng Ä‘á» cáº­p Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n Ä‘Ã³ trong bÃ i nÃ y mÃ  sáº½ dÃ nh thá»i gian nÃ³i tá»›i náº¿u cÃ³ dá»‹p trong tÆ°Æ¡ng lai, khi blog Ä‘Ã£ Ä‘á»§ lá»›n vÃ  Ä‘Ã£ trang bá»‹ cho cÃ¡c báº¡n má»™t lÆ°á»£ng kiáº¿n thá»©c nháº¥t Ä‘á»‹nh.</p>

<p>Tuy nhiÃªn, báº¡n Ä‘á»c nÃ o muá»‘n Ä‘á»c thÃªm cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c ráº¥t nhiá»u thÃ´ng tin há»¯u Ã­ch trong bÃ i nÃ y:
<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">An overview of gradient descent optimization algorithms </a>.</p>

<p><a name="-bien-the-cua-gradient-descent"></a></p>

<h2 id="2-biáº¿n-thá»ƒ-cá»§a-gradient-descent">2. Biáº¿n thá»ƒ cá»§a Gradient Descent</h2>
<p>TÃ´i xin má»™t láº§n ná»¯a dÃ¹ng bÃ i toÃ¡n <a href="/2016/12/28/linearregression/">Linear Regression</a> lÃ m vÃ­ dá»¥. HÃ m máº¥t mÃ¡t vÃ  Ä‘áº¡o hÃ m cá»§a nÃ³ cho bÃ i toÃ¡n nÃ y láº§n lÆ°á»£t lÃ  (Ä‘á»ƒ cho thuáº­n tiá»‡n, trong bÃ i nÃ y tÃ´i sáº½ dÃ¹ng kÃ½ hiá»‡u \(\mathbf{X}\) thay cho dá»¯ liá»‡u má»Ÿ rá»™ng \(\bar{\mathbf{X}}\)):</p>

<p>\[
J(\mathbf{w}) = \frac{1}{2N}||\mathbf{X}\mathbf{w} - \mathbf{y}||_2^2
\]
\[
~~~~ = \frac{1}{2N} \sum_{i=1}^N(\mathbf{x}_i \mathbf{w} - y_i)^2
\]
vÃ :
\[
\nabla_{\mathbf{w}} J(\mathbf{w}) = \frac{1}{N}\sum_{i=1}^N \mathbf{x}_i^T(\mathbf{x}_i\mathbf{w} - y_i)
\]</p>

<p><a name="-batch-gradient-descent"></a></p>

<h3 id="21-batch-gradient-descent">2.1. Batch Gradient Descent</h3>
<p>Thuáº­t toÃ¡n Gradient Descent chÃºng ta nÃ³i tá»« Ä‘áº§u pháº§n 1 Ä‘áº¿n giá» cÃ²n Ä‘Æ°á»£c gá»i lÃ  Batch Gradient Descent. Batch á»Ÿ Ä‘Ã¢y Ä‘Æ°á»£c hiá»ƒu lÃ  <em>táº¥t cáº£</em>, tá»©c khi cáº­p nháº­t \(\theta = \mathbf{w}\), chÃºng ta sá»­ dá»¥ng <strong>táº¥t cáº£</strong> cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u \(\mathbf{x}_i\).</p>

<p>CÃ¡ch lÃ m nÃ y cÃ³ má»™t vÃ i háº¡n cháº¿ Ä‘á»‘i vá»›i cÆ¡ sá»Ÿ dá»¯ liá»‡u cÃ³ vÃ´ cÃ¹ng nhiá»u Ä‘iá»ƒm (hÆ¡n 1 tá»‰ ngÆ°á»i dÃ¹ng cá»§a facebook cháº³ng háº¡n). Viá»‡c pháº£i tÃ­nh toÃ¡n láº¡i Ä‘áº¡o hÃ m vá»›i táº¥t cáº£ cÃ¡c Ä‘iá»ƒm nÃ y sau má»—i vÃ²ng láº·p trá»Ÿ nÃªn cá»“ng ká»nh vÃ  khÃ´ng hiá»‡u quáº£. ThÃªm ná»¯a, thuáº­t toÃ¡n nÃ y Ä‘Æ°á»£c coi lÃ  khÃ´ng hiá»‡u quáº£ vá»›i <em>online learning</em>.</p>

<p><a name="online-learning"></a></p>

<p><strong>Online learning</strong> lÃ  khi cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c (thÃªm ngÆ°á»i dÃ¹ng Ä‘Äƒng kÃ½ hÃ ng ngÃ y cháº³ng háº¡n), má»—i láº§n thÃªm vÃ i Ä‘iá»ƒm dá»¯ liá»‡u má»›i. KÃ©o theo Ä‘Ã³ lÃ  mÃ´ hÃ¬nh cá»§a chÃºng ta cÅ©ng pháº£i thay Ä‘á»•i má»™t chÃºt Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c dá»¯ liá»‡u má»›i nÃ y. Náº¿u lÃ m theo Batch Gradient Descent, tá»©c tÃ­nh láº¡i Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t táº¡i táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, thÃ¬ thá»i gian tÃ­nh toÃ¡n sáº½ ráº¥t lÃ¢u, vÃ  thuáº­t toÃ¡n cá»§a chÃºng ta coi nhÆ° khÃ´ng <em>online</em> ná»¯a do máº¥t quÃ¡ nhiá»u thá»i gian tÃ­nh toÃ¡n.</p>

<p>TrÃªn thá»±c táº¿, cÃ³ má»™t thuáº­t toÃ¡n Ä‘Æ¡n giáº£n hÆ¡n vÃ  tá» ra ráº¥t hiá»‡u quáº£, cÃ³ tÃªn gá»i lÃ  Stochastic Gradient Descent (SGD).
<a name="-stochastic-gradient-descent"></a></p>

<h3 id="22-stochastic-gradient-descent">2.2. Stochastic Gradient Descent.</h3>
<p>Trong thuáº­t toÃ¡n nÃ y, táº¡i 1 thá»i Ä‘iá»ƒm, ta chá»‰ tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t dá»±a trÃªn <em>chá»‰ má»™t</em> Ä‘iá»ƒm dá»¯ liá»‡u \(\mathbf{x_i}\) rá»“i cáº­p nháº­t \(\theta\) dá»±a trÃªn Ä‘áº¡o hÃ m nÃ y. Viá»‡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i tá»«ng Ä‘iá»ƒm trÃªn toÃ n bá»™ dá»¯ liá»‡u, sau Ä‘Ã³ láº·p láº¡i quÃ¡ trÃ¬nh trÃªn. Thuáº­t toÃ¡n ráº¥t Ä‘Æ¡n giáº£n nÃ y trÃªn thá»±c táº¿ láº¡i lÃ m viá»‡c ráº¥t hiá»‡u quáº£.</p>

<p>Má»—i láº§n duyá»‡t má»™t lÆ°á»£t qua <em>táº¥t cáº£</em> cÃ¡c Ä‘iá»ƒm trÃªn toÃ n bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c gá»i lÃ  má»™t epoch. Vá»›i GD thÃ´ng thÆ°á»ng thÃ¬ má»—i epoch á»©ng vá»›i 1 láº§n cáº­p nháº­t \(\theta\), vá»›i SGD thÃ¬ má»—i epoch á»©ng vá»›i \(N\) láº§n cáº­p nháº­t \(\theta\) vá»›i \(N\) lÃ  sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u. NhÃ¬n vÃ o má»™t máº·t, viá»‡c cáº­p nháº­t tá»«ng Ä‘iá»ƒm má»™t nhÆ° tháº¿ nÃ y cÃ³ thá»ƒ lÃ m giáº£m Ä‘i tá»‘c Ä‘á»™ thá»±c hiá»‡n 1 epoch. NhÆ°ng nhÃ¬n vÃ o má»™t máº·t khÃ¡c, SGD chá»‰ yÃªu cáº§u má»™t lÆ°á»£ng epoch ráº¥t nhá» (thÆ°á»ng lÃ  10 cho láº§n Ä‘áº§u tiÃªn, sau Ä‘Ã³ khi cÃ³ dá»¯ liá»‡u má»›i thÃ¬ chá»‰ cáº§n cháº¡y dÆ°á»›i má»™t epoch lÃ  Ä‘Ã£ cÃ³ nghiá»‡m tá»‘t). VÃ¬ váº­y SGD phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n cÃ³ lÆ°á»£ng cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»›n (chá»§ yáº¿u lÃ  Deep Learning mÃ  chÃºng ta sáº½ tháº¥y trong pháº§n sau cá»§a blog) vÃ  cÃ¡c bÃ i toÃ¡n yÃªu cáº§u mÃ´ hÃ¬nh thay Ä‘á»•i liÃªn tá»¥c, tá»©c online learning.</p>

<p><strong>Thá»© tá»± lá»±a chá»n Ä‘iá»ƒm dá»¯ liá»‡u</strong></p>

<p>Má»™t Ä‘iá»ƒm cáº§n lÆ°u Ã½ Ä‘Ã³ lÃ : sau má»—i epoch, chÃºng ta cáº§n shuffle (xÃ¡o trá»™n) thá»© tá»± cá»§a cÃ¡c dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh ngáº«u nhiÃªn. Viá»‡c nÃ y cÅ©ng áº£nh hÆ°á»Ÿng tá»›i hiá»‡u nÄƒng cá»§a SGD.</p>

<p>Má»™t cÃ¡ch toÃ¡n há»c, quy táº¯c cáº­p nháº­t cá»§a SGD lÃ :
\[
\theta = \theta - \eta \nabla_{\theta} J(\theta; \mathbf{x}_i; \mathbf{y}_i)
\]</p>

<p>trong Ä‘Ã³ \(J(\theta; \mathbf{x}_i; \mathbf{y}_i)\) lÃ  hÃ m máº¥t mÃ¡t vá»›i chá»‰ 1 cáº·p Ä‘iá»ƒm dá»¯ liá»‡u (input, label) lÃ  (\(\mathbf{x}_i, \mathbf{y}_i\)). <strong>ChÃº Ã½:</strong> chÃºng ta hoÃ n toÃ n cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n tÄƒng tá»‘c GD nhÆ° Momentum, AdaGrad,â€¦ vÃ o SGD.</p>

<p><a name="vi-du-voi-bai-toan-linear-regression"></a></p>

<h4 id="vÃ­-dá»¥-vá»›i-bÃ i-toÃ¡n-linear-regression">VÃ­ dá»¥ vá»›i bÃ i toÃ¡n Linear Regression</h4>
<p>Vá»›i bÃ i toÃ¡n Linear Regression, \(\theta = \mathbf{w}\), hÃ m máº¥t mÃ¡t táº¡i má»™t Ä‘iá»ƒm dá»¯ liá»‡u lÃ :
\[
J(\mathbf{w}; \mathbf{x}_i; y_i) = \frac{1}{2}(\mathbf{x}_i \mathbf{w} - y_i)^2
\]
Äáº¡o hÃ m theo \(\mathbf{w}\) tÆ°Æ¡ng á»©ng lÃ :
\[
\nabla_{\mathbf{w}}J(\mathbf{w}; \mathbf{x}_i; y_i) = \mathbf{x}_i^T(\mathbf{x}_i \mathbf{w} - y_i)
\]
VÃ  dÆ°á»›i Ä‘Ã¢y lÃ  hÃ m sá»‘ trong python Ä‘á»ƒ giáº£i Linear Regression theo SGD:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># single point gradient
</span><span class="k">def</span> <span class="nf">sgrad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">rd_id</span><span class="p">):</span>
    <span class="n">true_i</span> <span class="o">=</span> <span class="n">rd_id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">Xbar</span><span class="p">[</span><span class="n">true_i</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">true_i</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">yi</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">xi</span><span class="o">*</span><span class="n">a</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">w_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_init</span><span class="p">]</span>
    <span class="n">w_last_check</span> <span class="o">=</span> <span class="n">w_init</span>
    <span class="n">iter_check_w</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># shuffle data 
</span>        <span class="n">rd_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span> 
            <span class="n">g</span> <span class="o">=</span> <span class="n">sgrad</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="n">rd_id</span><span class="p">)</span>
            <span class="n">w_new</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">g</span>
            <span class="n">w</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">count</span><span class="o">%</span><span class="n">iter_check_w</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">w_this_check</span> <span class="o">=</span> <span class="n">w_new</span>                 
                <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_this_check</span> <span class="o">-</span> <span class="n">w_last_check</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>                                    
                    <span class="k">return</span> <span class="n">w</span>
                <span class="n">w_last_check</span> <span class="o">=</span> <span class="n">w_this_check</span>
    <span class="k">return</span> <span class="n">w</span>
</code></pre></div></div>

<p>Káº¿t quáº£ Ä‘Æ°á»£c cho nhÆ° hÃ¬nh dÆ°á»›i Ä‘Ã¢y (<a href="/2017/01/12/gradientdescent/#quay-lai-voi-bai-toan-linear-regression">vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o giá»‘ng nhÆ° á»Ÿ pháº§n 1</a>).</p>

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/GD/LR_SGD_contours.gif" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/GD/LR_SGD_loss.png" />
        </td>
    </tr>
</table> 
<div class="thecap"> TrÃ¡i: Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m vá»›i SGD. Pháº£i: giÃ¡ trá»‹ cá»§a loss function táº¡i 50 vÃ²ng láº·p Ä‘áº§u tiÃªn. </div>
</div>

<p>HÃ¬nh bÃªn trÃ¡i mÃ´ táº£ Ä‘Æ°á»ng Ä‘i cá»§a nghiá»‡m. ChÃºng ta tháº¥y ráº±ng Ä‘Æ°á»ng Ä‘i khÃ¡ lÃ  <em>zigzag</em> chá»© khÃ´ng <em>mÆ°á»£t</em> nhÆ° khi sá»­ dá»¥ng GD. Äiá»u nÃ y lÃ  dá»… hiá»ƒu vÃ¬ má»™t Ä‘iá»ƒm dá»¯ liá»‡u khÃ´ng thá»ƒ Ä‘áº¡i diá»‡n cho toÃ n bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c. Tuy nhiÃªn, chÃºng ta cÅ©ng tháº¥y ráº±ng thuáº­t toÃ¡n há»™i tá»¥ khÃ¡ nhanh Ä‘áº¿n vÃ¹ng lÃ¢n cáº­n cá»§a nghiá»‡m. Vá»›i 1000 Ä‘iá»ƒm dá»¯ liá»‡u, SGD chá»‰ cáº§n gáº§n 3 epoches (2911 tÆ°Æ¡ng á»©ng vá»›i 2911 láº§n cáº­p nháº­t, má»—i láº§n láº¥y 1 Ä‘iá»ƒm). Náº¿u so vá»›i con sá»‘ 49 vÃ²ng láº·p (epoches) nhÆ° káº¿t quáº£ tá»‘t nháº¥t cÃ³ Ä‘Æ°á»£c báº±ng GD, thÃ¬ káº¿t quáº£ nÃ y lá»£i hÆ¡n ráº¥t nhiá»u.</p>

<p>HÃ¬nh bÃªn pháº£i mÃ´ táº£ hÃ m máº¥t mÃ¡t cho toÃ n bá»™ dá»¯ liá»‡u sau khi <em>chá»‰</em> sá»­ dá»¥ng 50 Ä‘iá»ƒm dá»¯ liá»‡u Ä‘áº§u tiÃªn. Máº·c dÃ¹ khÃ´ng <em>mÆ°á»£t</em>, tá»‘c Ä‘á»™ há»™i tá»¥ váº«n ráº¥t nhanh.</p>

<p><em>Thá»±c táº¿ cho tháº¥y chá»‰ láº¥y khoáº£ng 10 Ä‘iá»ƒm lÃ  ta Ä‘Ã£ cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c gáº§n Ä‘Ãºng phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng cáº§n tÃ¬m rá»“i. ÄÃ¢y chÃ­nh lÃ  Æ°u Ä‘iá»ƒm cá»§a SGD - há»™i tá»¥ ráº¥t nhanh.</em></p>

<p><a name="-mini-batch-gradient-descent"></a></p>

<h3 id="23-mini-batch-gradient-descent">2.3. Mini-batch Gradient Descent</h3>
<p>KhÃ¡c vá»›i SGD, mini-batch sá»­ dá»¥ng má»™t sá»‘ lÆ°á»£ng \(n\) lá»›n hÆ¡n 1 (nhÆ°ng váº«n nhá» hÆ¡n tá»•ng sá»‘ dá»¯ liá»‡u \(N\)ráº¥t nhiá»u). Giá»‘ng vá»›i SGD, Mini-batch Gradient Descent báº¯t Ä‘áº§u má»—i epoch báº±ng viá»‡c xÃ¡o trá»™n ngáº«u nhiÃªn dá»¯ liá»‡u rá»“i chia toÃ n bá»™ dá»¯ liá»‡u thÃ nh cÃ¡c <em>mini-batch</em>, má»—i <em>mini-batch</em> cÃ³ \(n\) Ä‘iá»ƒm dá»¯ liá»‡u (trá»« mini-batch cuá»‘i cÃ³ thá»ƒ cÃ³ Ã­t hÆ¡n náº¿u \(N\) khÃ´ng chia háº¿t cho \(n\)). Má»—i láº§n cáº­p nháº­t, thuáº­t toÃ¡n nÃ y láº¥y ra má»™t mini-batch Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº¡o hÃ m rá»“i cáº­p nháº­t. CÃ´ng thá»©c cÃ³ thá»ƒ viáº¿t dÆ°á»›i dáº¡ng:
\[
\theta = \theta - \eta\nabla_{\theta} J(\theta; \mathbf{x}_{i:i+n}; \mathbf{y}_{i:i+n})
\]
Vá»›i \(\mathbf{x}_{i:i+n}\) Ä‘Æ°á»£c hiá»ƒu lÃ  dá»¯ liá»‡u tá»« thá»© \(i\) tá»›i thá»© \(i+n-1\) (theo kÃ½ hiá»‡u cá»§a Python). Dá»¯ liá»‡u nÃ y sau má»—i epoch lÃ  khÃ¡c nhau vÃ¬ chÃºng cáº§n Ä‘Æ°á»£c xÃ¡o trá»™n. Má»™t láº§n ná»¯a, cÃ¡c thuáº­t toÃ¡n khÃ¡c cho GD nhÆ° Momentum, Adagrad, Adadelta,â€¦ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o Ä‘Ã¢y.</p>

<p>Mini-batch GD Ä‘Æ°á»£c sá»­ dá»¥ng trong háº§u háº¿t cÃ¡c thuáº­t toÃ¡n Machine Learning, Ä‘áº·c biá»‡t lÃ  trong Deep Learning. GiÃ¡ trá»‹ \(n\) thÆ°á»ng Ä‘Æ°á»£c chá»n lÃ  khoáº£ng tá»« 50 Ä‘áº¿n 100.</p>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t má»—i khi cáº­p nháº­t tham sá»‘ \(\theta\) cá»§a má»™t bÃ i toÃ¡n khÃ¡c phá»©c táº¡p hÆ¡n.</p>

<div class="imgcap">
 <img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png" align="center" width="400" />
 <div class="thecap"> HÃ m máº¥t mÃ¡t <em>nháº£y lÃªn nháº£y xuá»‘ng</em> (fluctuate) sau má»—i láº§n cáº­p nháº­t nhÆ°ng nhÃ¬n chung giáº£m dáº§n vÃ  cÃ³ xu hÆ°á»›ng há»™i tá»¥ vá» cuá»‘i. (Nguá»“n: <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Wikipedia</a>). </div>
</div>

<p>Äá»ƒ cÃ³ thÃªm thÃ´ng tin chi tiáº¿t hÆ¡n, báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m trong <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">bÃ i viáº¿t ráº¥t tá»‘t nÃ y</a>.</p>

<p><a name="-stopping-criteria-dieu-kien-dung"></a></p>

<h2 id="3-stopping-criteria-Ä‘iá»u-kiá»‡n-dá»«ng">3. Stopping Criteria (Ä‘iá»u kiá»‡n dá»«ng)</h2>

<p>CÃ³ má»™t Ä‘iá»ƒm cÅ©ng quan trá»ng mÃ  tá»« Ä‘áº§u tÃ´i chÆ°a nháº¯c Ä‘áº¿n: khi nÃ o thÃ¬ chÃºng ta biáº¿t thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥ vÃ  dá»«ng láº¡i?</p>

<p>Trong thá»±c nghiá»‡m, cÃ³ má»™t vÃ i phÆ°Æ¡ng phÃ¡p nhÆ° dÆ°á»›i Ä‘Ã¢y:</p>

<ol>
  <li>Giá»›i háº¡n sá»‘ vÃ²ng láº·p: Ä‘Ã¢y lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t vÃ  cÅ©ng Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng chÆ°Æ¡ng trÃ¬nh cháº¡y khÃ´ng quÃ¡ lÃ¢u. Tuy nhiÃªn, má»™t nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡ch lÃ m nÃ y lÃ  cÃ³ thá»ƒ thuáº­t toÃ¡n dá»«ng láº¡i trÆ°á»›c khi Ä‘á»§ gáº§n vá»›i nghiá»‡m.</li>
  <li>So sÃ¡nh gradient cá»§a nghiá»‡m táº¡i hai láº§n cáº­p nháº­t liÃªn tiáº¿p, khi nÃ o giÃ¡ trá»‹ nÃ y Ä‘á»§ nhá» thÃ¬ dá»«ng láº¡i. PhÆ°Æ¡ng phÃ¡p nÃ y cÅ©ng cÃ³ má»™t nhÆ°á»£c Ä‘iá»ƒm lá»›n lÃ  viá»‡c tÃ­nh Ä‘áº¡o hÃ m Ä‘Ã´i khi trá»Ÿ nÃªn quÃ¡ phá»©c táº¡p (vÃ­ dá»¥ nhÆ° khi cÃ³ quÃ¡ nhiá»u dá»¯ liá»‡u), náº¿u Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y thÃ¬ coi nhÆ° ta khÃ´ng Ä‘Æ°á»£c lá»£i khi sá»­ dá»¥ng SGD vÃ  mini-batch GD.</li>
  <li>So sÃ¡nh giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t cá»§a nghiá»‡m táº¡i hai láº§n cáº­p nháº­t liÃªn tiáº¿p, khi nÃ o giÃ¡ trá»‹ nÃ y Ä‘á»§ nhá» thÃ¬ dá»«ng láº¡i. NhÆ°á»£c Ä‘iá»ƒm cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  náº¿u táº¡i má»™t thá»i Ä‘iá»ƒm, Ä‘á»“ thá»‹ hÃ m sá»‘ cÃ³ dáº¡ng <em>báº³ng pháº³ng</em> táº¡i má»™t khu vá»±c nhÆ°ng khu vá»±c Ä‘Ã³ khÃ´ng chá»©a Ä‘iá»ƒm local minimum (khu vá»±c nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  saddle points), thuáº­t toÃ¡n cÅ©ng dá»«ng láº¡i trÆ°á»›c khi Ä‘áº¡t giÃ¡ trá»‹ mong muá»‘n.</li>
  <li>Trong SGD vÃ  mini-batch GD, cÃ¡ch thÆ°á»ng dÃ¹ng lÃ  so sÃ¡nh nghiá»‡m sau má»™t vÃ i láº§n cáº­p nháº­t. Trong Ä‘oáº¡n code Python phÃ­a trÃªn vá» SGD, tÃ´i Ã¡p dá»¥ng viá»‡c so sÃ¡nh nÃ y má»—i khi nghiá»‡m Ä‘Æ°á»£c cáº­p nháº­t 10 láº§n. Viá»‡c lÃ m nÃ y cÅ©ng tá» ra khÃ¡ hiá»‡u quáº£.</li>
</ol>

<p><a name="-mot-phuong-phap-toi-uu-don-gian-khac-newtons-method"></a></p>

<h2 id="4-má»™t-phÆ°Æ¡ng-phÃ¡p-tá»‘i-Æ°u-Ä‘Æ¡n-giáº£n-khÃ¡c-newtons-method">4. Má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u Ä‘Æ¡n giáº£n khÃ¡c: Newtonâ€™s method</h2>

<p>NhÃ¢n tiá»‡n Ä‘ang nÃ³i vá» tá»‘i Æ°u, tÃ´i xin giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p ná»¯a cÃ³ cÃ¡ch giáº£i thÃ­ch Ä‘Æ¡n giáº£n: Newtonâ€™s method. CÃ¡c phÆ°Æ¡ng phÃ¡p GD tÃ´i Ä‘Ã£ trÃ¬nh bÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  first-order methods, vÃ¬ lá»i giáº£i tÃ¬m Ä‘Æ°á»£c dá»±a trÃªn Ä‘áº¡o hÃ m báº­c nháº¥t cá»§a hÃ m sá»‘. Newtonâ€™s method lÃ  má»™t second-order method, tá»©c lá»i giáº£i yÃªu cáº§u tÃ­nh Ä‘áº¿n Ä‘áº¡o hÃ m báº­c hai.</p>

<p>Nháº¯c láº¡i ráº±ng, cho tá»›i thá»i Ä‘iá»ƒm nÃ y, chÃºng ta luÃ´n giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t báº±ng 0 Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘iá»ƒm local minimun. (VÃ  trong nhiá»u trÆ°á»ng há»£p, coi nghiá»‡m tÃ¬m Ä‘Æ°á»£c lÃ  nghiá»‡m cá»§a bÃ i toÃ¡n tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t cá»§a hÃ m máº¥t mÃ¡t). CÃ³ má»™t thuáº­t toÃ¡n ná»‘i tiáº¿ng giÃºp giáº£i bÃ i toÃ¡n \(f(x) = 0\), thuáº­t toÃ¡n Ä‘Ã³ cÃ³ tÃªn lÃ  Newtonâ€™s method.</p>

<p><a name="newtons-method-cho-giai-phuong-trinh-\\fx--\\"></a></p>

<h3 id="newtons-method-cho-giáº£i-phÆ°Æ¡ng-trÃ¬nh-fx--0">Newtonâ€™s method cho giáº£i phÆ°Æ¡ng trÃ¬nh \(f(x) = 0\)</h3>

<p>Thuáº­t toÃ¡n Newtonâ€™s method Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh Ä‘á»™ng minh há»a dÆ°á»›i Ä‘Ã¢y:</p>

<div class="imgcap">
 <img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif" align="center" width="500" />
 <div class="thecap"> HÃ¬nh 3: Minh há»a thuáº­t toÃ¡n Newton's method trong giáº£i phÆ°Æ¡ng trÃ¬nh. (  Nguá»“n: <a href="https://en.wikipedia.org/wiki/Newton's_method"> Newton's method - Wikipedia</a>).</div>
</div>

<p>Ã tÆ°á»Ÿng giáº£i bÃ i toÃ¡n \(f(x) = 0\) báº±ng phÆ°Æ¡ng phÃ¡p Newtonâ€™s method nhÆ° sau. Xuáº¥t phÃ¡t tá»« má»™t Ä‘iá»ƒm \(x_0\) Ä‘Æ°á»£c cho lÃ  gáº§n vá»›i nghiá»‡m \(x^*\). Sau Ä‘Ã³ váº½ Ä‘Æ°á»ng tiáº¿p tuyáº¿n (máº·t tiáº¿p tuyáº¿n trong khÃ´ng gian nhiá»u chiá»u) vá»›i Ä‘á»“ thá»‹ hÃ m sá»‘ \(y = f(x)\) táº¡i Ä‘iá»ƒm trÃªn Ä‘á»“ thá»‹ cÃ³ hoÃ nh Ä‘á»™ \(x_0\). Giao Ä‘iá»ƒm \(x_1\) cá»§a Ä‘Æ°á»ng tiáº¿p tuyáº¿n nÃ y vá»›i trá»¥c hoÃ nh Ä‘Æ°á»£c xem lÃ  gáº§n vá»›i nghiá»‡m \(x^*\) hÆ¡n. Thuáº­t toÃ¡n láº·p láº¡i vá»›i Ä‘iá»ƒm má»›i \(x_1\) vÃ  cá»© nhÆ° váº­y Ä‘áº¿n khi ta Ä‘Æ°á»£c \(f(x_t) \approx 0\).</p>

<p>ÄÃ³ lÃ  Ã½ nghÄ©a hÃ¬nh há»c cá»§a Newtonâ€™s method, chÃºng ta cáº§n má»™t cÃ´ng thá»©c Ä‘á»ƒ cÃ³ thá»ƒ dá»±a vÃ o Ä‘Ã³ Ä‘á»ƒ láº­p trÃ¬nh. Viá»‡c nÃ y khÃ´ng quÃ¡ phá»©c táº¡p vá»›i cÃ¡c báº¡n thi Ä‘áº¡i há»c mÃ´n toÃ¡n á»Ÿ VN. Tháº­t váº­y, phÆ°Æ¡ng trÃ¬nh tiáº¿p tuyáº¿n vá»›i Ä‘á»“ thá»‹ cá»§a hÃ m \(f(x)\) táº¡i Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ \(x_t\) lÃ :
\[
y = fâ€™(x_t)(x - x_t) + f(x_t)
\]
Giao Ä‘iá»ƒm cá»§a Ä‘Æ°á»ng tháº³ng nÃ y vá»›i trá»¥c \(x\) tÃ¬m Ä‘Æ°á»£c báº±ng cÃ¡ch giáº£i phÆ°Æ¡ng trÃ¬nh váº¿ pháº£i cá»§a biá»ƒu thá»©c trÃªn báº±ng 0, tá»©c lÃ :
\[
x = x_t - \frac{f(x_t)}{fâ€™(x_t)} \triangleq x_{t+1}
\]</p>

<p><a name="newtons-method-trong-bai-toan-tim-local-minimun"></a></p>

<h3 id="newtons-method-trong-bÃ i-toÃ¡n-tÃ¬m-local-minimun">Newtonâ€™s method trong bÃ i toÃ¡n tÃ¬m local minimun</h3>
<p>Ãp dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y cho viá»‡c giáº£i phÆ°Æ¡ng trÃ¬nh \(fâ€™(x) = 0\) ta cÃ³:
\[
x_{t+1} = x_t -(fâ€(x_t))^{-1}{fâ€™(x_t)}
\]</p>

<p>VÃ  trong khÃ´ng gian nhiá»u chiá»u vá»›i \(\theta\) lÃ  biáº¿n:
\[
\theta = \theta - \mathbf{H}(J(\theta))^{-1} \nabla_{\theta} J(\theta)
\]
trong Ä‘Ã³ \(\mathbf{H}(J(\theta))\) lÃ  Ä‘áº¡o hÃ m báº­c hai cá»§a hÃ m máº¥t máº¥t (cÃ²n gá»i lÃ  <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>). Biá»ƒu thá»©c nÃ y lÃ  má»™t ma tráº­n náº¿u \(\theta\) lÃ  má»™t vector. VÃ  \(\mathbf{H}(J(\theta))^{-1}\) chÃ­nh lÃ  nghá»‹ch Ä‘áº£o cá»§a ma tráº­n Ä‘Ã³.</p>

<p><a name="han-che-cua-newtons-method"></a></p>

<h3 id="háº¡n-cháº¿-cá»§a-newtons-method">Háº¡n cháº¿ cá»§a Newtonâ€™s method</h3>
<ul>
  <li>Äiá»ƒm khá»Ÿi táº¡o pháº£i <em>ráº¥t</em> gáº§n vá»›i nghiá»‡m \(x^*\).
Ã tÆ°á»Ÿng sÃ¢u xa hÆ¡n cá»§a Newtonâ€™s method lÃ  dá»±a trÃªn khai triá»ƒn Taylor cá»§a hÃ m sá»‘ \(f(x)\) tá»›i Ä‘áº¡o hÃ m thá»© nháº¥t:
\[
0 = f(x^*) \approx f(x_t) + fâ€™(x_t)(x_t - x^*)
\]
Tá»« Ä‘Ã³ suy ra: \(x^* \approx x_t - \frac{f(x_t)}{fâ€™(x_t)}\). 
Má»™t Ä‘iá»ƒm ráº¥t quan trá»ng, khai triá»ƒn Taylor chá»‰ Ä‘Ãºng náº¿u \(x_t\) ráº¥t gáº§n vá»›i \(x^*\)!
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ kinh Ä‘iá»ƒn trÃªn Wikipedia vá» viá»‡c Newtonâ€™s method cho má»™t dÃ£y sá»‘ phÃ¢n ká»³ (divergence).</li>
</ul>
<div class="imgcap">
 <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/NewtonsMethodConvergenceFailure.svg/300px-NewtonsMethodConvergenceFailure.svg.png" align="center" width="400" />
 <div class="thecap"> HÃ¬nh 4: Nghiá»‡m lÃ  má»™t Ä‘iá»ƒm gáº§n -2. Tiáº¿p tuyáº¿n cá»§a Ä‘á»“ thá»‹ hÃ m sá»‘ táº¡i Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ báº±ng 0 cáº¯t trá»¥c hoÃ nh táº¡i 1, vÃ  ngÆ°á»£c láº¡i. Trong trÆ°á»ng há»£p nÃ y, Newton's method khÃ´ng bao giá» há»™i tá»¥. (Nguá»“n: <a href="https://en.wikipedia.org/wiki/Newton's_method">Wikipedia</a>). </div>
</div>

<ul>
  <li>
    <p>Nháº­n tháº¥y ráº±ng trong viá»‡c giáº£i phÆ°Æ¡ng trÃ¬nh \(f(x) = 0\), chÃºng ta cÃ³ Ä‘áº¡o hÃ m á»Ÿ máº«u sá»‘. Khi Ä‘áº¡o hÃ m nÃ y gáº§n vá»›i 0, ta sáº½ Ä‘Æ°á»£c má»™t Ä‘Æ°á»ng tháº±ng song song hoáº·c gáº§n song song vá»›i trá»¥c hoÃ nh. Ta sáº½ hoáº·c khÃ´ng tÃ¬m Ä‘Æ°á»£c giao Ä‘iá»ƒm, hoáº·c Ä‘Æ°á»£c má»™t giao Ä‘iá»ƒm á»Ÿ vÃ´ cÃ¹ng. Äáº·c biá»‡t, khi nghiá»‡m chÃ­nh lÃ  Ä‘iá»ƒm cÃ³ Ä‘áº¡o hÃ m báº±ng 0, thuáº­t toÃ¡n gáº§n nhÆ° sáº½ khÃ´ng tÃ¬m Ä‘Æ°á»£c nghiá»‡m!</p>
  </li>
  <li>
    <p>Khi Ã¡p dá»¥ng Newtonâ€™s method cho bÃ i toÃ¡n tá»‘i Æ°u trong khÃ´ng gian nhiá»u chiá»u, chÃºng ta cáº§n tÃ­nh nghá»‹ch Ä‘áº£o cá»§a Hessian matrix. Khi sá»‘ chiá»u vÃ  sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u lá»›n, Ä‘áº¡o hÃ m báº­c hai cá»§a hÃ m máº¥t mÃ¡t sáº½ lÃ  má»™t ma tráº­n ráº¥t lá»›n, áº£nh hÆ°á»Ÿng tá»›i cáº£ memory vÃ  tá»‘c Ä‘á»™ tÃ­nh toÃ¡n cá»§a há»‡ thá»‘ng.</p>
  </li>
</ul>

<p><a name="-ket-luan"></a></p>

<h2 id="5-káº¿t-luáº­n">5. Káº¿t luáº­n</h2>
<p>Qua hai bÃ i viáº¿t vá» Gradient Descent nÃ y, tÃ´i hy vá»ng cÃ¡c báº¡n Ä‘Ã£ hiá»ƒu vÃ  lÃ m quen vá»›i má»™t thuáº­t toÃ¡n tá»‘i Æ°u Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u nháº¥t trong Machine Learning vÃ  Ä‘áº·c biá»‡t lÃ  Deep Learning. CÃ²n nhiá»u biáº¿n thá»ƒ khÃ¡c khÃ¡ thÃº vá»‹ vá» GD (mÃ  ráº¥t cÃ³ thá»ƒ tÃ´i chÆ°a biáº¿t tá»›i), nhÆ°ng tÃ´i xin phÃ©p Ä‘Æ°á»£c dá»«ng chuá»—i bÃ i vá» GD táº¡i Ä‘Ã¢y vÃ  tiáº¿p tá»¥c chuyá»ƒn sang cÃ¡c thuáº­t toÃ¡n thÃº vá»‹ khÃ¡c.</p>

<p>Hy vá»ng bÃ i viáº¿t cÃ³ Ã­ch vá»›i cÃ¡c báº¡n.</p>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-tÃ i-liá»‡u-tham-kháº£o">6. TÃ i liá»‡u tham kháº£o</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Newton's_method">Newtonâ€™s method - Wikipedia</a></p>

<p>[2] <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">An overview of gradient descent optimization algorithms</a></p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent - Wikipedia</a></p>

<p>[4] <a href="https://www.youtube.com/watch?v=UfNU3Vhv5CA">Stochastic Gradient Descen - Andrew Ng</a></p>

<p>[5] Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543â€“ 547.</p>
:ET