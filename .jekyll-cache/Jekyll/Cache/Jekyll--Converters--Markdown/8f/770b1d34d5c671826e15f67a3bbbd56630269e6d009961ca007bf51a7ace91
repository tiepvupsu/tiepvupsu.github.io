I"‚º<p>Trong lo·∫°t b√†i ti·∫øp theo, t√¥i s·∫Ω tr√¨nh b√†y v·ªÅ m·ªôt trong nh·ªØng thu·∫≠t to√°n classification ph·ªï bi·∫øn nh·∫•t (c√πng v·ªõi <a href="/2017/02/17/softmax/">softmax regression</a>). C√≥ r·∫•t nhi·ªÅu suy lu·∫≠n to√°n h·ªçc trong ph·∫ßn n√†y y√™u c·∫ßu b·∫°n c·∫ßn c√≥ ki·∫øn th·ª©c v·ªÅ <a href="/2017/04/02/duality/">Duality</a> c≈©ng nh∆∞ v·ªÅ t·ªëi ∆∞u l·ªìi. B·∫°n ƒë∆∞·ª£c khuy·∫øn kh√≠ch ƒë·ªçc c√°c B√†i 16, 17, v√† 18 tr∆∞·ªõc khi ƒë·ªçc b√†i n√†y.</p>

<p><em>N·∫øu kh√¥ng mu·ªën ƒëi s√¢u v√†o ph·∫ßn to√°n, b·∫°n c√≥ th·ªÉ b·ªè qua m·ª•c 3.</em></p>

<p><strong>Trong trang n√†y:</strong> 
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Gi·ªõi thi·ªáu</a>
    <ul>
      <li><a href="#-khoang-cach-tu-mot-diem-toi-mot-sieu-mat-phang">1.1. Kho·∫£ng c√°ch t·ª´ m·ªôt ƒëi·ªÉm t·ªõi m·ªôt si√™u m·∫∑t ph·∫≥ng</a></li>
      <li><a href="#-nhac-lai-bai-toan-phan-chia-hai-classes">1.2. Nh·∫Øc l·∫°i b√†i to√°n ph√¢n chia hai classes</a></li>
    </ul>
  </li>
  <li><a href="#-xay-dung-bai-toan-toi-uu-cho-svm">2. X√¢y d·ª±ng b√†i to√°n t·ªëi ∆∞u cho SVM</a></li>
  <li><a href="#-bai-toan-doi-ngau-cho-svm">3. B√†i to√°n ƒë·ªëi ng·∫´u cho SVM</a>
    <ul>
      <li><a href="#-kiem-tra-tieu-chuan-slater">3.1. Ki·ªÉm tra ti√™u chu·∫©n Slater</a></li>
      <li><a href="#-lagrangian-cua-bai-toan-svm">3.2. Lagrangian c·ªßa b√†i to√°n SVM</a></li>
      <li><a href="#-ham-doi-ngau-lagrange">3.3. H√†m ƒë·ªëi ng·∫´u Lagrange</a></li>
      <li><a href="#-bai-toan-doi-ngau-lagrange">3.4. B√†i to√°n ƒë·ªëi ng·∫´u Lagrange</a></li>
      <li><a href="#-dieu-kien-kkt">3.5. ƒêi·ªÅu ki·ªán KKT</a></li>
    </ul>
  </li>
  <li><a href="#-lap-trinh-tim-nghiem-cho-svm">4. L·∫≠p tr√¨nh t√¨m nghi·ªám cho SVM</a>
    <ul>
      <li><a href="#-tim-nghiem-theo-cong-thuc">4.1. T√¨m nghi·ªám theo c√¥ng th·ª©c</a></li>
      <li><a href="#-tim-nghiem-theo-thu-vien">4.2. T√¨m nghi·ªám theo th∆∞ vi·ªán</a></li>
    </ul>
  </li>
  <li><a href="#-tom-tat-va-thao-luan">5. T√≥m t·∫Øt v√† th·∫£o lu·∫≠n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. T√†i li·ªáu tham kh·∫£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>
<p>Tr∆∞·ªõc khi ƒëi v√†o ph·∫ßn √Ω t∆∞·ªüng ch√≠nh c·ªßa Support Vector Machine, t√¥i xin m·ªôt l·∫ßn n·ªØa nh·∫Øc l·∫°i ki·∫øn th·ª©c v·ªÅ h√¨nh h·ªçc gi·∫£i t√≠ch m√† ch√∫ng ta ƒë√£ qu√° quen khi √¥n thi ƒë·∫°i h·ªçc.</p>

<p><a name="-khoang-cach-tu-mot-diem-toi-mot-sieu-mat-phang"></a></p>

<h3 id="11-kho·∫£ng-c√°ch-t·ª´-m·ªôt-ƒëi·ªÉm-t·ªõi-m·ªôt-si√™u-m·∫∑t-ph·∫≥ng">1.1. Kho·∫£ng c√°ch t·ª´ m·ªôt ƒëi·ªÉm t·ªõi m·ªôt si√™u m·∫∑t ph·∫≥ng</h3>
<p>Trong kh√¥ng gian 2 chi·ªÅu, ta bi·∫øt r·∫±ng kho·∫£ng c√°ch t·ª´ m·ªôt ƒëi·ªÉm c√≥ to·∫° ƒë·ªô \((x_0, y_0)\) t·ªõi <em>ƒë∆∞·ªùng th·∫≥ng</em> c√≥ ph∆∞∆°ng tr√¨nh \(w_1x + w_2y + b = 0\) ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi: 
\[
\frac{|w_1x_0 + w_2y_0 + b|}{\sqrt{w_1^2 + w_2^2}}
\]</p>

<p>Trong kh√¥ng gian ba chi·ªÅu, kho·∫£ng c√°ch t·ª´ m·ªôt ƒëi·ªÉm c√≥ to·∫° ƒë·ªô \((x_0, y_0, z_0)\) t·ªõi m·ªôt <em>m·∫∑t ph·∫≥ng</em> c√≥ ph∆∞∆°ng tr√¨nh \(w_1x + w_2y + w_3 z + b = 0\) ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi: 
\[
\frac{|w_1x_0 + w_2y_0 + w_3z_0 + b |}{\sqrt{w_1^2 + w_2^2 + w_3^2}}
\]</p>

<p>H∆°n n·ªØa, n·∫øu ta b·ªè d·∫•u tr·ªã tuy·ªát ƒë·ªëi ·ªü t·ª≠ s·ªë, ch√∫ng ta c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒëi·ªÉm ƒë√≥ n·∫±m v·ªÅ ph√≠a n√†o c·ªßa <em>ƒë∆∞·ªùng th·∫≥ng</em> hay <em>m·∫∑t ph·∫≥ng</em> ƒëang x√©t. Nh·ªØng ƒëi·ªÉm l√†m cho bi·ªÉu th·ª©c trong d·∫•u gi√° tr·ªã tuy·ªát ƒë·ªëi mang d·∫•u d∆∞∆°ng n·∫±m v·ªÅ c√πng 1 ph√≠a (t√¥i t·∫°m g·ªçi ƒë√¢y l√† <em>ph√≠a d∆∞∆°ng</em> c·ªßa ƒë∆∞·ªùng th·∫≥ng), nh·ªØng ƒëi·ªÉm l√†m cho bi·ªÉu th·ª©c trong d·∫•u gi√° tr·ªã tuy·ªát ƒë·ªëi mang d·∫•u √¢m n·∫±m v·ªÅ ph√≠a c√≤n l·∫°i (t√¥i g·ªç l√† <em>ph√≠a √¢m</em>). Nh·ªØng ƒëi·ªÉm n·∫±m tr√™n <em>ƒë∆∞·ªùng th·∫≥ng</em>/<em>mƒÉt ph·∫≥ng</em> s·∫Ω l√†m cho t·ª≠ s·ªë c√≥ gi√° tr·ªã b·∫±ng 0, t·ª©c kho·∫£ng c√°ch b·∫±ng 0.</p>

<p>Vi·ªác n√†y c√≥ th·ªÉ ƒë∆∞·ª£c t·ªïng qu√°t l√™n kh√¥ng gian nhi·ªÅu chi·ªÅu: Kho·∫£ng c√°ch t·ª´ m·ªôt ƒëi·ªÉm (vector) c√≥ to·∫° ƒë·ªô \(\mathbf{x}_0\) t·ªõi <em>si√™u m·∫∑t ph·∫≥ng</em> (<em>hyperplane</em>) c√≥ ph∆∞∆°ng tr√¨nh \(\mathbf{w}^T\mathbf{x} + b = 0\) ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi: 
\[
\frac{|\mathbf{w}^T\mathbf{x}_0 + b|}{||\mathbf{w}||_2}
\]</p>

<p>V·ªõi \(||\mathbf{w}||_2 = \sqrt{\sum_{i=1}^d w_i^2}\) v·ªõi \(d\) l√† s·ªë chi·ªÅu c·ªßa kh√¥ng gian.</p>

<p><a name="-nhac-lai-bai-toan-phan-chia-hai-classes"></a></p>

<h3 id="12-nh·∫Øc-l·∫°i-b√†i-to√°n-ph√¢n-chia-hai-classes">1.2. Nh·∫Øc l·∫°i b√†i to√°n ph√¢n chia hai classes</h3>

<p>Ch√∫ng ta c√πng quay l·∫°i v·ªõi b√†i to√°n trong <a href="/2017/01/21/perceptron/">Perceptron Learning Algorithm (PLA)</a>. Gi·∫£ s·ª≠ r·∫±ng c√≥ hai class kh√°c nhau ƒë∆∞·ª£c m√¥ t·∫£ b·ªüi c√°c ƒëi·ªÉm trong kh√¥ng gian nhi·ªÅu chi·ªÅu, hai classes n√†y <em>linearly separable</em>, t·ª©c t·ªìn t·∫°i m·ªôt si√™u ph·∫≥ng ph√¢n chia ch√≠nh x√°c hai classes ƒë√≥. H√£y t√¨m m·ªôt si√™u m·∫∑t ph·∫≥ng ph√¢n chia hai classes ƒë√≥, t·ª©c t·∫•t c·∫£ c√°c ƒëi·ªÉm thu·ªôc m·ªôt class n·∫±m v·ªÅ c√πng m·ªôt ph√≠a c·ªßa si√™u m·∫∑t ph·∫≥ng ƒë√≥ v√† ng∆∞·ª£c ph√≠a v·ªõi to√†n b·ªô c√°c ƒëi·ªÉm thu·ªôc class c√≤n l·∫°i. Ch√∫ng ta ƒë√£ bi·∫øt r·∫±ng, thu·∫≠t to√°n PLA c√≥ th·ªÉ l√†m ƒë∆∞·ª£c vi·ªác n√†y nh∆∞ng n√≥ c√≥ th·ªÉ cho ch√∫ng ta v√¥ s·ªë nghi·ªám nh∆∞ H√¨nh 1 d∆∞·ªõi ƒë√¢y:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm1.png" align="center" width="500" />
 <div class="thecap">H√¨nh 1: C√°c m·∫∑t ph√¢n c√°ch hai classes linearly separable.</div>
</div>
<hr />

<p>C√¢u h·ªèi ƒë·∫∑t ra l√†: trong v√¥ s·ªë c√°c m·∫∑t ph√¢n chia ƒë√≥, ƒë√¢u l√† m·∫∑t ph√¢n chia t·ªët nh·∫•t <em>theo m·ªôt ti√™u chu·∫©n n√†o ƒë√≥</em>? Trong ba ƒë∆∞·ªùng th·∫≥ng minh h·ªça trong H√¨nh 1 ph√≠a tr√™n, c√≥ hai ƒë∆∞·ªùng th·∫≥ng kh√° <em>l·ªách</em> v·ªÅ ph√≠a class h√¨nh tr√≤n ƒë·ªè. ƒêi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn cho l·ªõp m√†u ƒë·ªè <em>kh√¥ng vui v√¨ l√£nh th·ªï xem ra b·ªã l·∫•n nhi·ªÅu qu√°</em>. Li·ªáu c√≥ c√°ch n√†o ƒë·ªÉ t√¨m ƒë∆∞·ª£c ƒë∆∞·ªùng ph√¢n chia m√† c·∫£ hai classes ƒë·ªÅu c·∫£m th·∫•y <em>c√¥ng b·∫±ng</em> v√† <em>h·∫°nh ph√∫c</em> nh·∫•t hay kh√¥ng?</p>

<p>Ch√∫ng ta c·∫ßn t√¨m m·ªôt ti√™u chu·∫©n ƒë·ªÉ ƒëo s·ª± <em>h·∫°nh ph√∫c</em> c·ªßa m·ªói class. H√£y xem H√¨nh 2 d∆∞·ªõi ƒë√¢y:</p>
<hr />

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/19_svm/svm2.png" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/19_svm/svm5.png" />
        </td>

    </tr>

</table>
<div class="thecap"> H√¨nh 2: Margin c·ªßa hai classes l√† b·∫±ng nhau v√† l·ªõn nh·∫•t c√≥ th·ªÉ.
</div>
</div>
<hr />

<!-- <hr>
<div class="imgcap">
 <img src ="/assets/19_svm/svm2.png" align = "center" width = "500">
 <div class = "thecap">H√¨nh 2: .</div>
</div>
<hr> -->

<p>N·∫øu ta ƒë·ªãnh nghƒ©a <em>m·ª©c ƒë·ªô h·∫°nh ph√∫c</em> c·ªßa m·ªôt class t·ªâ l·ªá thu·∫≠n v·ªõi kho·∫£ng c√°ch g·∫ßn nh·∫•t t·ª´ m·ªôt ƒëi·ªÉm c·ªßa class ƒë√≥ t·ªõi ƒë∆∞·ªùng/m·∫∑t ph√¢n chia, th√¨ ·ªü H√¨nh 2 tr√°i, class tr√≤n ƒë·ªè s·∫Ω <em>kh√¥ng ƒë∆∞·ª£c h·∫°nh ph√∫c cho l·∫Øm</em> v√¨ ƒë∆∞·ªùng ph√¢n chia g·∫ßn n√≥ h∆°n class vu√¥ng xanh r·∫•t nhi·ªÅu. Ch√∫ng ta c·∫ßn m·ªôt ƒë∆∞·ªùng ph√¢n chia sao cho kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm g·∫ßn nh·∫•t c·ªßa m·ªói class (c√°c ƒëi·ªÉm ƒë∆∞·ª£c khoanh tr√≤n) t·ªõi ƒë∆∞·ªùng ph√¢n chia l√† nh∆∞ nhau, nh∆∞ th·∫ø th√¨ m·ªõi <em>c√¥ng b·∫±ng</em>. Kho·∫£ng c√°ch nh∆∞ nhau n√†y ƒë∆∞·ª£c g·ªçi l√† <em>margin</em> (<em>l·ªÅ</em>).</p>

<p>ƒê√£ c√≥ <em>c√¥ng b·∫±ng</em> r·ªìi, ch√∫ng ta c·∫ßn <em>vƒÉn minh</em> n·ªØa. <em>C√¥ng b·∫±ng</em> m√† c·∫£ hai ƒë·ªÅu <em>k√©m h·∫°nh ph√∫c nh∆∞ nhau</em> th√¨ ch∆∞a ph·∫£i l√† <em>vƒÉn m√¨nh</em> cho l·∫Øm.</p>

<p>Ch√∫ng ta x√©t ti·∫øp H√¨nh 2 b√™n ph·∫£i khi kho·∫£ng c√°ch t·ª´ ƒë∆∞·ªùng ph√¢n chia t·ªõi c√°c ƒëi·ªÉm g·∫ßn nh·∫•t c·ªßa m·ªói class l√† nh∆∞ nhau. X√©t hai c√°ch ph√¢n chia b·ªüi ƒë∆∞·ªùng n√©t li·ªÅn m√†u ƒëen v√† ƒë∆∞·ªùng n√©t ƒë·ª©t m√†u l·ª•c, ƒë∆∞·ªùng n√†o s·∫Ω l√†m cho c·∫£ hai class <em>h·∫°nh ph√∫c h∆°n</em>? R√µ r√†ng ƒë√≥ ph·∫£i l√† ƒë∆∞·ªùng n√©t li·ªÅn m√†u ƒëen v√¨ n√≥ t·∫°o ra m·ªôt <em>margin</em> r·ªông h∆°n.</p>

<p>Vi·ªác <em>margin</em> r·ªông h∆°n s·∫Ω mang l·∫°i hi·ªáu ·ª©ng ph√¢n l·ªõp t·ªët h∆°n v√¨ <em>s·ª± ph√¢n chia gi·ªØa hai classes l√† r·∫°ch r√≤i h∆°n</em>. Vi·ªác n√†y, sau n√†y c√°c b·∫°n s·∫Ω th·∫•y, l√† m·ªôt ƒëi·ªÉm kh√° quan tr·ªçng gi√∫p <em>Support Vector Machine</em> mang l·∫°i k·∫øt qu·∫£ ph√¢n lo·∫°i t·ªët h∆°n so v·ªõi <em>Neural Network v·ªõi 1 layer</em>, t·ª©c Perceptron Learning Algorithm.</p>

<p>B√†i to√°n t·ªëi ∆∞u trong <em>Support Vector Machine</em> (SVM) ch√≠nh l√† b√†i to√°n ƒëi t√¨m ƒë∆∞·ªùng ph√¢n chia sao cho <em>margin</em> l√† l·ªõn nh·∫•t. ƒê√¢y c≈©ng l√† l√Ω do v√¨ sao SVM c√≤n ƒë∆∞·ª£c g·ªçi l√† <em>Maximum Margin Classifier</em>. Ngu·ªìn g·ªëc c·ªßa t√™n g·ªçi Support Vector Machine s·∫Ω s·ªõm ƒë∆∞·ª£c l√†m s√°ng t·ªè.</p>

<p><a name="-xay-dung-bai-toan-toi-uu-cho-svm"></a></p>

<h2 id="2-x√¢y-d·ª±ng-b√†i-to√°n-t·ªëi-∆∞u-cho-svm">2. X√¢y d·ª±ng b√†i to√°n t·ªëi ∆∞u cho SVM</h2>
<p>Gi·∫£ s·ª≠ r·∫±ng c√°c c·∫∑p d·ªØ li·ªáu c·ªßa <em>training set</em> l√† \((\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_N, y_N)\) v·ªõi vector \(\mathbf{x}_i \in \mathbb{R}^d\) th·ªÉ hi·ªán <em>ƒë·∫ßu v√†o</em> c·ªßa m·ªôt ƒëi·ªÉm d·ªØ li·ªáu v√† \(y_i\) l√† <em>nh√£n</em> c·ªßa ƒëi·ªÉm d·ªØ li·ªáu ƒë√≥. \(d\) l√† s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu v√† \(N\) l√† s·ªë ƒëi·ªÉm d·ªØ li·ªáu. Gi·∫£ s·ª≠ r·∫±ng <em>nh√£n</em> c·ªßa m·ªói ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi \(y_i = 1\) (class 1) ho·∫∑c \(y_i = -1\) (class 2) gi·ªëng nh∆∞ trong PLA.</p>

<p>ƒê·ªÉ gi√∫p c√°c b·∫°n d·ªÖ h√¨nh dung, ch√∫ng ta c√πng x√©t tr∆∞·ªùng h·ª£p trong kh√¥ng gian hai chi·ªÅu d∆∞·ªõi ƒë√¢y. <em>Kh√¥ng gian hai chi·ªÅu ƒë·ªÉ c√°c b·∫°n d·ªÖ h√¨nh dung, c√°c ph√©p to√°n ho√†n to√†n c√≥ th·ªÉ ƒë∆∞·ª£c t·ªïng qu√°t l√™n kh√¥ng gian nhi·ªÅu chi·ªÅu.</em></p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm6.png" align="center" width="500" />
 <div class="thecap">H√¨nh 3: Ph√¢n t√≠ch b√†i to√°n SVM.</div>
</div>
<hr />

<p>Gi·∫£ s·ª≠ r·∫±ng c√°c ƒëi·ªÉm vu√¥ng xanh thu·ªôc class 1, c√°c ƒëi·ªÉm tr√≤n ƒë·ªè thu·ªôc class -1 v√† m·∫∑t \(\mathbf{w}^T\mathbf{x} + b = w_1x_1 + w_2x_2 + b = 0\) l√† m·∫∑t ph√¢n chia gi·ªØa hai classes (H√¨nh 3). H∆°n n·ªØa, class 1 n·∫±m v·ªÅ <em>ph√≠a d∆∞∆°ng</em>, class -1 n·∫±m v·ªÅ <em>ph√≠a √¢m</em> c·ªßa m·∫∑t ph√¢n chia. N·∫øu ng∆∞·ª£c l·∫°i, ta ch·ªâ c·∫ßn ƒë·ªïi d·∫•u c·ªßa \(\mathbf{w}\) v√† \(b\). Ch√∫ √Ω r·∫±ng ch√∫ng ta c·∫ßn ƒëi t√¨m c√°c h·ªá s·ªë \(\mathbf{w}\) v√† \(b\).</p>

<p>Ta quan s√°t th·∫•y m·ªôt ƒëi·ªÉm quan tr·ªçng sau ƒë√¢y: v·ªõi c·∫∑p d·ªØ li·ªáu \((\mathbf{x}_n, y_n)\) b·∫•t k·ª≥, kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm ƒë√≥ t·ªõi m·∫∑t ph√¢n chia l√†: 
\[
\frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2}
\]</p>

<p>ƒêi·ªÅu n√†y c√≥ th·ªÉ d·ªÖ nh·∫≠n th·∫•y v√¨ theo gi·∫£ s·ª≠ ·ªü tr√™n, \(y_n\) lu√¥n c√πng d·∫•u v·ªõi <em>ph√≠a</em> c·ªßa \(\mathbf{x}_n\). T·ª´ ƒë√≥ suy ra \(y_n\) c√πng d·∫•u v·ªõi \((\mathbf{w}^T\mathbf{x}_n + b)\), v√† t·ª≠ s·ªë lu√¥n l√† 1 s·ªë kh√¥ng √¢m.</p>

<p>V·ªõi m·∫∑t ph·∫ßn chia nh∆∞ tr√™n, <em>margin</em> ƒë∆∞·ª£c t√≠nh l√† kho·∫£ng c√°ch g·∫ßn nh·∫•t t·ª´ 1 ƒëi·ªÉm t·ªõi m·∫∑t ƒë√≥ (b·∫•t k·ªÉ ƒëi·ªÉm n√†o trong hai classes):
\[
\text{margin} = \min_{n} \frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2}
\]</p>

<p>B√†i to√°n t·ªëi ∆∞u trong SVM ch√≠nh l√† b√†i to√°n t√¨m \(\mathbf{w}\) v√† \(b\) sao cho <em>margin</em> n√†y ƒë·∫°t gi√° tr·ªã l·ªõn nh·∫•t: 
\[
(\mathbf{w}, b) = \arg\max_{\mathbf{w}, b} \left\{
    \min_{n} \frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2} 
\right\}
= \arg\max_{\mathbf{w}, b}\left\{
    \frac{1}{||\mathbf{w}||_2} \min_{n} y_n(\mathbf{w}^T\mathbf{x}_n + b)
\right\} ~~~ (1)
\]</p>

<p>Vi·ªác gi·∫£i tr·ª±c ti·∫øp b√†i to√°n n√†y s·∫Ω r·∫•t ph·ª©c t·∫°p, nh∆∞ng c√°c b·∫°n s·∫Ω th·∫•y c√≥ c√°ch ƒë·ªÉ ƒë∆∞a n√≥ v·ªÅ b√†i to√°n ƒë∆°n gi·∫£n h∆°n.</p>

<p>Nh·∫≠n x√©t quan tr·ªçng nh·∫•t l√† n·∫øu ta thay vector h·ªá s·ªë \(\mathbf{w}\) b·ªüi \(k\mathbf{w}\) v√† \(b\) b·ªüi \(kb\) trong ƒë√≥ \(k\) l√† m·ªôt h·∫±ng s·ªë d∆∞∆°ng th√¨ m·∫∑t ph√¢n chia kh√¥ng thay ƒë·ªïi, t·ª©c kho·∫£ng c√°ch t·ª´ t·ª´ng ƒëi·ªÉm ƒë·∫øn m·∫∑t ph√¢n chia kh√¥ng ƒë·ªïi, t·ª©c <em>margin</em> kh√¥ng ƒë·ªïi. D·ª±a tr√™n t√≠nh ch·∫•t n√†y, ta c√≥ th·ªÉ gi·∫£ s·ª≠: 
\[
y_n(\mathbf{w}^T\mathbf{x}_n + b) = 1
\]</p>

<p><strong>v·ªõi nh·ªØng ƒëi·ªÉm n·∫±m g·∫ßn m·∫∑t ph√¢n chia nh·∫•t</strong> nh∆∞ H√¨nh 4 d∆∞·ªõi ƒë√¢y:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm3.png" align="center" width="500" />
 <div class="thecap">H√¨nh 4: C√°c ƒëi·ªÉm g·∫ßn m·∫∑t ph√¢n c√°ch nh·∫•t c·ªßa hai classes ƒë∆∞·ª£c khoanh tr√≤n.</div>
</div>
<hr />

<p>Nh∆∞ v·∫≠y, v·ªõi m·ªçi \(n\), ta c√≥: 
\[
y_n(\mathbf{w}^T\mathbf{x}_n + b) \geq 1
\]</p>

<p>V·∫≠y b√†i to√°n t·ªëi ∆∞u \((1)\) c√≥ th·ªÉ ƒë∆∞a v·ªÅ b√†i to√°n t·ªëi ∆∞u c√≥ r√†ng bu·ªôc sau ƒë√¢y: 
\[
\begin{eqnarray}
    (\mathbf{w}, b) &amp;=&amp; \arg \max_{\mathbf{w}, b} \frac{1}{||\mathbf{w}||_2}   \newline
    \text{subject to:}~ &amp;&amp; y_n(\mathbf{w}^T\mathbf{x}_n + b) \geq 1, \forall n = 1, 2, \dots, N ~~~~(2)
\end{eqnarray}
\]</p>

<p>B·∫±ng 1 bi·∫øn ƒë·ªïi ƒë∆°n gi·∫£n, ta c√≥ th·ªÉ ƒë∆∞a b√†i to√°n n√†y v·ªÅ b√†i to√°n d∆∞·ªõi ƒë√¢y:
\[
\begin{eqnarray}
    (\mathbf{w}, b) &amp;=&amp; \arg \min_{\mathbf{w}, b} \frac{1}{2}||\mathbf{w}||_2^2   \newline
    \text{subject to:}~ &amp;&amp; 1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) \leq 0, \forall n = 1, 2, \dots, N ~~~~ (3)
\end{eqnarray}
\]
·ªû ƒë√¢y, ch√∫ng ta ƒë√£ l·∫•y ngh·ªãch ƒë·∫£o h√†m m·ª•c ti√™u, b√¨nh ph∆∞∆°ng n√≥ ƒë·ªÉ ƒë∆∞·ª£c m·ªôt h√†m kh·∫£ vi, v√† nh√¢n v·ªõi \(\frac{1}{2}\) ƒë·ªÉ bi·ªÉu th·ª©c ƒë·∫°o h√†m ƒë·∫πp h∆°n.</p>

<p><strong>Quan s√°t quan tr·ªçng:</strong> Trong b√†i to√°n \((3)\), <a href="/2017/03/12/convexity/#-norms">h√†m m·ª•c ti√™u l√† m·ªôt norm, n√™n l√† m·ªôt h√†m l·ªìi</a>. C√°c h√†m b·∫•t ƒë·∫≥ng th·ª©c r√†ng bu·ªôc l√† c√°c h√†m tuy·∫øn t√≠nh theo \(\mathbf{w}\) v√† \(b\), n√™n ch√∫ng c≈©ng l√† c√°c h√†m l·ªìi. V·∫≠y b√†i to√°n t·ªëi ∆∞u \((3)\) c√≥ h√†m m·ª•c ti√™u l√† l·ªìi, v√† c√°c h√†m r√†ng bu·ªôc c≈©ng l√† l·ªìi, n√™n n√≥ l√† m·ªôt b√†i to√°n l·ªìi. H∆°n n·ªØa, n√≥ l√† m·ªôt <a href="/2017/03/19/convexopt/#-quadratic-programming">Quadratic Programming</a>. Th·∫≠m ch√≠, h√†m m·ª•c ti√™u l√† <em>strictly convex</em> v√¨ \(||\mathbf{w}||_2^2 = \mathbf{w}^T\mathbf{I}\mathbf{w}\) v√† \(\mathbf{I}\) l√† ma tr·∫≠n ƒë∆°n v·ªã - l√† m·ªôt ma tr·∫≠n x√°c ƒë·ªãnh d∆∞∆°ng. T·ª´ ƒë√¢y c√≥ th·ªÉ suy ra nghi·ªám cho SVM l√† <em>duy nh·∫•t</em>.</p>

<p>ƒê·∫øn ƒë√¢y th√¨ b√†i to√°n n√†y c√≥ th·ªÉ gi·∫£i ƒë∆∞·ª£c b·∫±ng c√°c c√¥ng c·ª• h·ªó tr·ª£ t√¨m nghi·ªám cho Quadratic Programing, v√≠ d·ª• <a href="/2017/03/19/convexopt/#-gioi-thieu-thu-vien-cvxopt">CVXOPT</a>.</p>

<p>Tuy nhi√™n, vi·ªác gi·∫£i b√†i to√°n n√†y tr·ªü n√™n ph·ª©c t·∫°p khi s·ªë chi·ªÅu \(d\) c·ªßa kh√¥ng gian d·ªØ li·ªáu v√† s·ªë ƒëi·ªÉm d·ªØ li·ªáu \(N\) tƒÉng l√™n cao.</p>

<p>Ng∆∞·ªùi ta th∆∞·ªùng gi·∫£i <a href="/2017/04/02/duality/#-bai-toan-doi-ngau-lagrange-the-lagrange-dual-problem">b√†i to√°n ƒë·ªëi ng·∫´u</a> c·ªßa b√†i to√°n n√†y. Th·ª© nh·∫•t, b√†i to√°n ƒë·ªëi ng·∫´u c√≥ nh·ªØng t√≠nh ch·∫•t th√∫ v·ªã h∆°n khi·∫øn n√≥ ƒë∆∞·ª£c gi·∫£i hi·ªáu qu·∫£ h∆°n. Th·ª© hai, trong qu√° tr√¨nh x√¢y d·ª±ng b√†i to√°n ƒë·ªëi ng·∫´u, ng∆∞·ªùi ta th·∫•y r·∫±ng SVM c√≥ th·ªÉ ƒë∆∞·ª£c √°p d·ª•ng cho nh·ªØng b√†i to√°n m√† d·ªØ li·ªáu kh√¥ng <em>linearly separable</em>, t·ª©c c√°c ƒë∆∞·ªùng ph√¢n chia kh√¥ng ph·∫£i l√† m·ªôt m·∫∑t ph·∫≥ng m√† c√≥ th·ªÉ l√† c√°c m·∫∑t c√≥ h√¨nh th√π ph·ª©c t·∫°p h∆°n.</p>

<p><em>ƒê·∫øn ƒë√¢y, b·∫°n ƒë·ªçc c√≥ th·ªÉ b·∫Øt ƒë·∫ßu hi·ªÉu t·∫°i sao t√¥i c·∫ßn vi·∫øt 3 b√†i 16-18 tr∆∞·ªõc khi vi·∫øt b√†i n√†y. N·∫øu b·∫°n mu·ªën hi·ªÉu s√¢u h∆°n v·ªÅ SVM, t√¥i khuy·∫øn kh√≠ch ƒë·ªçc M·ª•c 3 d∆∞·ªõi ƒë√¢y. N·∫øu kh√¥ng, b·∫°n c√≥ th·ªÉ sang <a href="#-lap-trinh-tim-nghiem-cho-svm">M·ª•c 4</a> ƒë·ªÉ xem v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng SVM khi l·∫≠p tr√¨nh.</em></p>

<p><strong>X√°c ƒë·ªãnh class cho m·ªôt ƒëi·ªÉm d·ªØ li·ªáu m·ªõi:</strong> Sau khi t√¨m ƒë∆∞·ª£c m·∫∑t ph√¢n c√°ch \(\mathbf{w}^T\mathbf{x} + b = 0\), class c·ªßa b·∫•t k·ª≥ m·ªôt ƒëi·ªÉm n√†o s·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh ƒë∆°n gi·∫£n b·∫±ng c√°ch:</p>

<p>\[
\text{class}(\mathbf{x}) = \text{sgn} (\mathbf{w}^T\mathbf{x} + b )
\]
Trong ƒë√≥ h√†m \(\text{sgn}\) l√† h√†m x√°c ƒë·ªãnh d·∫•u, nh·∫≠n gi√° tr·ªã 1 n·∫øu ƒë·ªëi s·ªë l√† kh√¥ng √¢m v√† -1 n·∫øu ng∆∞·ª£c l·∫°i.</p>

<p><a name="-bai-toan-doi-ngau-cho-svm"></a></p>

<h2 id="3-b√†i-to√°n-ƒë·ªëi-ng·∫´u-cho-svm">3. B√†i to√°n ƒë·ªëi ng·∫´u cho SVM</h2>
<p>Nh·∫Øc l·∫°i r·∫±ng b√†i to√°n t·ªëi ∆∞u \((3)\) l√† m·ªôt b√†i to√°n l·ªìi. Ch√∫ng ta bi·∫øt r·∫±ng: n·∫øu m·ªôt <a href="/2017/04/02/duality/#-strong-duality-va-slaters-constraint-qualification">b√†i to√°n l·ªìi tho·∫£ m√£n ti√™u chu·∫©n Slater th√¨ <em>strong duality</em> tho·∫£ m√£n</a>. V√† n·∫øu <em>strong duality</em> tho·∫£ m√£n th√¨ nghi·ªám c·ªßa b√†i to√°n ch√≠nh l√† nghi·ªám c·ªßa h·ªá <a href="/2017/04/02/duality/#-kkt-optimality-conditions">ƒëi·ªÅu ki·ªán KKT</a>.</p>

<p><a name="-kiem-tra-tieu-chuan-slater"></a></p>

<h3 id="31-ki·ªÉm-tra-ti√™u-chu·∫©n-slater">3.1. Ki·ªÉm tra ti√™u chu·∫©n Slater</h3>
<p>B∆∞·ªõc ti·∫øp theo, ch√∫ng ta s·∫Ω ch·ª©ng minh b√†i to√°n t·ªëi ∆∞u \((3)\) tho·∫£ m√£n ƒëi·ªÅu ki·ªán Slater. ƒêi·ªÅu ki·ªán Slater n√≥i r·∫±ng, n·∫øu t·ªìn t·∫°i \(\mathbf{w}, b\) tho·∫£ m√£n:
\[
1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) &lt; 0, ~~\forall n = 1, 2, \dots, N
\]
th√¨ <em>strong duality</em> tho·∫£ m√£n.</p>

<p>Vi·ªác ki·ªÉm tra n√†y t∆∞∆°ng ƒë·ªëi ƒë∆°n gi·∫£n. V√¨ ta bi·∫øt r·∫±ng lu√¥n lu√¥n c√≥ m·ªôt (si√™u) m·∫∑t ph·∫≥ng ph√¢n chia hai classes n·∫øu hai class ƒë√≥ l√† <em>linearly separable</em>, t·ª©c b√†i to√°n c√≥ nghi·ªám, n√™n <em>feasible set</em> c·ªßa b√†i to√°n t·ªëi ∆∞u \((3)\) ph·∫£i kh√°c r·ªóng. T·ª©c lu√¥n lu√¥n t·ªìn t·∫°i c·∫∑p \((\mathbf{w}_0, b_0)\) sao cho:
\[
\begin{eqnarray}
1 - y_n(\mathbf{w}_0^T\mathbf{x}_n + b_0) &amp;\leq&amp; 0, ~~\forall n = 1, 2, \dots, N \newline
\Leftrightarrow 2 - y_n(2\mathbf{w}_0^T\mathbf{x}_n + 2b_0) &amp;\leq&amp; 0, ~~\forall n = 1, 2, \dots, N 
\end{eqnarray}
\]</p>

<p>V·∫≠y ch·ªâ c·∫ßn ch·ªçn \(\mathbf{w}_1 = 2\mathbf{w}_0\) v√† \(b_1 = 2b_0\), ta s·∫Ω c√≥: 
\[
1 - y_n(\mathbf{w}_1^T\mathbf{x}_n + b_1) \leq -1 &lt; 0, ~~\forall n = 1, 2, \dots, N
\]</p>

<p>T·ª´ ƒë√≥ suy ra ƒëi·ªÅu ki·ªán Slater tho·∫£ m√£n.</p>

<p><a name="-lagrangian-cua-bai-toan-svm"></a></p>

<h3 id="32-lagrangian-c·ªßa-b√†i-to√°n-svm">3.2. Lagrangian c·ªßa b√†i to√°n SVM</h3>
<p><a href="/2017/04/02/duality/#-lagrangian">Lagrangian</a> c·ªßa b√†i to√°n \((3)\) l√†: 
\[
\mathcal{L}(\mathbf{w}, b, \lambda) = \frac{1}{2} ||\mathbf{w}||_2^2 + \sum_{n=1}^N \lambda_n(1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) ) ~~~~~~(4)
\]</p>

<p>v·ªõi \(\lambda = [\lambda_1, \lambda_2, \dots, \lambda_N]^T\) v√† \(\lambda_n \geq 0, ~\forall n = 1, 2, \dots, N\).
<a name="-ham-doi-ngau-lagrange"></a></p>

<h3 id="33-h√†m-ƒë·ªëi-ng·∫´u-lagrange">3.3. H√†m ƒë·ªëi ng·∫´u Lagrange</h3>
<p><a href="/2017/04/02/duality/#-ham-doi-ngau-lagrange-the-lagrange-dual-function">H√†m ƒë·ªëi ng·∫´u Lagrange</a> ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†: 
\[
g(\lambda) = \min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \lambda) 
\]
v·ªõi \(\lambda \succeq 0\).</p>

<p>Vi·ªác t√¨m gi√° tr·ªã nh·ªè nh·∫•t c·ªßa h√†m n√†y theo \(\mathbf{w}\) v√† \(b\) c√≥ th·ªÉ ƒë·ª±·ª£c th·ª±c hi·ªán b·∫±ng c√°ch gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m c·ªßa \(\mathcal{L}(\mathbf{w}, b, \lambda)\) theo \(\mathbf{w}\) v√† \(b\) b·∫±ng 0:</p>

<p>\[
\begin{eqnarray}
\frac{\partial \mathcal{L}(\mathbf{w}, b, \lambda)}{\partial \mathbf{w}} &amp;=&amp; \mathbf{w} - \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n = 0 \Rightarrow \mathbf{w} = \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n  ~~~~~ (5)\newline
\frac{\partial \mathcal{L}(\mathbf{w}, b, \lambda)}{\partial b} &amp;=&amp; 
-\sum_{n=1}^N \lambda_ny_n = 0 ~~~~~~~~~~(6)
\end{eqnarray}
\]</p>

<p>Thay \((5)\) v√† \((6)\) v√†o \((4)\) ta thu ƒë∆∞·ª£c \(g(\lambda)\)(<em>ph·∫ßn n√†y t√¥i r√∫t g·ªçn, coi nh∆∞ m·ªôt b√†i t·∫≠p nh·ªè cho b·∫°n n√†o mu·ªën hi·ªÉu s√¢u</em>):
\[
g(\lambda) = \sum_{n=1}^N \lambda_n  -\frac{1}{2}\sum_{n=1}^N \sum_{m=1}^N \lambda_n\lambda_m y_n y_m \mathbf{x}_n^T\mathbf{x}_m~~~~~~~~~(7)
\]</p>

<p><strong>ƒê√¢y l√† h√†m s·ªë quan tr·ªçng nh·∫•t trong SVM</strong>, c√°c b·∫°n s·∫Ω th·∫•y r√µ h∆°n ·ªü b√†i sau.</p>

<!-- X√©t ma tr·∫≠n vu√¥ng \\(\mathbf{P} \in \mathbb{R}^{N \times N}\\) v·ªõi ph·∫ßn t·ª≠ ·ªü h√†ng th·ª© \\(n\\) v√† c·ªôt th·ª© \\(m\\): 
\\[
p_{nm} =  y_n y_m \mathbf{x}\_n^T\mathbf{x}\_m = p_{mn}
\\] -->

<p>X√©t ma tr·∫≠n:
\[
\mathbf{V} = \left[y_1 \mathbf{x}_1, y_2 \mathbf{x}_2, \dots, y_N \mathbf{x}_N \right]
\]
v√† vector \(\mathbf{1} = [1, 1, \dots, 1]^T\), ta c√≥ th·ªÉ vi·∫øt l·∫°i \(g(\lambda)\) d∆∞·ªõi d·∫°ng: 
\[
g(\lambda) = -\frac{1}{2}\lambda^T\mathbf{V}^T\mathbf{V}\mathbf{\lambda} + \mathbf{1}^T\lambda. ~~~~~~~~~~~~~~~(8)
\]</p>

<p>(<em>N·∫øu kh√≥ tin, b·∫°n c√≥ th·ªÉ vi·∫øt ra ƒë·ªÉ quen d·∫ßn v·ªõi c√°c bi·ªÉu th·ª©c ƒë·∫°i s·ªë tuy·∫øn t√≠nh.</em>)</p>

<p>ƒê·∫∑t \(\mathbf{K} = \mathbf{V}^T\mathbf{V}\), ta c√≥ m·ªôt quan s√°t quan tr·ªçng: \(\mathbf{K}\) l√† m·ªôt <a href="/2017/03/12/convexity/#positive-semidefinite">ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng</a>. Th·∫≠t v·∫≠y, v·ªõi m·ªçi vector \(\lambda\), ta c√≥:
\[
\lambda^T\mathbf{K}\mathbf{\lambda} = \lambda^T\mathbf{V}^T\mathbf{V}\mathbf{\lambda} = ||\mathbf{V}\lambda||_2^2 \geq 0.
\]</p>

<p>(<em>ƒê√¢y ch√≠nh l√† ƒë·ªãnh nghƒ©a c·ªßa ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng.</em>)</p>

<p>V·∫≠y \(g(\lambda) = -\frac{1}{2}\lambda^T\mathbf{K}\mathbf{\lambda} + \mathbf{1}^T\lambda\) l√† m·ªôt <a href="/2017/03/12/convexity/#concave-function">h√†m <em>concave</em></a>.</p>

<p><a name="-bai-toan-doi-ngau-lagrange"></a></p>

<h3 id="34-b√†i-to√°n-ƒë·ªëi-ng·∫´u-lagrange">3.4. B√†i to√°n ƒë·ªëi ng·∫´u Lagrange</h3>
<p>T·ª´ ƒë√≥, k·∫øt h·ª£p h√†m ƒë·ªëi ng·∫´u Lagrange v√† c√°c ƒëi·ªÅu ki·ªán r√†ng bu·ªôc c·ªßa \(\lambda\), ta s·∫Ω thu ƒë∆∞·ª£c <a href="/2017/04/02/duality/#-bai-toan-doi-ngau-lagrange-the-lagrange-dual-problem">b√†i to√°n ƒë·ªëi ng·∫´u Lagrange</a>:</p>

<p>\[
 \begin{eqnarray}
     \lambda &amp;=&amp; \arg \max_{\lambda} g(\lambda)   \newline
     \text{subject to:}~ &amp;&amp; \lambda \succeq 0~~~~~~~~~~ (9)\newline
     &amp;&amp; \sum_{n=1}^N \lambda_ny_n = 0 
 \end{eqnarray}
 \] 
R√†ng bu·ªôc th·ª© hai ƒë∆∞·ª£c l·∫•y t·ª´ \((6)\).</p>

<p>ƒê√¢y l√† m·ªôt b√†i to√°n l·ªìi v√¨ ta ƒëang ƒëi t√¨m gi√° tr·ªã l·ªõn nh·∫•t c·ªßa m·ªôt h√†m m·ª•c ti√™u l√† <em>concave</em> tr√™n m·ªôt <a href="/2017/03/12/convexity/#-giao-cua-cac-tap-loi-la-mot-tap-loi"><em>polyhedron</em></a>.</p>

<p>B√†i to√°n n√†y c≈©ng ƒë∆∞·ª£c l√† m·ªôt Quadratic Programming v√† c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i b·∫±ng c√°c th∆∞ vi·ªán nh∆∞ CVXOPT.</p>

<p>Trong b√†i to√°n ƒë·ªëi ng·∫´u n√†y, s·ªë tham s·ªë (parameters) ph·∫£i t√¨m l√† \(N\), l√† chi·ªÅu c·ªßa \(\lambda\), t·ª©c s·ªë ƒëi·ªÉm d·ªØ li·ªáu. Trong khi ƒë√≥, v·ªõi b√†i to√°n g·ªëc \((3)\), s·ªë tham s·ªë ph·∫£i t√¨m l√† \(d + 1\), l√† t·ªïng s·ªë chi·ªÅu c·ªßa \(\mathbf{w}\) v√† \(b\), t·ª©c s·ªë chi·ªÅu c·ªßa m·ªói ƒëi·ªÉm d·ªØ li·ªáu c·ªông v·ªõi 1. Trong r·∫•t nhi·ªÅu tr∆∞·ªùng h·ª£p, s·ªë ƒëi·ªÉm d·ªØ li·ªáu c√≥ ƒë∆∞·ª£c trong <em>training set</em> l·ªõn h∆°n s·ªë chi·ªÅu d·ªØ li·ªáu r·∫•t nhi·ªÅu. N·∫øu gi·∫£i tr·ª±c ti·∫øp b·∫±ng c√°c c√¥ng c·ª• gi·∫£i Quadratic Programming, c√≥ th·ªÉ b√†i to√°n ƒë·ªëi ng·∫´u c√≤n ph·ª©c t·∫°p h∆°n (t·ªën th·ªùi gian h∆°n) so v·ªõi b√†i to√†n g·ªëc. Tuy nhi√™n, ƒëi·ªÅu h·∫•p d·∫´n c·ªßa b√†i to√°n ƒë·ªëi ng·∫´u n√†y ƒë·∫øn t·ª´ ph·∫ßn <em>Kernel Support Vector Machine (Kernel SVM)</em>, t·ª©c cho c√°c b√†i to√°n m√† d·ªØ li·ªáu kh√¥ng ph·∫£i l√† <em>linearly separable</em> ho·∫∑c <em>g·∫ßn linearly separable</em>. Ph·∫ßn <em>Kernel SVM</em> s·∫Ω ƒë∆∞·ª£c t√¥i tr√¨nh b√†y sau 1 ho·∫∑c 2 b√†i n·ªØa. Ngo√†i ra, d·ª±a v√†o t√≠nh ch·∫•t ƒë·∫∑c bi·ªát c·ªßa h·ªá ƒëi·ªÅu ki·ªán KKT m√† SVM c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i b·∫±ng nhi·ªÅu ph∆∞∆°ng ph√°p hi·ªáu qu·∫£ h∆°n.</p>

<p><a name="-dieu-kien-kkt"></a></p>

<h3 id="35-ƒëi·ªÅu-ki·ªán-kkt">3.5. ƒêi·ªÅu ki·ªán KKT</h3>
<p>Quay tr·ªü l·∫°i b√†i to√°n, v√¨ ƒë√¢y l√† m·ªôt b√†i to√°n l·ªìi v√† <em>strong duality</em> tho·∫£ m√£n, nghi·ªám c·ªßa b√†i to√°n s·∫Ω tho·∫£ m√£n h·ªá <a href="/2017/04/02/duality/#-kkt-optimality-conditions">ƒëi·ªÅu ki·ªán KKT</a> sau ƒë√¢y v·ªõi bi·∫øn s·ªë l√† \(\mathbf{w}, b\) v√† \(\lambda\): 
\[
\begin{eqnarray}
1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) &amp;\leq&amp; 0, ~ \forall n = 1, 2, \dots, N ~~~~(10) \newline
\lambda_n &amp;\geq&amp; 0, ~\forall n = 1, 2, \dots, N  \newline
\lambda_n (1 - y_n(\mathbf{w}^T\mathbf{x}_n + b)) &amp;=&amp; 0, ~\forall n = 1, 2, \dots, N ~~~~(11) \newline
 \mathbf{w} &amp;=&amp; \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n ~~~~~~~~~~~(12)\newline 
 \sum_{n=1}^N \lambda_ny_n &amp;=&amp; 0 ~~~~~~~~~~~~~~~~~~~(13)
\end{eqnarray}
\]</p>

<p>Trong nh·ªØng ƒëi·ªÅu ki·ªán tr√™n, ƒëi·ªÅu ki·ªán \((11)\) l√† th√∫ v·ªã nh·∫•t. T·ª´ ƒë√≥ ta c√≥ th·ªÉ suy ra ngay, v·ªõi \(n\) b·∫•t k·ª≥, ho·∫∑c \(\lambda_n =0\) ho·∫∑c \(1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) = 0\). Tr∆∞·ªùng h·ª£p th·ª© hai ch√≠nh l√†:
\[
\mathbf{w}^T\mathbf{x}_n + b = y_n~~~~ (14)
\] 
v·ªõi ch√∫ √Ω r·∫±ng \(y_n^2 = 1, ~\forall n\).</p>

<p>Nh·ªØng ƒëi·ªÉm tho·∫£ m√£n \((14)\) ch√≠nh l√† nh·ªØng ƒëi·ªÉm n·∫±m g·∫ßn m·∫∑t ph√¢n chia nh·∫•t, l√† nh·ªØng ƒëi·ªÉm ƒë∆∞·ª£c khoanh tr√≤n trong H√¨nh 4 ph√≠a tr√™n. Hai ƒë∆∞·ªùng th·∫≥ng \(\mathbf{w}^T\mathbf{x}_n + b = \pm 1\) <em>t·ª±a</em> l√™n c√°c ƒëi·ªÉm tho·∫£ m√£n \((14)\). V·∫≠y n√™n nh·ªØng ƒëi·ªÉm (vectors) tho·∫£ m√£n \((14)\) c√≤n ƒë∆∞·ª£c g·ªçi l√† c√°c <em>Support Vectors</em>. V√† t·ª´ ƒë√≥, c√°i t√™n <em>Support Vector Machine</em> ra ƒë·ªùi.</p>

<p>M·ªôt quan s√°t kh√°c, s·ªë l∆∞·ª£ng nh·ªØng ƒëi·ªÉm tho·∫£ m√£n \((14)\) th∆∞·ªùng chi·∫øm s·ªë l∆∞·ª£ng r·∫•t nh·ªè trong s·ªë \(N\) ƒëi·ªÉm. Ch·ªâ c·∫ßn d·ª±a tr√™n nh·ªØng <em>support vectors</em> n√†y, ch√∫ng ta ho√†n to√†n c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c m·∫∑t ph√¢n c√°ch c·∫ßn t√¨m. Nh√¨n theo m·ªôt c√°ch kh√°c, h·∫ßu h·∫øt c√°c \(\lambda_n\) b·∫±ng 0. V·∫≠y l√† m·∫∑c d√π vector \(\lambda \in \mathbb{R}^N\) c√≥ s·ªë chi·ªÅu c√≥ th·ªÉ r·∫•t l·ªõn, s·ªë l∆∞·ª£ng c√°c ph·∫ßn t·ª≠ kh√°c 0 c·ªßa n√≥ r·∫•t √≠t. N√≥i c√°ch kh√°c, vector \(\lambda\) l√† m·ªôt <em>sparse</em> vector. Support Vector Machine v√¨ v·∫≠y c√≤n ƒë∆∞·ª£c x·∫øp v√†o <em>Sparse Models</em>. C√°c <em>Sparse Models</em> th∆∞·ªùng c√≥ c√°ch gi·∫£i hi·ªáu qu·∫£ (nhanh) h∆°n c√°c m√¥ h√¨nh t∆∞∆°ng t·ª± v·ªõi nghi·ªám l√† <em>dense</em> (h·∫ßu h·∫øt kh√°c 0). ƒê√¢y ch√≠nh l√† l√Ω do th·ª© hai c·ªßa vi·ªác b√†i to√°n ƒë·ªëi ng·∫´u SVM ƒë∆∞·ª£c quan t√¢m nhi·ªÅu h∆°n l√† b√†i to√°n g·ªëc.</p>

<p>Ti·∫øp t·ª•c ph√¢n t√≠ch, v·ªõi nh·ªØng b√†i to√°n c√≥ s·ªë ƒëi·ªÉm d·ªØ li·ªáu \(N\) nh·ªè, ta c√≥ th·ªÉ gi·∫£i h·ªá ƒëi·ªÅu ki·ªán KKT ph√≠a tr√™n b·∫±ng c√°ch x√©t c√°c tr∆∞·ªùng h·ª£p \(\lambda_n = 0\) ho·∫∑c \(\lambda_n \neq 0\). T·ªïng s·ªë tr∆∞·ªùng h·ª£p ph·∫£i x√©t l√† \(2^N\). V·ªõi \(N &gt; 50\) (th∆∞·ªùng l√† nh∆∞ th·∫ø), ƒë√¢y l√† m·ªôt con s·ªë r·∫•t l·ªõn, gi·∫£i b·∫±ng c√°ch n√†y s·∫Ω kh√¥ng kh·∫£ thi. T√¥i s·∫Ω kh√¥ng ƒëi s√¢u ti·∫øp v√†o vi·ªác gi·∫£i h·ªá KKT nh∆∞ th·∫ø n√†o, trong ph·∫ßn ti·∫øp theo ch√∫ng ta s·∫Ω gi·∫£i b√†i to√°n t·ªëi ∆∞u \((9)\) b·∫±ng CVXOPT v√† b·∫±ng th∆∞ vi·ªán <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<p>Sau khi t√¨m ƒë∆∞·ª£c \(\lambda\) t·ª´ b√†i to√°n \((9)\), ta c√≥ th·ªÉ suy ra ƒë∆∞·ª£c \(\mathbf{w}\) d·ª±a v√†o \((12)\) v√† \(b\) d·ª±a v√†o \((11)\) v√† \((13)\). R√µ r√†ng ta ch·ªâ c·∫ßn quan t√¢m t·ªõi \(\lambda_n \neq 0\).</p>

<p>G·ªçi t·∫≠p h·ª£p \(\mathcal{S} = \{n: \lambda_n \neq 0\}\) v√† \(N_{\mathcal{S}}\) l√† s·ªë ph·∫ßn t·ª≠ c·ªßa t·∫≠p \(\mathcal{S}\). V·ªõi m·ªói \(n \in \mathcal{S}\), ta c√≥:
\[
1 = y_n(\mathbf{w}^T\mathbf{x}_n + b) \Leftrightarrow b + \mathbf{w}^T\mathbf{x}_n = y_n 
\]
M·∫∑c d√π t·ª´ ch·ªâ m·ªôt c·∫∑p \((\mathbf{x}_n, y_n)\), ta c√≥ th·ªÉ suy ra ngay ƒë∆∞·ª£c \(b\) n·∫øu ƒë√£ bi·∫øt \(\mathbf{w}\), m·ªôt phi√™n b·∫£n kh√°c ƒë·ªÉ t√≠nh \(b\) th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng v√† ƒë∆∞·ª£c cho l√† <em>·ªïn ƒë·ªãnh h∆°n trong t√≠nh to√°n</em> (<em>numerically more stable</em>) l√†:</p>

<p>\[
b = \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}}(y_n - \mathbf{w}^T\mathbf{x}_n) = \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}} \left(y_n - \sum_{m\in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x}_n\right)~~~~~ (15)
\]</p>

<p>t·ª©c trung b√¨nh c·ªông c·ªßa m·ªçi c√°ch t√≠nh \(b\).</p>

<p>Tr∆∞·ªõc ƒë√≥, \(\mathbf{w}\) ƒë√£ ƒë∆∞·ª£c t√≠nh b·∫±ng: 
\[
\mathbf{w} = \sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m ~~~~~~ (16)
\]
theo \((12)\).</p>

<p>Quan s√°t quan tr·ªçng: ƒê·ªÉ x√°c ƒë·ªãnh m·ªôt ƒëi·ªÉm \(\mathbf{x}\) m·ªõi thu·ªôc v√†o class n√†o, ta c·∫ßn x√°c ƒë·ªãnh d·∫•u c·ªßa bi·ªÉu th·ª©c: 
\[
\mathbf{w}^T\mathbf{x} + b = \sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x} + \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}} \left(y_n - \sum_{m\in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x}_n\right)
\]
Bi·ªÉu th·ª©c n√†y ph·ª• thu·ªôc v√†o c√°ch t√≠nh t√≠ch v√¥ h∆∞·ªõng gi·ªØa c√°c c·∫∑p vector \(\mathbf{x}\) v√† t·ª´ng \(\mathbf{x}_n \in \mathcal{S}\). Nh·∫≠n x√©t quan tr·ªçng n√†y s·∫Ω gi√∫p √≠ch cho ch√∫ng ta trong b√†i Kernal SVM.</p>

<p><a name="-lap-trinh-tim-nghiem-cho-svm"></a></p>

<h2 id="4-l·∫≠p-tr√¨nh-t√¨m-nghi·ªám-cho-svm">4. L·∫≠p tr√¨nh t√¨m nghi·ªám cho SVM</h2>
<p>Trong m·ª•c n√†y, t√¥i s·∫Ω tr√¨nh b√†y hai c√°ch t√≠nh nghi·ªám cho SVM. C√°ch th·ª© nh·∫•t d·ª±a theo b√†i to√°n \((9)\) v√† c√°c c√¥ng th·ª©c \((15)\) v√† \((16)\). C√°ch th·ª© hai s·ª≠ d·ª•ng tr·ª±c ti·∫øp th∆∞ vi·ªán <code class="language-plaintext highlighter-rouge">sklearn</code>. C√°ch th·ª© nh·∫•t ch·ªâ l√† ƒë·ªÉ ch·ª©ng minh n√£y gi·ªù t√¥i kh√¥ng <em>vi·∫øt nh·∫£m</em>, b·∫±ng c√°ch minh ho·∫° k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c v√† so s√°nh v·ªõi nghi·ªám t√¨m ƒë∆∞·ª£c b·∫±ng c√°ch th·ª© hai.</p>

<p><a name="-tim-nghiem-theo-cong-thuc"></a></p>

<h3 id="41-t√¨m-nghi·ªám-theo-c√¥ng-th·ª©c">4.1. T√¨m nghi·ªám theo c√¥ng th·ª©c</h3>
<p>Tr∆∞·ªõc ti√™n ch√∫ng ta g·ªçi c√°c <em>modules</em> c·∫ßn d√πng v√† t·∫°o d·ªØ li·ªáu gi·∫£ (d·ªØ li·ªáu n√†y ch√≠nh l√† d·ªØ li·ªáu t√¥i d√πng trong c√°c h√¨nh ph√≠a tr√™n n√™n ch√∫ng ta bi·∫øt ch·∫Øc r·∫±ng hai classes l√† <em>linearly separable</em>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[.</span><span class="mi">3</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">],</span> <span class="p">[.</span><span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">]]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="c1"># class 1
</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="c1"># class -1 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># all data 
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># labels 
</span></code></pre></div></div>

<p>Ti·∫øp theo, ch√∫ng ta gi·∫£i b√†i to√°n \((9)\) b·∫±ng CVXOPT:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">cvxopt</span> <span class="kn">import</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">solvers</span>
<span class="c1"># build K
</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">))</span> <span class="c1"># see definition of V, K near eq (8)
</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="c1"># all-one vector 
# build A, b, G, h 
</span><span class="n">G</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">))</span> <span class="c1"># for all lambda_n &gt;= 0
</span><span class="n">h</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># the equality constrain is actually y^T lambda = 0
</span><span class="n">b</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> 
<span class="n">solvers</span><span class="p">.</span><span class="n">options</span><span class="p">[</span><span class="s">'show_progress'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">sol</span> <span class="o">=</span> <span class="n">solvers</span><span class="p">.</span><span class="n">qp</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="s">'x'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'lambda = '</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>K·∫øt qu·∫£:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lambda = 
 [[  8.54018321e-01   2.89132533e-10   1.37095535e+00   6.36030818e-10
    4.04317408e-10   8.82390106e-10   6.35001881e-10   5.49567576e-10
    8.33359230e-10   1.20982928e-10   6.86678649e-10   1.25039745e-10
    2.22497367e+00   4.05417905e-09   1.26763684e-10   1.99008949e-10
    2.13742578e-10   1.51537487e-10   3.75329509e-10   3.56161975e-10]]
</code></pre></div></div>

<p>Ta nh·∫≠n th·∫•y r·∫±ng h·∫ßu h·∫øt c√°c gi√° tr·ªã c·ªßa <code class="language-plaintext highlighter-rouge">lambda</code> ƒë·ªÅu r·∫•t nh·ªè, t·ªõi \(10^{-9}\) ho·∫∑c \(10^{-10}\). ƒê√¢y ch√≠nh l√† c√°c gi√° tr·ªã b·∫±ng 0 nh∆∞ng v√¨ sai s·ªë t√≠nh to√°n n√™n n√≥ kh√°c 0 m·ªôt ch√∫t. Ch·ªâ c√≥ 3 gi√° tr·ªã kh√°c 0, ta d·ª± ƒëo√°n l√† s·∫Ω c√≥ 3 ƒëi·ªÉm l√† <em>support vectors</em>.</p>

<p>Ta ƒëi t√¨m <em>support set</em> \(\mathcal{S}\) r·ªìi t√¨m nghi·ªám c·ªßa b√†i to√°n:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span> <span class="c1"># just a small number, greater than 1e-9
</span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">l</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">VS</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">XS</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">yS</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">lS</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">S</span><span class="p">]</span>
<span class="c1"># calculate w and b
</span><span class="n">w</span> <span class="o">=</span> <span class="n">VS</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lS</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yS</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XS</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b = '</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w =  [[-2.00984381  0.64068336]]
b =  4.66856063387
</code></pre></div></div>

<p>Minh ho·∫° k·∫øt qu·∫£:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm4.png" align="center" width="500" />
 <div class="thecap">H√¨nh 5: Minh ho·∫° nghi·ªám t√¨m ƒë∆∞·ª£c b·ªüi SVM.</div>
</div>
<hr />

<p>ƒê∆∞·ªùng m√†u ƒëen ƒë·∫≠m ·ªü gi·ªØa ch√≠nh l√† m·∫∑t ph√¢n c√°ch t√¨m ƒë∆∞·ª£c b·∫±ng SVM. T·ª´ ƒë√¢y c√≥ th·ªÉ th·∫•y <em>nhi·ªÅu kh·∫£ nƒÉng l√† c√°c t√≠nh to√°n c·ªßa ta l√† ch√≠nh x√°c</em>. ƒê·ªÉ ki·ªÉm tra xem c√°c t√≠nh to√°n ph√≠a tr√™n c√≥ ch√≠nh x√°c kh√¥ng, ta c·∫ßn t√¨m nghi·ªám b·∫±ng c√°c c√¥ng c·ª• c√≥ s·∫µn, v√≠ d·ª• nh∆∞ <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<p>Source code cho ph·∫ßn n√†y c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/19_svm/plt/SVM-example.ipynb">·ªü ƒë√¢y</a>.</p>

<p><a name="-tim-nghiem-theo-thu-vien"></a></p>

<h3 id="42-t√¨m-nghi·ªám-theo-th∆∞-vi·ªán">4.2. T√¨m nghi·ªám theo th∆∞ vi·ªán</h3>
<p>Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code></a> ·ªü ƒë√¢y. C√°c b√†i to√°n th·ª±c t·∫ø th∆∞·ªùng s·ª≠ d·ª•ng th∆∞ vi·ªán <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> ƒë∆∞·ª£c vi·∫øt tr√™n ng√¥n ng·ªØ C, c√≥ API cho Python v√† Matlab.</p>

<p>N·∫øu d√πng th∆∞ vi·ªán th√¨ s·∫Ω nh∆∞ sau:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,))</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="c1"># each sample is one row
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">)</span> <span class="c1"># just a big number 
</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span> 

<span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">coef_</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">intercept_</span>
<span class="k">print</span><span class="p">(</span><span class="s">'w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b = '</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w =  [[-2.00971102  0.64194082]]
b =  [ 4.66595309]
</code></pre></div></div>

<p>K·∫øt qu·∫£ n√†y kh√° gi·ªëng v·ªõi k·∫øt qu·∫£ ch√∫ng ta t√¨m ƒë∆∞·ª£c ·ªü ph·∫ßn tr√™n. C√≥ r·∫•t nhi·ªÅu tu·ª≥ ch·ªçn cho SVM, c√°c b·∫°n s·∫Ω d·∫ßn th·∫•y trong c√°c b√†i sau.</p>

<p><a name="-tom-tat-va-thao-luan"></a></p>

<h2 id="5-t√≥m-t·∫Øt-v√†-th·∫£o-lu·∫≠n">5. T√≥m t·∫Øt v√† th·∫£o lu·∫≠n</h2>

<ul>
  <li>
    <p>V·ªõi b√†i to√°n binary classification m√† 2 classes l√† <em>linearly separable</em>, c√≥ v√¥ s·ªë c√°c si√™u m·∫∑t ph·∫≥ng gi√∫p ph√¢n bi·ªát hai classes, t·ª©c m·∫∑t ph√¢n c√°ch. V·ªõi m·ªói m·∫∑t ph√¢n c√°ch, ta c√≥ m·ªôt <em>classifier</em>. Kho·∫£ng c√°ch g·∫ßn nh·∫•t t·ª´ 1 ƒëi·ªÉm d·ªØ li·ªáu t·ªõi m·∫∑t ph√¢n c√°ch ·∫•y ƒë∆∞·ª£c g·ªçi l√† <em>margin</em> c·ªßa classifier ƒë√≥.</p>
  </li>
  <li>
    <p>Support Vector Machine l√† b√†i to√°n ƒëi t√¨m m·∫∑t ph√¢n c√°ch sao cho <em>margin</em> t√¨m ƒë∆∞·ª£c l√† l·ªõn nh·∫•t, ƒë·ªìng nghƒ©a v·ªõi vi·ªác c√°c ƒëi·ªÉm d·ªØ li·ªáu <em>an to√†n nh·∫•t</em> so v·ªõi m·∫∑t ph√¢n c√°ch.</p>
  </li>
  <li>
    <p>B√†i to√°n t·ªëi ∆∞u trong SVM l√† m·ªôt b√†i to√°n l·ªìi v·ªõi h√†m m·ª•c ti√™u l√† <em>stricly convex</em>, nghi·ªám c·ªßa b√†i to√°n n√†y l√† duy nh·∫•t. H∆°n n·ªØa, b√†i to√°n t·ªëi ∆∞u ƒë√≥ l√† m·ªôt Quadratic Programming (QP).</p>
  </li>
  <li>
    <p>M·∫∑c d√π c√≥ th·ªÉ tr·ª±c ti·∫øp gi·∫£i SVM qua b√†i to√°n t·ªëi ∆∞u g·ªëc n√†y, th√¥ng th∆∞·ªùng ng∆∞·ªùi ta th∆∞·ªùng gi·∫£i b√†i to√°n ƒë·ªëi ng·∫´u. B√†i to√°n ƒë·ªëi ng·∫´u c≈©ng l√† m·ªôt QP nh∆∞ng nghi·ªám l√† <em>sparse</em> n√™n c√≥ nh·ªØng ph∆∞∆°ng ph√°p gi·∫£i hi·ªáu qu·∫£ h∆°n.</p>
  </li>
  <li>
    <p>V·ªõi c√°c b√†i to√°n m√† d·ªØ li·ªáu <em>g·∫ßn linearly separable</em> ho·∫∑c <em>nonlinear separable</em>, c√≥ nh·ªØng c·∫£i ti·ªÅn kh√°c c·ªßa SVM ƒë·ªÉ th√≠ch nghi v·ªõi d·ªØ li·ªáu ƒë√≥. M·ªùi b·∫°n ƒë√≥n ƒë·ªçc b√†i ti·∫øp theo.</p>
  </li>
  <li>
    <p><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/19_svm/plt/SVM-example.ipynb">Source code</a>.
<a name="-tai-lieu-tham-khao"></a></p>
  </li>
</ul>

<h2 id="6-t√†i-li·ªáu-tham-kh·∫£o">6. T√†i li·ªáu tham kh·∫£o</h2>

<p>[1] Bishop, Christopher M. ‚ÄúPattern recognition and Machine Learning.‚Äù, Springer  (2006). (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[2] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley &amp; Sons, 2012.</p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code></a></p>

<p>[4] <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">LIBSVM ‚Äì A Library for Support Vector Machines</a></p>
:ET