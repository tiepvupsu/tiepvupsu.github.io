I"ƒe<p><em>TÃ´i xin táº¡m dá»«ng cÃ¡c bÃ i viáº¿t vá» Decision Tree Ä‘á»ƒ chuyá»ƒn sang Deep Learning.
TÃ´i sáº½ quay láº¡i vá»›i cÃ¡c thuáº­t toÃ¡n Machine Learning cá»• Ä‘iá»ƒn khi cÃ³ dá»‹p</em></p>

<p>Trong trang nÃ y: 
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#gioi-thieu">Giá»›i thiá»‡u</a></li>
  <li><a href="#nhung-dau-moc-quan-trong-cua-deep-learning">Nhá»¯ng dáº¥u má»‘c quan trá»ng cá»§a deep learning</a>
    <ul>
      <li><a href="#perceptron-s">Perceptron (60s)</a></li>
      <li><a href="#mlp-va-backpropagation-ra-doi-s">MLP vÃ  Backpropagation ra Ä‘á»i (80s)</a></li>
      <li><a href="#mua-dong-ai-thu-hai-s---dau-s">MÃ¹a Ä‘Ã´ng AI thá»© hai (90s - Ä‘áº§u 2000s)</a></li>
      <li><a href="#cai-ten-duoc-lam-moi----deep-learning-">CÃ¡i tÃªn Ä‘Æ°á»£c lÃ m má»›i â€“ Deep Learning (2006)</a></li>
      <li><a href="#dot-pha-">Äá»™t phÃ¡ (2012)</a></li>
    </ul>
  </li>
  <li><a href="#dieu-gi-mang-den-su-thanh-cong-cua-deep-learning">Äiá»u gÃ¬ mang Ä‘áº¿n sá»± thÃ nh cÃ´ng cá»§a deep learning?</a></li>
  <li><a href="#ket-luan">Káº¿t luáº­n</a></li>
  <li><a href="#tai-lieu-tham-khao">TÃ i liá»‡u tham kháº£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="gioi-thieu"></a></p>

<h2 id="giá»›i-thiá»‡u">Giá»›i thiá»‡u</h2>
<p>NhÆ° Ä‘Ã£ má»™t láº§n nháº¯c Ä‘áº¿n trong <a href="/2016/12/26/introduce/">bÃ i Ä‘áº§u tiÃªn cá»§a blog</a>,
trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘ang len lá»i vÃ o trong cuá»™c sá»‘ng vÃ  áº£nh hÆ°á»Ÿng sÃ¢u rá»™ng tá»›i má»—i
chÃºng ta. Ká»ƒ tá»« khi tÃ´i viáº¿t bÃ i Ä‘áº§u tiÃªn, táº§n suáº¥t chÃºng ta nghe tháº¥y cÃ¡c cá»¥m
tá»« â€˜artificial intelligenceâ€™, â€˜machine learningâ€™, â€˜deep learningâ€™ cÅ©ng ngÃ y má»™t
tÄƒng lÃªn. NguyÃªn nhÃ¢n chÃ­nh dáº«n Ä‘áº¿n viá»‡c nÃ y (vÃ  viá»‡c ra Ä‘á»i blog nÃ y) lÃ  sá»±
xuáº¥t hiá»‡n cá»§a deep learning trong 5-6 nÄƒm gáº§n Ä‘Ã¢y.</p>

<p>Má»™t láº§n ná»¯a xin Ä‘Æ°á»£c dÃ¹ng láº¡i hÃ¬nh váº½ mÃ´ táº£ má»‘i quan há»‡ giá»¯a artificial
intelligence, machine learning, vÃ  deep learning:</p>

<hr />

<div class="imgcap">
<div>
    <img src="/assets/introduce/aimldl.png" width="800" />
</div>
<div class="thecap">Má»‘i quan há»‡ giá»¯a AI, Machine Learning vÃ  Deep Learning. <br /> (Nguá»“n: <a href="https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/">Whatâ€™s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?</a>)</div>
</div>
<hr />

<p>Trong bÃ i viáº¿t nÃ y, tÃ´i sáº½ trÃ¬nh bÃ y sÆ¡ lÆ°á»£c vá» lá»‹ch sá»­ deep learning. Trong cÃ¡c
bÃ i tiáº¿p theo, tÃ´i cÃ³ tham vá»ng viáº¿t tháº­t ká»¹ vá» cÃ¡c thÃ nh pháº§n cÆ¡ báº£n cá»§a cÃ¡c há»‡
thá»‘ng deep learning. Xa hÆ¡n ná»¯a, blog sáº½ cÃ³ thÃªm cÃ¡c bÃ i hÆ°á»›ng dáº«n cho nhiá»u bÃ i
toÃ¡n thá»±c táº¿.</p>

<p><strong>Blog luÃ´n Ä‘Ã³n nháº­n nhá»¯ng Ä‘Ã³ng gÃ³p Ä‘á»ƒ cháº¥t lÆ°á»£ng cÃ¡c bÃ i viáº¿t Ä‘Æ°á»£c tá»‘t hÆ¡n. Náº¿u
báº¡n cÃ³ Ä‘Ã³ng gÃ³p nÃ o, vui lÃ²ng Ä‘á»ƒ láº¡i trong pháº§n comment, tÃ´i sáº½ cáº­p nháº­t bÃ i
viáº¿t cho phÃ¹ há»£p. Cáº£m Æ¡n báº¡n.</strong></p>

<p><a name="nhung-dau-moc-quan-trong-cua-deep-learning"></a></p>

<h2 id="nhá»¯ng-dáº¥u-má»‘c-quan-trá»ng-cá»§a-deep-learning">Nhá»¯ng dáº¥u má»‘c quan trá»ng cá»§a deep learning</h2>
<p>Deep learning Ä‘Æ°á»£c nháº¯c Ä‘áº¿n nhiá»u trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, nhÆ°ng nhá»¯ng ná»n táº£ng cÆ¡ báº£n Ä‘Ã£ xuáº¥t hiá»‡n tá»« ráº¥t lÃ¢u â€¦</p>

<p>ChÃºng ta cÃ¹ng quan sÃ¡t hÃ¬nh dÆ°á»›i Ä‘Ã¢y:</p>
<hr />

<div class="imgcap">
<div>
    <img src="/assets/35_deeplearning/nn_timeline.jpg" width="800" />
</div>
<div class="thecap">Lá»‹ch sá»­ deep learning (Nguá»“n: HÃ¬nh Ä‘Æ°á»£c láº¥y tá»« <a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html"> Deep Learning 101 - Part 1: History and Background </a>. TÃ¡c giáº£ bÃ i viáº¿t khÃ´ng biáº¿t chÃ­nh xÃ¡c nguá»“n gá»‘c cá»§a hÃ¬nh.)</div>
</div>
<hr />

<p><a name="perceptron-s"></a></p>

<h3 id="perceptron-60s">Perceptron (60s)</h3>

<p>Má»™t trong nhá»¯ng ná»n mÃ³ng Ä‘áº§u tiÃªn cá»§a neural network vÃ  deep learning lÃ 
<a href="/2017/01/21/perceptron/">perceptron learning algorithm</a> (hoáº·c gá»n lÃ 
perceptron). Perceptron lÃ  má»™t thuáº­t toÃ¡n supervised learning giÃºp giáº£i quyáº¿t
bÃ i toÃ¡n phÃ¢n lá»›p nhá»‹ phÃ¢n, Ä‘Æ°á»£c khá»Ÿi nguá»“n bá»Ÿi <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank
Rosenblatt</a> nÄƒm 1957 trong má»™t
nghiÃªn cá»©u Ä‘Æ°á»£c tÃ i trá»£ bá»Ÿi VÄƒn phÃ²ng nghiÃªn cá»©u háº£i quÃ¢n Hoa Ká»³ (U.S Office of
Naval Research â€“ <em>tá»« má»™t cÆ¡ quan liÃªn quan Ä‘áº¿n quÃ¢n sá»±</em>). Thuáº­t toÃ¡n perceptron
Ä‘Æ°á»£c chá»©ng minh lÃ  há»™i tá»¥ náº¿u hai lá»›p dá»¯ liá»‡u lÃ  <em>linearly separable</em>. Vá»›i thÃ nh
cÃ´ng nÃ y, nÄƒm 1958, trong má»™t há»™i tháº£o, Rosenblatt Ä‘Ã£ cÃ³ má»™t phÃ¡t biá»ƒu gÃ¢y tranh
cÃ£i. Tá»« phÃ¡t biá»ƒu nÃ y, tá» New York Times Ä‘Ã£ cÃ³ má»™t bÃ i bÃ¡o cho ráº±ng perceptron
Ä‘Æ°á»£c Háº£i quÃ¢n Hoa Ká»³ mong Ä‘á»£i â€œcÃ³ thá»ƒ Ä‘i, nÃ³i chuyá»‡n, nhÃ¬n, viáº¿t, tá»± sinh sáº£n,
vÃ  tá»± nháº­n thá»©c Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a mÃ¬nhâ€. (<em>ChÃºng ta biáº¿t ráº±ng cho tá»›i giá» cÃ¡c
há»‡ thá»‘ng nÃ¢ng cao hÆ¡n perceptron nhiá»u láº§n váº«n chÆ°a thá»ƒ</em>).</p>

<p>Máº·c dÃ¹ thuáº­t toÃ¡n nÃ y mang láº¡i nhiá»u ká»³ vá»ng, nÃ³ nhanh chÃ³ng Ä‘Æ°á»£c chá»©ng minh khÃ´ng thá»ƒ giáº£i quyáº¿t nhá»¯ng bÃ i toÃ¡n Ä‘Æ¡n giáº£n. NÄƒm 1969, <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a> vÃ  <a href="https://en.wikipedia.org/wiki/Seymour_Papert">Seymour Papert</a> trong cuá»‘n sÃ¡ch ná»•i tiáº¿ng <a href="https://en.wikipedia.org/wiki/Perceptrons_(book)">Perceptrons</a> Ä‘Ã£ chá»©ng minh ráº±ng <a href="/2017-02-24-mlp.markdown#-bieu-dien-ham-xor-voi-neural-network">khÃ´ng thá»ƒ â€˜há»câ€™ Ä‘Æ°á»£c hÃ m sá»‘ XOR</a> khi sá»­ dá»¥ng perceptron. PhÃ¡t hiá»‡n nÃ y lÃ m choÃ¡ng vÃ¡ng giá»›i khoa há»c thá»i gian Ä‘Ã³ (<em>bÃ¢y giá» chÃºng ta tháº¥y viá»‡c nÃ y khÃ¡ hiá»ƒn nhiÃªn</em>). Perceptron Ä‘Æ°á»£c chá»©ng minh ráº±ng chá»‰ hoáº¡t Ä‘á»™ng náº¿u dá»¯ liá»‡u lÃ  <em>linearly separable</em>.</p>

<p><em>PhÃ¡t hiá»‡n nÃ y khiáº¿n cho cÃ¡c nghiÃªn cá»©u vá» perceptron bá»‹ giÃ¡n Ä‘oáº¡n gáº§n 20 nÄƒm. Thá»i ká»³ nÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  <strong>MÃ¹a Ä‘Ã´ng AI thá»© nháº¥t (The First AI winter)</strong>.</em></p>

<p>Cho tá»›i khiâ€¦</p>

<p><a name="mlp-va-backpropagation-ra-doi-s"></a></p>

<h3 id="mlp-vÃ -backpropagation-ra-Ä‘á»i-80s">MLP vÃ  Backpropagation ra Ä‘á»i (80s)</h3>

<p><a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a> tá»‘t nghiá»‡p PhD ngÃ nh neural networks nÄƒm 1978. NÄƒm 1986, Ã´ng cÃ¹ng vá»›i hai tÃ¡c giáº£ khÃ¡c xuáº¥t báº£n má»™t bÃ i bÃ¡o khoa há»c trÃªn Nature vá»›i tá»±a Ä‘á» <a href="http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html">â€œLearning representations by back-propagating errorsâ€</a>. Trong bÃ i bÃ¡o nÃ y, nhÃ³m cá»§a Ã´ng chá»©ng minh ráº±ng neural nets vá»›i nhiá»u hidden layer (Ä‘Æ°á»£c gá»i lÃ  multi-layer perceptron hoáº·c MLP) cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n má»™t cÃ¡ch hiá»‡u quáº£ dá»±a trÃªn má»™t quy trÃ¬nh Ä‘Æ¡n giáº£n Ä‘Æ°á»£c gá»i lÃ  <a href="/2017/02/24/mlp/#-backpropagation"><strong>backpropagation</strong></a> (<em>backpropagation lÃ  tÃªn gá»i má»¹ miá»u cá»§a quy táº¯c chuá»—i â€“ chain rule â€“ trong tÃ­nh Ä‘áº¡o hÃ m. Viá»‡c tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ phá»©c táº¡p mÃ´ táº£ quan há»‡ giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a má»™t neural net lÃ  ráº¥t quan trá»ng vÃ¬ háº§u háº¿t cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u Ä‘á»u Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua viá»‡c tÃ­nh Ä‘áº¡o hÃ m, <a href="/2017/01/12/gradientdescent/">gradient descent</a> lÃ  má»™t vÃ­ dá»¥</em>). Viá»‡c nÃ y giÃºp neural nets <em>thoÃ¡t</em> Ä‘Æ°á»£c nhá»¯ng háº¡n cháº¿ cá»§a perceptron vá» viá»‡c chá»‰ biá»ƒu diá»…n Ä‘Æ°á»£c cÃ¡c quan há»‡ tuyáº¿n tÃ­nh. Äá»ƒ biá»ƒu diá»…n cÃ¡c quan há»‡ phi tuyáº¿n, phÃ­a sau má»—i layer lÃ  má»™t hÃ m kÃ­ch hoáº¡t phi tuyáº¿n, vÃ­ dá»¥ hÃ m sigmoid hoáº·c tanh. (ReLU ra Ä‘á»i nÄƒm 2012). Vá»›i hidden layers, neural nets Ä‘Æ°á»£c chá»©ng minh ráº±ng cÃ³ kháº£ nÄƒng xáº¥p xá»‰ háº§u háº¿t báº¥t ká»³ hÃ m sá»‘ nÃ o qua má»™t Ä‘á»‹nh lÃ½ Ä‘Æ°á»£c gá»i lÃ  <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximation theorem</a>. <em>Neurel nets quay trá»Ÿ láº¡i cuá»™c chÆ¡i</em>.</p>

<p>Thuáº­t toÃ¡n nÃ y mang láº¡i má»™t vÃ i thÃ nh cÃ´ng ban Ä‘áº§u, ná»•i trá»™i lÃ  <strong>convolutional neural nets</strong> (convnets hay CNN) (cÃ²n Ä‘Æ°á»£c gá»i lÃ  <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a>) cho bÃ i toÃ¡n nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay Ä‘Æ°á»£c khá»Ÿi nguá»“n bá»Ÿi Yann LeCun táº¡i AT&amp;T Bell Labs (Yann LeCun lÃ  sinh viÃªn sau cao há»c cá»§a Hinton táº¡i Ä‘áº¡i há»c Toronto nÄƒm 1987-1988). DÆ°á»›i Ä‘Ã¢y lÃ  báº£n demo Ä‘Æ°á»£c láº¥y tá»« trang web cá»§a LeNet, network lÃ  má»™t CNN vá»›i 5 layer, cÃ²n Ä‘Æ°á»£c gá»i lÃ  LeNet-5 (1998).</p>

<hr />

<div class="imgcap">
<div>
    <img src="http://yann.lecun.com/exdb/lenet/gifs/asamples.gif" width="500" />
</div>
<div class="thecap">LeNet-5 cho bÃ i toÃ¡n nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay. (Nguá»“n: <a href="http://yann.lecun.com">http://yann.lecun.com</a>)</div>
</div>
<hr />

<p>MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c há»‡ thá»‘ng Ä‘á»c sá»‘ viáº¿t tay trÃªn cÃ¡c check (sÃ©c ngÃ¢n hÃ ng) vÃ  mÃ£ vÃ¹ng bÆ°u Ä‘iá»‡n cá»§a nÆ°á»›c Má»¹.</p>

<p>LeNet lÃ  thuáº­t toÃ¡n tá»‘t nháº¥t thá»i gian Ä‘Ã³ cho bÃ i toÃ¡n nháº­n dáº¡ng áº£nh chá»¯ sá»‘ viáº¿t
tay. NÃ³ tá»‘t hÆ¡n MLP thÃ´ng thÆ°á»ng (vá»›i fully connected layer) vÃ¬ nÃ³ cÃ³ kháº£ nÄƒng
trÃ­ch xuáº¥t Ä‘Æ°á»£c Ä‘áº·c trÆ°ng trong khÃ´ng gian hai chiá»u cá»§a áº£nh thÃ´ng qua cÃ¡c
filters (bá»™ lá»c) hai chiá»u. HÆ¡n ná»¯a, cÃ¡c filter nÃ y nhá» nÃªn viá»‡c lÆ°u trá»¯ vÃ  tÃ­nh
toÃ¡n cÅ©ng tá»‘t hÆ¡n so vá»›i MLP thÃ´ng thÆ°á»ng. (<em>Yan LeCun cÃ³ xuáº¥t phÃ¡t tá»«
Electrical Engineering nÃªn ráº¥t quen thuá»™c vá»›i cÃ¡c bá»™ lá»c.</em>)</p>

<p><a name="mua-dong-ai-thu-hai-s---dau-s"></a></p>

<h3 id="mÃ¹a-Ä‘Ã´ng-ai-thá»©-hai-90s---Ä‘áº§u-2000s">MÃ¹a Ä‘Ã´ng AI thá»© hai (90s - Ä‘áº§u 2000s)</h3>

<p>CÃ¡c mÃ´ hÃ¬nh tÆ°Æ¡ng tá»± Ä‘Æ°á»£c ká»³ vá»ng sáº½ giáº£i quyáº¿t nhiá»u bÃ i toÃ¡n image
classification khÃ¡c. Tuy nhiÃªn, khÃ´ng nhÆ° cÃ¡c chá»¯ sá»‘, cÃ¡c loáº¡i áº£nh khÃ¡c láº¡i ráº¥t
háº¡n cháº¿ vÃ¬ mÃ¡y áº£nh sá»‘ chÆ°a phá»• biáº¿n táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³. áº¢nh Ä‘Æ°á»£c gÃ¡n nhÃ£n láº¡i cÃ ng
hiáº¿m. Trong khi Ä‘á»ƒ cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘Æ°á»£c mÃ´ hÃ¬nh convnets, ta cáº§n ráº¥t nhiá»u dá»¯
liá»‡u huáº¥n luyá»‡n. Ngay cáº£ khi dá»¯ liá»‡u cÃ³ Ä‘á»§, má»™t váº¥n Ä‘á» nan giáº£i khÃ¡c lÃ  kháº£ nÄƒng
tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ¡y tÃ­nh thá»i Ä‘Ã³ cÃ²n ráº¥t háº¡n cháº¿.</p>

<p>Má»™t háº¡n cháº¿ khÃ¡c cá»§a cÃ¡c kiáº¿n trÃºc MLP nÃ³i chung lÃ  hÃ m máº¥t mÃ¡t khÃ´ng pháº£i lÃ 
má»™t <a href="/2017/03/12/convexity/#-convex-functions">hÃ m
lá»“i</a>.
Viá»‡c nÃ y khiáº¿n cho viá»‡c tÃ¬m nghiá»‡m tá»‘i Æ°u toÃ n cá»¥c cho bÃ i toÃ¡n tá»‘i Æ°u hÃ m máº¥t
mÃ¡t trá»Ÿ nÃªn ráº¥t khÃ³ khÄƒn. Má»™t váº¥n Ä‘á» khÃ¡c liÃªn quan Ä‘áº¿n giá»›i háº¡n tÃ­nh toÃ¡n cá»§a
mÃ¡y tÃ­nh cÅ©ng khiáº¿n cho viá»‡c huáº¥n luyá»‡n MLP khÃ´ng hiá»‡u quáº£ khi sá»‘ lÆ°á»£ng hidden
layers lá»›n lÃªn. Váº¥n Ä‘á» nÃ y cÃ³ tÃªn lÃ  <strong>vanishing gradient</strong>.</p>

<p>Nháº¯c láº¡i ráº±ng hÃ m kÃ­ch hoáº¡t Ä‘Æ°á»£c sá»­ dá»¥ng thá»i gian Ä‘Ã³ lÃ  sigmoid hoáº·c tanh â€“ lÃ 
cÃ¡c hÃ m bá»‹ cháº·n trong khoáº£ng (0, 1) hoáº·c (-1, 1) (Nháº¯c láº¡i <a href="/2017/01/27/logisticregression/#sigmoid-function">Ä‘áº¡o hÃ m cá»§a hÃ m sigmoid</a> \(\sigma(z)\) lÃ  \(\sigma(z)(1 - \sigma(z))\) lÃ  tÃ­ch cá»§a hai sá»‘ nhá»
hÆ¡n 1). Khi sá»­ dá»¥ng backpropagation Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cho cÃ¡c ma tráº­n há»‡ sá»‘ á»Ÿ cÃ¡c
lá»›p Ä‘áº§u tiÃªn, ta cáº§n pháº£i nhÃ¢n ráº¥t nhiá»u cÃ¡c giÃ¡ trá»‹ nhá» hÆ¡n 1 vá»›i nhau. Viá»‡c
nÃ y khiáº¿n cho nhiá»u Ä‘áº¡o hÃ m thÃ nh pháº§n báº±ng 0 do xáº¥p xá»‰ tÃ­nh toÃ¡n. Khi Ä‘áº¡o hÃ m
cá»§a má»™t thÃ nh pháº§n báº±ng 0, nÃ³ sáº½ khÃ´ng Ä‘Æ°á»£c cáº­p nháº­t thÃ´ng qua gradient descent!</p>

<p>Nhá»¯ng háº¡n cháº¿ nÃ y khiáº¿n cho neural nets má»™t láº§n ná»¯a rÆ¡i vÃ o thá»i ká»³ <em>bÄƒng giÃ¡</em>. VÃ o thá»i Ä‘iá»ƒm nhá»¯ng nÄƒm 1990 vÃ  Ä‘áº§u nhá»¯ng nÄƒm 2000, neural nets dáº§n
Ä‘Æ°á»£c thay tháº¿ bá»Ÿi <a href="/2017/04/09/smv/">support vector machines
â€“SVM</a>. SVMs cÃ³ Æ°u Ä‘iá»ƒm lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘á»ƒ tÃ¬m cÃ¡c tham sá»‘ cá»§a nÃ³ lÃ  má»™t bÃ i toÃ¡n lá»“i â€“ cÃ³
nhiá»u cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hiá»‡u quáº£ giÃºp tÃ¬m nghiá»‡m cá»§a nÃ³. CÃ¡c <a href="/2017/04/22/kernelsmv/">ká»¹ thuáº­t vá»
kernel</a> cÅ©ng phÃ¡t triá»ƒn
giÃºp SVMs giáº£i quyáº¿t Ä‘Æ°á»£c cáº£ cÃ¡c váº¥n Ä‘á» vá» viá»‡c dá»¯ liá»‡u khÃ´ng phÃ¢n biá»‡t tuyáº¿n
tÃ­nh.</p>

<p>Nhiá»u nhÃ  khoa há»c lÃ m machine learning chuyá»ƒn sang nghiÃªn cá»©u SVM trong thá»i gian Ä‘Ã³, trá»« má»™t vÃ i nhÃ  khoa há»c cá»©ng Ä‘áº§uâ€¦</p>

<p><a name="cai-ten-duoc-lam-moi----deep-learning-"></a></p>

<h3 id="cÃ¡i-tÃªn-Ä‘Æ°á»£c-lÃ m-má»›i--deep-learning-2006">CÃ¡i tÃªn Ä‘Æ°á»£c lÃ m má»›i â€“ Deep Learning (2006)</h3>
<p>NÄƒm 2006, Hinton má»™t láº§n ná»¯a cho ráº±ng Ã´ng biáº¿t <a href="https://www.youtube.com/watch?v=mlXzufEk-2E">bá»™ nÃ£o hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o</a>, vÃ  giá»›i thiá»‡u Ã½ tÆ°á»Ÿng cá»§a <em>tiá»n huáº¥n luyá»‡n khÃ´ng giÃ¡m sÃ¡t</em> (<a href="https://metacademy.org/graphs/concepts/unsupervised_pre_training"><em>unsupervised pretraining</em></a>) thÃ´ng qua <a href="https://en.wikipedia.org/wiki/Deep_belief_network">deep belief nets (DBN)</a>. DBN cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° sá»± xáº¿p chá»“ng cÃ¡c unsupervised networks Ä‘Æ¡n giáº£n nhÆ° <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">restricted Boltzman machine</a> hay <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">autoencoders</a>.</p>

<p>Láº¥y vÃ­ dá»¥ vá»›i autoencoder. Má»—i autoencoder lÃ  má»™t neural net vá»›i má»™t hidden
layer. Sá»‘ hidden <a href="/2017/02/24/mlp/#-units">unit</a> Ã­t hÆ¡n sá»‘ input unit, vÃ  sá»‘
output unit báº±ng vá»›i sá»‘ input unit. Network nÃ y Ä‘Æ¡n giáº£n Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ káº¿t
quáº£ á»Ÿ output layer giá»‘ng vá»›i káº¿t quáº£ á»Ÿ input layer (vÃ  vÃ¬ váº­y Ä‘Æ°á»£c gá»i lÃ 
autoencoder). QuÃ¡ trÃ¬nh dá»¯ liá»‡u Ä‘i tá»« input layer tá»›i hidden layer cÃ³ thá»ƒ coi lÃ 
<em>mÃ£ hoÃ¡</em>, quÃ¡ trÃ¬nh dá»¯ liá»‡u Ä‘i tá»« hidden layer ra output layer cÃ³ thá»ƒ Ä‘Æ°á»£c coi
lÃ  <em>giáº£i mÃ£</em>. Khi output giá»‘ng vá»›i input, ta cÃ³ thá»ƒ tháº¥y ráº±ng hidden layer vá»›i
Ã­t unit hÆ¡n cÃ³ Ä‘á»ƒ mÃ£ hoÃ¡ input khÃ¡ thÃ nh cÃ´ng, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c coi mang nhá»¯ng
tÃ­nh cháº¥t cá»§a input. Náº¿u ta bá» output layer, <em>cá»‘ Ä‘á»‹nh</em> (<em>freeze</em>) káº¿t ná»‘i giá»¯a
input vÃ  hidden layer, coi Ä‘áº§u ra cá»§a hidden layer lÃ  má»™t input má»›i, sau Ä‘Ã³ huáº¥n
luyá»‡n má»™t autoencoder khÃ¡c, ta Ä‘Æ°á»£c thÃªm má»™t hidden layer ná»¯a. QuÃ¡ trÃ¬nh nÃ y
tiáº¿p tá»¥c kÃ©o dÃ i ta sáº½ Ä‘Æ°á»£c má»™t network Ä‘á»§ <em>sÃ¢u</em> mÃ  output cá»§a network lá»›n nÃ y
(chÃ­nh lÃ  hidden layer cá»§a autoencoder cuá»‘i cÃ¹ng) mang thÃ´ng tin cá»§a input ban
Ä‘áº§u. Sau Ä‘Ã³ ta cÃ³ thá»ƒ thÃªm cÃ¡c layer khÃ¡c tuá»³ thuá»™c vÃ o bÃ i toÃ¡n (cháº³ng háº¡n thÃªm
softmax layer á»Ÿ cuá»‘i cho bÃ i toÃ¡n classification). Cáº£ network Ä‘Æ°á»£c huáº¥n luyá»‡n
thÃªm má»™t vÃ i epoch ná»¯a. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  <em>tinh chá»‰nh</em> (<em>fine tuning</em>).</p>

<p>Táº¡i sao quÃ¡ trÃ¬nh huáº¥n luyá»‡n nhÆ° trÃªn mang láº¡i nhiá»u lá»£i Ã­ch?</p>

<p>Má»™t trong nhá»¯ng háº¡n cháº¿ Ä‘Ã£ Ä‘á» cáº­p cá»§a MLP lÃ  váº¥n Ä‘á» <em>vanishing gradient</em>. Nhá»¯ng
ma tráº­n trá»ng sá»‘ á»©ng vá»›i cÃ¡c layer Ä‘áº§u cá»§a network ráº¥t khÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ¬
Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t theo cÃ¡c ma tráº­n nÃ y nhá». Vá»›i Ã½ tÆ°á»Ÿng cá»§a DBN, cÃ¡c ma
tráº­n trá»ng sá»‘ á»Ÿ nhá»¯ng hidden layer Ä‘áº§u tiÃªn Ä‘Æ°á»£c <em>tiá»n huáº¥n luyá»‡n</em>
(<em>pretrained</em>). CÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n nÃ y cÃ³ thá»ƒ coi lÃ  giÃ¡ trá»‹ khá»Ÿi
táº¡o tá»‘t cho cÃ¡c hidden layer phÃ­a Ä‘áº§u. Viá»‡c nÃ y giÃºp pháº§n nÃ o trÃ¡nh Ä‘Æ°á»£c sá»±
phiá»n hÃ  cá»§a <em>vanishing gradient</em>.</p>

<p>Ká»ƒ tá»« Ä‘Ã¢y, neural networks vá»›i nhiá»u hidden layer Ä‘Æ°á»£c Ä‘á»•i tÃªn thÃ nh <strong>deep learning</strong>.</p>

<p>Váº¥n Ä‘á» <em>vanishing gradient</em> Ä‘Æ°á»£c giáº£i quyáº¿t pháº§n nÃ o (váº«n chÆ°a thá»±c sá»± triá»‡t
Ä‘á»ƒ), nhÆ°ng váº«n cÃ²n nhá»¯ng váº¥n Ä‘á» khÃ¡c cá»§a deep learning: dá»¯ liá»‡u huáº¥n luyá»‡n quÃ¡
Ã­t, vÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a CPU cÃ²n ráº¥t háº¡n cháº¿ trong viá»‡c huáº¥n luyá»‡n cÃ¡c deep
networks.</p>

<p>NÄƒm 2010, giÃ¡o sÆ° Fei-Fei Li, má»™t giÃ¡o sÆ° ngÃ nh computer vision Ä‘áº§u ngÃ nh táº¡i
Stanford, cÃ¹ng vá»›i nhÃ³m cá»§a bÃ  táº¡o ra má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u cÃ³ tÃªn
<a href="http://www.image-net.org/">ImageNet</a> vá»›i hÃ ng triá»‡u bá»©c áº£nh thuá»™c 1000 lá»›p dá»¯
liá»‡u khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n. Dá»± Ã¡n nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n nhá» vÃ o sá»± bÃ¹ng ná»• cá»§a
internet nhá»¯ng nÄƒm 2000 vÃ  lÆ°á»£ng áº£nh khá»•ng lá»“ Ä‘Æ°á»£c upload lÃªn internet thá»i gian
Ä‘Ã³. CÃ¡c bá»©c áº£nh nÃ y Ä‘Æ°á»£c gÃ¡n nhÃ£n bá»Ÿi ráº¥t nhiá»u ngÆ°á»i (Ä‘Æ°á»£c tráº£ cÃ´ng).</p>

<p>Xem thÃªm <a href="https://www.youtube.com/watch?v=40riCqvRoMs">How we teach computers to understand pictures. Fei-Fei Li</a></p>

<p>Bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c cáº­p nháº­t hÃ ng nÄƒm, vÃ  ká»ƒ tá»« nÄƒm 2010, nÃ³ Ä‘Æ°á»£c dÃ¹ng
trong má»™t cuá»™c thi thÆ°á»ng niÃªn cÃ³ tÃªn <a href="http://www.image-net.org/challenges/LSVRC/">ImageNet Large Scale Visual Recognition
Challenge (ILSVRC)</a>. Trong cuá»™c thi
nÃ y, dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c giao cho cÃ¡c Ä‘á»™i tham gia. Má»—i Ä‘á»™i cáº§n sá»­ dá»¥ng dá»¯
liá»‡u nÃ y Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phÃ¢n lá»›p, cÃ¡c mÃ´ hÃ¬nh nÃ y sáº½ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ
dá»± Ä‘oÃ¡n nhÃ£n cá»§a dá»¯ liá»‡u má»›i (Ä‘Æ°á»£c giá»¯ bá»Ÿi ban tá»• chá»©c). Trong hai nÄƒm 2010 vÃ 
2011, cÃ³ ráº¥t nhiá»u Ä‘á»™i tham gia. CÃ¡c mÃ´ hÃ¬nh trong hai nÄƒm nÃ y chá»§ yáº¿u lÃ  sá»± káº¿t
há»£p cá»§a SVM vá»›i cÃ¡c feature Ä‘Æ°á»£c xÃ¢y dá»±ng bá»Ÿi cÃ¡c bá»™ <em>hand-crafted descriptors</em>
(SIFT, HoG, v.v.). MÃ´ hÃ¬nh giÃ nh chiáº¿n tháº¯ng cÃ³ top-5 error rate lÃ  28% (cÃ ng
nhá» cÃ ng tá»‘t). MÃ´ hÃ¬nh giÃ nh chiáº¿n tháº¯ng nÄƒm 2011 cÃ³ top-5 error rate lÃ  26%.
Cáº£i thiá»‡n khÃ´ng nhiá»u!</p>

<p><em>NgoÃ i lá»: top-5 error rate Ä‘Æ°á»£c tÃ­nh nhÆ° sau. Má»—i mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n 5 nhÃ£n cá»§a
má»™t bá»©c áº£nh. Náº¿u nhÃ£n tháº­t cá»§a bá»©c áº£nh náº±m trong 5 nhÃ£n Ä‘Ã³, ta cÃ³ má»™t Ä‘iá»ƒm Ä‘Æ°á»£c
phÃ¢n lá»›p chÃ­nh xÃ¡c. NgoÃ i ra, bá»©c áº£nh Ä‘Ã³ Ä‘Æ°á»£c coi lÃ  má»™t error. Top-5 error rate
lÃ  tá»‰ lá»‡ sá»‘ bá»©c áº£nh error trong toÃ n bá»™ sá»‘ áº£nh kiá»ƒm thá»­ vá»›i error Ä‘Æ°á»£c tÃ­nh theo
cÃ¡ch nÃ y. Top-1 error cá»™ng vá»›i classification accuracy (pháº§n trÄƒm) chÃ­nh báº±ng
100 pháº§n trÄƒm.</em></p>

<p><a name="dot-pha-"></a></p>

<h3 id="Ä‘á»™t-phÃ¡-2012">Äá»™t phÃ¡ (2012)</h3>
<p>NÄƒm 2012, cÅ©ng táº¡i ILSVRC, Alex Krizhevsky, Ilya Sutskever, vÃ  Geoffrey Hinton
(láº¡i lÃ  Ã´ng) tham gia vÃ  Ä‘áº¡t káº¿t quáº£ top-5 error rate 16%. Káº¿t quáº£ nÃ y lÃ m sá»¯ng
sá» giá»›i nghiÃªn cá»©u thá»i gian Ä‘Ã³. MÃ´ hÃ¬nh lÃ  má»™t Deep Convolutional Neural
Network, sau nÃ y Ä‘Æ°á»£c gá»i lÃ  <a href="https://papers.nips.cc/paper/4824-imagene
t-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>.</p>

<p>Trong bÃ i bÃ¡o nÃ y, ráº¥t nhiá»u cÃ¡c ká»¹ thuáº­t má»›i Ä‘Æ°á»£c giá»›i thiá»‡u. Trong Ä‘Ã³ hai Ä‘Ã³ng
gÃ³p ná»•i báº­t nháº¥t lÃ  <a href="/2017/02/24/mlp/#-relu">hÃ m
ReLU</a> vÃ  dropout. HÃ m
ReLU (\(\text{ReLU}(x) = \max(x, 0)\)) vá»›i cÃ¡ch tÃ­nh vÃ  Ä‘áº¡o hÃ m Ä‘Æ¡n giáº£n (báº±ng 1 khi
Ä‘áº§u vÃ o khÃ´ng Ã¢m, báº±ng 0 khi ngÆ°á»£c láº¡i) giÃºp tá»‘c Ä‘á»™ huáº¥n luyá»‡n tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ.
NgoÃ i ra, viá»‡c ReLU khÃ´ng bá»‹ cháº·n trÃªn bá»Ÿi 1 (nhÆ° softmax hay tanh) khiáº¿n cho
váº¥n Ä‘á» vanishing gradient cÅ©ng Ä‘Æ°á»£c giáº£i quyáº¿t pháº§n nÃ o. Dropout cÅ©ng lÃ  má»™t ká»¹
thuáº­t Ä‘Æ¡n giáº£n vÃ  cá»±c ká»³ hiá»‡u quáº£. Trong quÃ¡ trÃ¬nh training, nhiá»u hidden unit
bá»‹ <em>táº¯t</em> ngáº«u nhiÃªn vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ tham sá»‘ cÃ²n láº¡i.
Trong quÃ¡ trÃ¬nh test, toÃ n bá»™ cÃ¡c unit sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng. CÃ¡ch lÃ m nÃ y khÃ¡ lÃ  cÃ³
lÃ½ khi Ä‘á»‘i chiáº¿u vá»›i con ngÆ°á»i. Náº¿u chá»‰ dÃ¹ng má»™t pháº§n nÄƒng lá»±c Ä‘Ã£ Ä‘em láº¡i hiá»‡u
quáº£ thÃ¬ dÃ¹ng toÃ n bá»™ nÄƒng lá»±c sáº½ mang láº¡i hiá»‡u quáº£ cao hÆ¡n. Viá»‡c nÃ y cÅ©ng giÃºp
cho mÃ´ hÃ¬nh trÃ¡nh Ä‘Æ°á»£c
<a href="/2017/03/04/overfitting/">overfitting</a> vÃ  cÅ©ng
Ä‘Æ°á»£c coi giá»‘ng vá»›i ká»¹ thuáº­t
<a href="https://en.wikipedia.org/wiki/Ensemble_learning"><em>ensemble</em></a> trong cÃ¡c há»‡ thá»‘ng
machine learning khÃ¡c. Vá»›i má»—i cÃ¡ch <em>táº¯t</em> cÃ¡c unit, ta cÃ³ má»™t mÃ´ hÃ¬nh khÃ¡c nhau.
Vá»›i nhiá»u tá»• há»£p unit bá»‹ táº¯t khÃ¡c nhau, ta thu Ä‘Æ°á»£c nhiá»u mÃ´ hÃ¬nh. Viá»‡c káº¿t há»£p
á»Ÿ cuá»‘i cÃ¹ng Ä‘Æ°á»£c coi nhÆ° sá»± káº¿t há»£p cá»§a nhiá»u mÃ´ hÃ¬nh (vÃ  vÃ¬ váº­y, nÃ³ giá»‘ng vá»›i
<em>ensemble learning</em>).</p>

<p>Má»™t trong nhá»¯ng yáº¿u tá»‘ quan trá»ng nháº¥t giÃºp AlexNet thÃ nh cÃ´ng lÃ  viá»‡c sá»­ dá»¥ng
GPU (card Ä‘á»“ hoáº¡) Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. GPU Ä‘Æ°á»£c táº¡o ra cho game thá»§, vá»›i kháº£
nÄƒng cháº¡y song song nhiá»u lÃµi, Ä‘Ã£ trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ cá»±c ká»³ phÃ¹ há»£p vá»›i cÃ¡c
thuáº­t toÃ¡n deep learning, giÃºp tÄƒng tá»‘c thuáº­t toÃ¡n lÃªn nhiá»u láº§n so vá»›i CPU.</p>

<p>Sau AlexNet, táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh giÃ nh giáº£i cao trong cÃ¡c nÄƒm tiáº¿p theo Ä‘á»u lÃ 
cÃ¡c deep networks (ZFNet 2013, GoogLeNet 2014, VGG 2014, ResNet 2015). TÃ´i sáº½
giÃ nh má»™t bÃ i cá»§a blog Ä‘á»ƒ viáº¿t vá» cÃ¡c kiáº¿n trÃºc quan trá»ng nÃ y. Xu tháº¿ chung cÃ³
thá»ƒ tháº¥y lÃ  cÃ¡c mÃ´ hÃ¬nh cÃ ng ngÃ y cÃ ng <em>deep</em>. Xem hÃ¬nh dÆ°á»›i Ä‘Ã¢y.</p>

<hr />

<div class="imgcap">
<div>
    <img src="/assets/35_deeplearning/imagenet_results.png" width="800" />
</div>
<div class="thecap">Káº¿t quáº£ ILSVRC qua cÃ¡c nÄƒm. (Nguá»“n: <a href="https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5">CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more ...</a>)</div>
</div>
<hr />

<p>Nhá»¯ng cÃ´ng ty cÃ´ng nghá»‡ lá»›n cÅ©ng Ä‘á»ƒ Ã½ tá»›i viá»‡c phÃ¡t triá»ƒn cÃ¡c phÃ²ng nghiÃªn cá»©u
deep learning trong thá»i gian nÃ y. Ráº¥t nhiá»u cÃ¡c á»©ng dá»¥ng cÃ´ng nghá»‡ Ä‘á»™t phÃ¡ Ä‘Ã£
Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o cuá»™c sá»‘ng hÃ ng ngÃ y. CÅ©ng ká»ƒ tá»« nÄƒm 2012, sá»‘ lÆ°á»£ng cÃ¡c bÃ i bÃ¡o
khoa há»c vá» deep learning tÄƒng lÃªn theo hÃ m sá»‘ mÅ©. CÃ¡c blog vá» deep learning
cÅ©ng tÄƒng lÃªn tá»«ng ngÃ y.</p>

<p><a name="dieu-gi-mang-den-su-thanh-cong-cua-deep-learning"></a></p>

<h2 id="Ä‘iá»u-gÃ¬-mang-Ä‘áº¿n-sá»±-thÃ nh-cÃ´ng-cá»§a-deep-learning">Äiá»u gÃ¬ mang Ä‘áº¿n sá»± thÃ nh cÃ´ng cá»§a deep learning?</h2>
<p>Ráº¥t nhiá»u nhá»¯ng Ã½ tÆ°á»Ÿng cÆ¡ báº£n cá»§a deep learning Ä‘Æ°á»£c Ä‘áº·t ná»n mÃ³ng tá»« nhá»¯ng nÄƒm
80-90 cá»§a tháº¿ ká»· trÆ°á»›c, tuy nhiÃªn deep learning chá»‰ Ä‘á»™t phÃ¡ trong khoáº£ng 5-6 nÄƒm
nay. VÃ¬ sao?</p>

<p>CÃ³ nhiá»u nhÃ¢n tá»‘ dáº«n Ä‘áº¿n sá»± bÃ¹ng ná»• nÃ y:</p>

<ul>
  <li>
    <p>Sá»± ra Ä‘á»i cá»§a cÃ¡c bá»™ dá»¯ liá»‡u lá»›n Ä‘Æ°á»£c gÃ¡n nhÃ£n.</p>
  </li>
  <li>
    <p>Kháº£ nÄƒng tÃ­nh toÃ¡n song song tá»‘c Ä‘á»™ cao cá»§a GPU.</p>
  </li>
  <li>
    <p>Sá»± ra Ä‘á»i cá»§a ReLU vÃ  cÃ¡c hÃ m kÃ­ch hoáº¡t liÃªn quan lÃ m háº¡n cháº¿ váº¥n Ä‘á» vanishing gradient.</p>
  </li>
  <li>
    <p>Sá»± cáº£i tiáº¿n cá»§a cÃ¡c kiáº¿n trÃºc: GoogLeNet, VGG, ResNet, â€¦ vÃ  cÃ¡c ká»¹ thuáº­t transfer learning, fine tuning.</p>
  </li>
  <li>
    <p>Nhiá»u ká»¹ thuáº­t regularization má»›i: dropout, batch normalization, data augmentation.</p>
  </li>
  <li>
    <p>Nhiá»u thÆ° viá»‡n má»›i há»— trá»£ viá»‡c huáº¥n luyá»‡n deep network vá»›i GPU: theano, caffe, mxnet, tensorflow, pytorch, keras, â€¦</p>
  </li>
  <li>
    <p>Nhiá»u ká»¹ thuáº­t tá»‘i Æ°u má»›i: Adagrad, RMSProp, Adam, â€¦</p>
  </li>
</ul>

<p><a name="ket-luan"></a></p>

<h2 id="káº¿t-luáº­n">Káº¿t luáº­n</h2>
<p>Ráº¥t nhiá»u báº¡n Ä‘á»c cÃ³ yÃªu cáº§u tÃ´i viáº¿t vá» deep learning tá»« lÃ¢u. Tuy nhiÃªn, trÆ°á»›c
Ä‘Ã³ tÃ´i tá»± nháº­n ráº±ng mÃ¬nh chÆ°a Ä‘á»§ kiáº¿n thá»©c vá» lÄ©nh vá»±c nÃ y Ä‘á»ƒ viáº¿t cho Ä‘á»™c giáº£.
Chá»‰ khi cÃ³ nhá»¯ng bÃ i cÆ¡ báº£n vá» machine learning vÃ  báº£n thÃ¢n Ä‘Ã£ tÃ­ch luá»¹ Ä‘Æ°á»£c má»™t
lÆ°á»£ng kiáº¿n thá»©c nháº¥t Ä‘á»‹nh tÃ´i má»›i quyáº¿t Ä‘á»‹nh báº¯t Ä‘áº§u vÃ o chá»§ Ä‘á» Ä‘Æ°á»£c nhiá»u báº¡n
quan tÃ¢m nÃ y.</p>

<p>CÃ¡c thuáº­t toÃ¡n machine learning cá»• Ä‘iá»ƒn khÃ¡c váº«n cÃ³ thá»ƒ xuáº¥t hiá»‡n trong cÃ¡c bÃ i sau cá»§a blog.</p>

<p><a name="tai-lieu-tham-khao"></a></p>

<p><a name="tai-lieu-tham-khao"></a></p>

<h2 id="tÃ i-liá»‡u-tham-kháº£o">TÃ i liá»‡u tham kháº£o</h2>
<p>[1] <a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">Deep Learning 101 - Part 1: History and Background</a></p>

<p>[2] <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">autoencoders</a></p>

<p>[3] <a href="https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5">CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more â€¦</a></p>

<p>[4] <a href="http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/">A â€˜Briefâ€™ History off Neural Nets and Deep Learning</a></p>

<p>[5] <a href="https://medium.com/@Jaconda/a-concise-history-of-neural-networks-2070655d3fec#.alhfvwwl2">A Concise History of Neural Networks</a></p>
:ET