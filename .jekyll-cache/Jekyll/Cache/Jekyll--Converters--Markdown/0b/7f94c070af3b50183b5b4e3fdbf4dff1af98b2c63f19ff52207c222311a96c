I"Ӽ<p>Trong loạt bài tiếp theo, tôi sẽ trình bày về một trong những thuật toán classification phổ biến nhất (cùng với <a href="/2017/02/17/softmax/">softmax regression</a>). Có rất nhiều suy luận toán học trong phần này yêu cầu bạn cần có kiến thức về <a href="/2017/04/02/duality/">Duality</a> cũng như về tối ưu lồi. Bạn được khuyến khích đọc các Bài 16, 17, và 18 trước khi đọc bài này.</p>

<p><em>Nếu không muốn đi sâu vào phần toán, bạn có thể bỏ qua mục 3.</em></p>

<p><strong>Trong trang này:</strong> 
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Giới thiệu</a>
    <ul>
      <li><a href="#-khoang-cach-tu-mot-diem-toi-mot-sieu-mat-phang">1.1. Khoảng cách từ một điểm tới một siêu mặt phẳng</a></li>
      <li><a href="#-nhac-lai-bai-toan-phan-chia-hai-classes">1.2. Nhắc lại bài toán phân chia hai classes</a></li>
    </ul>
  </li>
  <li><a href="#-xay-dung-bai-toan-toi-uu-cho-svm">2. Xây dựng bài toán tối ưu cho SVM</a></li>
  <li><a href="#-bai-toan-doi-ngau-cho-svm">3. Bài toán đối ngẫu cho SVM</a>
    <ul>
      <li><a href="#-kiem-tra-tieu-chuan-slater">3.1. Kiểm tra tiêu chuẩn Slater</a></li>
      <li><a href="#-lagrangian-cua-bai-toan-svm">3.2. Lagrangian của bài toán SVM</a></li>
      <li><a href="#-ham-doi-ngau-lagrange">3.3. Hàm đối ngẫu Lagrange</a></li>
      <li><a href="#-bai-toan-doi-ngau-lagrange">3.4. Bài toán đối ngẫu Lagrange</a></li>
      <li><a href="#-dieu-kien-kkt">3.5. Điều kiện KKT</a></li>
    </ul>
  </li>
  <li><a href="#-lap-trinh-tim-nghiem-cho-svm">4. Lập trình tìm nghiệm cho SVM</a>
    <ul>
      <li><a href="#-tim-nghiem-theo-cong-thuc">4.1. Tìm nghiệm theo công thức</a></li>
      <li><a href="#-tim-nghiem-theo-thu-vien">4.2. Tìm nghiệm theo thư viện</a></li>
    </ul>
  </li>
  <li><a href="#-tom-tat-va-thao-luan">5. Tóm tắt và thảo luận</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. Tài liệu tham khảo</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-giới-thiệu">1. Giới thiệu</h2>
<p>Trước khi đi vào phần ý tưởng chính của Support Vector Machine, tôi xin một lần nữa nhắc lại kiến thức về hình học giải tích mà chúng ta đã quá quen khi ôn thi đại học.</p>

<p><a name="-khoang-cach-tu-mot-diem-toi-mot-sieu-mat-phang"></a></p>

<h3 id="11-khoảng-cách-từ-một-điểm-tới-một-siêu-mặt-phẳng">1.1. Khoảng cách từ một điểm tới một siêu mặt phẳng</h3>
<p>Trong không gian 2 chiều, ta biết rằng khoảng cách từ một điểm có toạ độ \((x_0, y_0)\) tới <em>đường thẳng</em> có phương trình \(w_1x + w_2y + b = 0\) được xác định bởi: 
\[
\frac{|w_1x_0 + w_2y_0 + b|}{\sqrt{w_1^2 + w_2^2}}
\]</p>

<p>Trong không gian ba chiều, khoảng cách từ một điểm có toạ độ \((x_0, y_0, z_0)\) tới một <em>mặt phẳng</em> có phương trình \(w_1x + w_2y + w_3 z + b = 0\) được xác định bởi: 
\[
\frac{|w_1x_0 + w_2y_0 + w_3z_0 + b |}{\sqrt{w_1^2 + w_2^2 + w_3^2}}
\]</p>

<p>Hơn nữa, nếu ta bỏ dấu trị tuyệt đối ở tử số, chúng ta có thể xác định được điểm đó nằm về phía nào của <em>đường thẳng</em> hay <em>mặt phẳng</em> đang xét. Những điểm làm cho biểu thức trong dấu giá trị tuyệt đối mang dấu dương nằm về cùng 1 phía (tôi tạm gọi đây là <em>phía dương</em> của đường thẳng), những điểm làm cho biểu thức trong dấu giá trị tuyệt đối mang dấu âm nằm về phía còn lại (tôi gọ là <em>phía âm</em>). Những điểm nằm trên <em>đường thẳng</em>/<em>măt phẳng</em> sẽ làm cho tử số có giá trị bằng 0, tức khoảng cách bằng 0.</p>

<p>Việc này có thể được tổng quát lên không gian nhiều chiều: Khoảng cách từ một điểm (vector) có toạ độ \(\mathbf{x}_0\) tới <em>siêu mặt phẳng</em> (<em>hyperplane</em>) có phương trình \(\mathbf{w}^T\mathbf{x} + b = 0\) được xác định bởi: 
\[
\frac{|\mathbf{w}^T\mathbf{x}_0 + b|}{||\mathbf{w}||_2}
\]</p>

<p>Với \(||\mathbf{w}||_2 = \sqrt{\sum_{i=1}^d w_i^2}\) với \(d\) là số chiều của không gian.</p>

<p><a name="-nhac-lai-bai-toan-phan-chia-hai-classes"></a></p>

<h3 id="12-nhắc-lại-bài-toán-phân-chia-hai-classes">1.2. Nhắc lại bài toán phân chia hai classes</h3>

<p>Chúng ta cùng quay lại với bài toán trong <a href="/2017/01/21/perceptron/">Perceptron Learning Algorithm (PLA)</a>. Giả sử rằng có hai class khác nhau được mô tả bởi các điểm trong không gian nhiều chiều, hai classes này <em>linearly separable</em>, tức tồn tại một siêu phẳng phân chia chính xác hai classes đó. Hãy tìm một siêu mặt phẳng phân chia hai classes đó, tức tất cả các điểm thuộc một class nằm về cùng một phía của siêu mặt phẳng đó và ngược phía với toàn bộ các điểm thuộc class còn lại. Chúng ta đã biết rằng, thuật toán PLA có thể làm được việc này nhưng nó có thể cho chúng ta vô số nghiệm như Hình 1 dưới đây:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm1.png" align="center" width="500" />
 <div class="thecap">Hình 1: Các mặt phân cách hai classes linearly separable.</div>
</div>
<hr />

<p>Câu hỏi đặt ra là: trong vô số các mặt phân chia đó, đâu là mặt phân chia tốt nhất <em>theo một tiêu chuẩn nào đó</em>? Trong ba đường thẳng minh họa trong Hình 1 phía trên, có hai đường thẳng khá <em>lệch</em> về phía class hình tròn đỏ. Điều này có thể khiến cho lớp màu đỏ <em>không vui vì lãnh thổ xem ra bị lấn nhiều quá</em>. Liệu có cách nào để tìm được đường phân chia mà cả hai classes đều cảm thấy <em>công bằng</em> và <em>hạnh phúc</em> nhất hay không?</p>

<p>Chúng ta cần tìm một tiêu chuẩn để đo sự <em>hạnh phúc</em> của mỗi class. Hãy xem Hình 2 dưới đây:</p>
<hr />

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/19_svm/svm2.png" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/19_svm/svm5.png" />
        </td>

    </tr>

</table>
<div class="thecap"> Hình 2: Margin của hai classes là bằng nhau và lớn nhất có thể.
</div>
</div>
<hr />

<!-- <hr>
<div class="imgcap">
 <img src ="/assets/19_svm/svm2.png" align = "center" width = "500">
 <div class = "thecap">Hình 2: .</div>
</div>
<hr> -->

<p>Nếu ta định nghĩa <em>mức độ hạnh phúc</em> của một class tỉ lệ thuận với khoảng cách gần nhất từ một điểm của class đó tới đường/mặt phân chia, thì ở Hình 2 trái, class tròn đỏ sẽ <em>không được hạnh phúc cho lắm</em> vì đường phân chia gần nó hơn class vuông xanh rất nhiều. Chúng ta cần một đường phân chia sao cho khoảng cách từ điểm gần nhất của mỗi class (các điểm được khoanh tròn) tới đường phân chia là như nhau, như thế thì mới <em>công bằng</em>. Khoảng cách như nhau này được gọi là <em>margin</em> (<em>lề</em>).</p>

<p>Đã có <em>công bằng</em> rồi, chúng ta cần <em>văn minh</em> nữa. <em>Công bằng</em> mà cả hai đều <em>kém hạnh phúc như nhau</em> thì chưa phải là <em>văn mình</em> cho lắm.</p>

<p>Chúng ta xét tiếp Hình 2 bên phải khi khoảng cách từ đường phân chia tới các điểm gần nhất của mỗi class là như nhau. Xét hai cách phân chia bởi đường nét liền màu đen và đường nét đứt màu lục, đường nào sẽ làm cho cả hai class <em>hạnh phúc hơn</em>? Rõ ràng đó phải là đường nét liền màu đen vì nó tạo ra một <em>margin</em> rộng hơn.</p>

<p>Việc <em>margin</em> rộng hơn sẽ mang lại hiệu ứng phân lớp tốt hơn vì <em>sự phân chia giữa hai classes là rạch ròi hơn</em>. Việc này, sau này các bạn sẽ thấy, là một điểm khá quan trọng giúp <em>Support Vector Machine</em> mang lại kết quả phân loại tốt hơn so với <em>Neural Network với 1 layer</em>, tức Perceptron Learning Algorithm.</p>

<p>Bài toán tối ưu trong <em>Support Vector Machine</em> (SVM) chính là bài toán đi tìm đường phân chia sao cho <em>margin</em> là lớn nhất. Đây cũng là lý do vì sao SVM còn được gọi là <em>Maximum Margin Classifier</em>. Nguồn gốc của tên gọi Support Vector Machine sẽ sớm được làm sáng tỏ.</p>

<p><a name="-xay-dung-bai-toan-toi-uu-cho-svm"></a></p>

<h2 id="2-xây-dựng-bài-toán-tối-ưu-cho-svm">2. Xây dựng bài toán tối ưu cho SVM</h2>
<p>Giả sử rằng các cặp dữ liệu của <em>training set</em> là \((\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_N, y_N)\) với vector \(\mathbf{x}_i \in \mathbb{R}^d\) thể hiện <em>đầu vào</em> của một điểm dữ liệu và \(y_i\) là <em>nhãn</em> của điểm dữ liệu đó. \(d\) là số chiều của dữ liệu và \(N\) là số điểm dữ liệu. Giả sử rằng <em>nhãn</em> của mỗi điểm dữ liệu được xác định bởi \(y_i = 1\) (class 1) hoặc \(y_i = -1\) (class 2) giống như trong PLA.</p>

<p>Để giúp các bạn dễ hình dung, chúng ta cùng xét trường hợp trong không gian hai chiều dưới đây. <em>Không gian hai chiều để các bạn dễ hình dung, các phép toán hoàn toàn có thể được tổng quát lên không gian nhiều chiều.</em></p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm6.png" align="center" width="500" />
 <div class="thecap">Hình 3: Phân tích bài toán SVM.</div>
</div>
<hr />

<p>Giả sử rằng các điểm vuông xanh thuộc class 1, các điểm tròn đỏ thuộc class -1 và mặt \(\mathbf{w}^T\mathbf{x} + b = w_1x_1 + w_2x_2 + b = 0\) là mặt phân chia giữa hai classes (Hình 3). Hơn nữa, class 1 nằm về <em>phía dương</em>, class -1 nằm về <em>phía âm</em> của mặt phân chia. Nếu ngược lại, ta chỉ cần đổi dấu của \(\mathbf{w}\) và \(b\). Chú ý rằng chúng ta cần đi tìm các hệ số \(\mathbf{w}\) và \(b\).</p>

<p>Ta quan sát thấy một điểm quan trọng sau đây: với cặp dữ liệu \((\mathbf{x}_n, y_n)\) bất kỳ, khoảng cách từ điểm đó tới mặt phân chia là: 
\[
\frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2}
\]</p>

<p>Điều này có thể dễ nhận thấy vì theo giả sử ở trên, \(y_n\) luôn cùng dấu với <em>phía</em> của \(\mathbf{x}_n\). Từ đó suy ra \(y_n\) cùng dấu với \((\mathbf{w}^T\mathbf{x}_n + b)\), và tử số luôn là 1 số không âm.</p>

<p>Với mặt phần chia như trên, <em>margin</em> được tính là khoảng cách gần nhất từ 1 điểm tới mặt đó (bất kể điểm nào trong hai classes):
\[
\text{margin} = \min_{n} \frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2}
\]</p>

<p>Bài toán tối ưu trong SVM chính là bài toán tìm \(\mathbf{w}\) và \(b\) sao cho <em>margin</em> này đạt giá trị lớn nhất: 
\[
(\mathbf{w}, b) = \arg\max_{\mathbf{w}, b} \left\{
    \min_{n} \frac{y_n(\mathbf{w}^T\mathbf{x}_n + b)}{||\mathbf{w}||_2} 
\right\}
= \arg\max_{\mathbf{w}, b}\left\{
    \frac{1}{||\mathbf{w}||_2} \min_{n} y_n(\mathbf{w}^T\mathbf{x}_n + b)
\right\} ~~~ (1)
\]</p>

<p>Việc giải trực tiếp bài toán này sẽ rất phức tạp, nhưng các bạn sẽ thấy có cách để đưa nó về bài toán đơn giản hơn.</p>

<p>Nhận xét quan trọng nhất là nếu ta thay vector hệ số \(\mathbf{w}\) bởi \(k\mathbf{w}\) và \(b\) bởi \(kb\) trong đó \(k\) là một hằng số dương thì mặt phân chia không thay đổi, tức khoảng cách từ từng điểm đến mặt phân chia không đổi, tức <em>margin</em> không đổi. Dựa trên tính chất này, ta có thể giả sử: 
\[
y_n(\mathbf{w}^T\mathbf{x}_n + b) = 1
\]</p>

<p><strong>với những điểm nằm gần mặt phân chia nhất</strong> như Hình 4 dưới đây:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm3.png" align="center" width="500" />
 <div class="thecap">Hình 4: Các điểm gần mặt phân cách nhất của hai classes được khoanh tròn.</div>
</div>
<hr />

<p>Như vậy, với mọi \(n\), ta có: 
\[
y_n(\mathbf{w}^T\mathbf{x}_n + b) \geq 1
\]</p>

<p>Vậy bài toán tối ưu \((1)\) có thể đưa về bài toán tối ưu có ràng buộc sau đây: 
\[
\begin{eqnarray}
    (\mathbf{w}, b) &amp;=&amp; \arg \max_{\mathbf{w}, b} \frac{1}{||\mathbf{w}||_2}   \<br />
    \text{subject to:}~ &amp;&amp; y_n(\mathbf{w}^T\mathbf{x}_n + b) \geq 1, \forall n = 1, 2, \dots, N ~~~~(2)
\end{eqnarray}
\]</p>

<p>Bằng 1 biến đổi đơn giản, ta có thể đưa bài toán này về bài toán dưới đây:
\[
\begin{eqnarray}
    (\mathbf{w}, b) &amp;=&amp; \arg \min_{\mathbf{w}, b} \frac{1}{2}||\mathbf{w}||_2^2   \<br />
    \text{subject to:}~ &amp;&amp; 1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) \leq 0, \forall n = 1, 2, \dots, N ~~~~ (3)
\end{eqnarray}
\]
Ở đây, chúng ta đã lấy nghịch đảo hàm mục tiêu, bình phương nó để được một hàm khả vi, và nhân với \(\frac{1}{2}\) để biểu thức đạo hàm đẹp hơn.</p>

<p><strong>Quan sát quan trọng:</strong> Trong bài toán \((3)\), <a href="/2017/03/12/convexity/#-norms">hàm mục tiêu là một norm, nên là một hàm lồi</a>. Các hàm bất đẳng thức ràng buộc là các hàm tuyến tính theo \(\mathbf{w}\) và \(b\), nên chúng cũng là các hàm lồi. Vậy bài toán tối ưu \((3)\) có hàm mục tiêu là lồi, và các hàm ràng buộc cũng là lồi, nên nó là một bài toán lồi. Hơn nữa, nó là một <a href="/2017/03/19/convexopt/#-quadratic-programming">Quadratic Programming</a>. Thậm chí, hàm mục tiêu là <em>strictly convex</em> vì \(||\mathbf{w}||_2^2 = \mathbf{w}^T\mathbf{I}\mathbf{w}\) và \(\mathbf{I}\) là ma trận đơn vị - là một ma trận xác định dương. Từ đây có thể suy ra nghiệm cho SVM là <em>duy nhất</em>.</p>

<p>Đến đây thì bài toán này có thể giải được bằng các công cụ hỗ trợ tìm nghiệm cho Quadratic Programing, ví dụ <a href="/2017/03/19/convexopt/#-gioi-thieu-thu-vien-cvxopt">CVXOPT</a>.</p>

<p>Tuy nhiên, việc giải bài toán này trở nên phức tạp khi số chiều \(d\) của không gian dữ liệu và số điểm dữ liệu \(N\) tăng lên cao.</p>

<p>Người ta thường giải <a href="/2017/04/02/duality/#-bai-toan-doi-ngau-lagrange-the-lagrange-dual-problem">bài toán đối ngẫu</a> của bài toán này. Thứ nhất, bài toán đối ngẫu có những tính chất thú vị hơn khiến nó được giải hiệu quả hơn. Thứ hai, trong quá trình xây dựng bài toán đối ngẫu, người ta thấy rằng SVM có thể được áp dụng cho những bài toán mà dữ liệu không <em>linearly separable</em>, tức các đường phân chia không phải là một mặt phẳng mà có thể là các mặt có hình thù phức tạp hơn.</p>

<p><em>Đến đây, bạn đọc có thể bắt đầu hiểu tại sao tôi cần viết 3 bài 16-18 trước khi viết bài này. Nếu bạn muốn hiểu sâu hơn về SVM, tôi khuyến khích đọc Mục 3 dưới đây. Nếu không, bạn có thể sang <a href="#-lap-trinh-tim-nghiem-cho-svm">Mục 4</a> để xem ví dụ về cách sử dụng SVM khi lập trình.</em></p>

<p><strong>Xác định class cho một điểm dữ liệu mới:</strong> Sau khi tìm được mặt phân cách \(\mathbf{w}^T\mathbf{x} + b = 0\), class của bất kỳ một điểm nào sẽ được xác định đơn giản bằng cách:</p>

<p>\[
\text{class}(\mathbf{x}) = \text{sgn} (\mathbf{w}^T\mathbf{x} + b )
\]
Trong đó hàm \(\text{sgn}\) là hàm xác định dấu, nhận giá trị 1 nếu đối số là không âm và -1 nếu ngược lại.</p>

<p><a name="-bai-toan-doi-ngau-cho-svm"></a></p>

<h2 id="3-bài-toán-đối-ngẫu-cho-svm">3. Bài toán đối ngẫu cho SVM</h2>
<p>Nhắc lại rằng bài toán tối ưu \((3)\) là một bài toán lồi. Chúng ta biết rằng: nếu một <a href="/2017/04/02/duality/#-strong-duality-va-slaters-constraint-qualification">bài toán lồi thoả mãn tiêu chuẩn Slater thì <em>strong duality</em> thoả mãn</a>. Và nếu <em>strong duality</em> thoả mãn thì nghiệm của bài toán chính là nghiệm của hệ <a href="/2017/04/02/duality/#-kkt-optimality-conditions">điều kiện KKT</a>.</p>

<p><a name="-kiem-tra-tieu-chuan-slater"></a></p>

<h3 id="31-kiểm-tra-tiêu-chuẩn-slater">3.1. Kiểm tra tiêu chuẩn Slater</h3>
<p>Bước tiếp theo, chúng ta sẽ chứng minh bài toán tối ưu \((3)\) thoả mãn điều kiện Slater. Điều kiện Slater nói rằng, nếu tồn tại \(\mathbf{w}, b\) thoả mãn:
\[
1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) &lt; 0, ~~\forall n = 1, 2, \dots, N
\]
thì <em>strong duality</em> thoả mãn.</p>

<p>Việc kiểm tra này tương đối đơn giản. Vì ta biết rằng luôn luôn có một (siêu) mặt phẳng phân chia hai classes nếu hai class đó là <em>linearly separable</em>, tức bài toán có nghiệm, nên <em>feasible set</em> của bài toán tối ưu \((3)\) phải khác rỗng. Tức luôn luôn tồn tại cặp \((\mathbf{w}_0, b_0)\) sao cho:
\[
\begin{eqnarray}
1 - y_n(\mathbf{w}_0^T\mathbf{x}_n + b_0) &amp;\leq&amp; 0, ~~\forall n = 1, 2, \dots, N \<br />
\Leftrightarrow 2 - y_n(2\mathbf{w}_0^T\mathbf{x}_n + 2b_0) &amp;\leq&amp; 0, ~~\forall n = 1, 2, \dots, N 
\end{eqnarray}
\]</p>

<p>Vậy chỉ cần chọn \(\mathbf{w}_1 = 2\mathbf{w}_0\) và \(b_1 = 2b_0\), ta sẽ có: 
\[
1 - y_n(\mathbf{w}_1^T\mathbf{x}_n + b_1) \leq -1 &lt; 0, ~~\forall n = 1, 2, \dots, N
\]</p>

<p>Từ đó suy ra điều kiện Slater thoả mãn.</p>

<p><a name="-lagrangian-cua-bai-toan-svm"></a></p>

<h3 id="32-lagrangian-của-bài-toán-svm">3.2. Lagrangian của bài toán SVM</h3>
<p><a href="/2017/04/02/duality/#-lagrangian">Lagrangian</a> của bài toán \((3)\) là: 
\[
\mathcal{L}(\mathbf{w}, b, \lambda) = \frac{1}{2} ||\mathbf{w}||_2^2 + \sum_{n=1}^N \lambda_n(1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) ) ~~~~~~(4)
\]</p>

<p>với \(\lambda = [\lambda_1, \lambda_2, \dots, \lambda_N]^T\) và \(\lambda_n \geq 0, ~\forall n = 1, 2, \dots, N\).
<a name="-ham-doi-ngau-lagrange"></a></p>

<h3 id="33-hàm-đối-ngẫu-lagrange">3.3. Hàm đối ngẫu Lagrange</h3>
<p><a href="/2017/04/02/duality/#-ham-doi-ngau-lagrange-the-lagrange-dual-function">Hàm đối ngẫu Lagrange</a> được định nghĩa là: 
\[
g(\lambda) = \min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \lambda) 
\]
với \(\lambda \succeq 0\).</p>

<p>Việc tìm giá trị nhỏ nhất của hàm này theo \(\mathbf{w}\) và \(b\) có thể đựợc thực hiện bằng cách giải hệ phương trình đạo hàm của \(\mathcal{L}(\mathbf{w}, b, \lambda)\) theo \(\mathbf{w}\) và \(b\) bằng 0:</p>

<p>\[
\begin{eqnarray}
\frac{\partial \mathcal{L}(\mathbf{w}, b, \lambda)}{\partial \mathbf{w}} &amp;=&amp; \mathbf{w} - \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n = 0 \Rightarrow \mathbf{w} = \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n  ~~~~~ (5)\<br />
\frac{\partial \mathcal{L}(\mathbf{w}, b, \lambda)}{\partial b} &amp;=&amp; 
-\sum_{n=1}^N \lambda_ny_n = 0 ~~~~~~~~~~(6)
\end{eqnarray}
\]</p>

<p>Thay \((5)\) và \((6)\) vào \((4)\) ta thu được \(g(\lambda)\)(<em>phần này tôi rút gọn, coi như một bài tập nhỏ cho bạn nào muốn hiểu sâu</em>):
\[
g(\lambda) = \sum_{n=1}^N \lambda_n  -\frac{1}{2}\sum_{n=1}^N \sum_{m=1}^N \lambda_n\lambda_m y_n y_m \mathbf{x}_n^T\mathbf{x}_m~~~~~~~~~(7)
\]</p>

<p><strong>Đây là hàm số quan trọng nhất trong SVM</strong>, các bạn sẽ thấy rõ hơn ở bài sau.</p>

<!-- Xét ma trận vuông \\(\mathbf{P} \in \mathbb{R}^{N \times N}\\) với phần tử ở hàng thứ \\(n\\) và cột thứ \\(m\\): 
\\[
p_{nm} =  y_n y_m \mathbf{x}\_n^T\mathbf{x}\_m = p_{mn}
\\] -->

<p>Xét ma trận:
\[
\mathbf{V} = \left[y_1 \mathbf{x}_1, y_2 \mathbf{x}_2, \dots, y_N \mathbf{x}_N \right]
\]
và vector \(\mathbf{1} = [1, 1, \dots, 1]^T\), ta có thể viết lại \(g(\lambda)\) dưới dạng: 
\[
g(\lambda) = -\frac{1}{2}\lambda^T\mathbf{V}^T\mathbf{V}\mathbf{\lambda} + \mathbf{1}^T\lambda. ~~~~~~~~~~~~~~~(8)
\]</p>

<p>(<em>Nếu khó tin, bạn có thể viết ra để quen dần với các biểu thức đại số tuyến tính.</em>)</p>

<p>Đặt \(\mathbf{K} = \mathbf{V}^T\mathbf{V}\), ta có một quan sát quan trọng: \(\mathbf{K}\) là một <a href="/2017/03/12/convexity/#positive-semidefinite">ma trận nửa xác định dương</a>. Thật vậy, với mọi vector \(\lambda\), ta có:
\[
\lambda^T\mathbf{K}\mathbf{\lambda} = \lambda^T\mathbf{V}^T\mathbf{V}\mathbf{\lambda} = ||\mathbf{V}\lambda||_2^2 \geq 0.
\]</p>

<p>(<em>Đây chính là định nghĩa của ma trận nửa xác định dương.</em>)</p>

<p>Vậy \(g(\lambda) = -\frac{1}{2}\lambda^T\mathbf{K}\mathbf{\lambda} + \mathbf{1}^T\lambda\) là một <a href="/2017/03/12/convexity/#concave-function">hàm <em>concave</em></a>.</p>

<p><a name="-bai-toan-doi-ngau-lagrange"></a></p>

<h3 id="34-bài-toán-đối-ngẫu-lagrange">3.4. Bài toán đối ngẫu Lagrange</h3>
<p>Từ đó, kết hợp hàm đối ngẫu Lagrange và các điều kiện ràng buộc của \(\lambda\), ta sẽ thu được <a href="/2017/04/02/duality/#-bai-toan-doi-ngau-lagrange-the-lagrange-dual-problem">bài toán đối ngẫu Lagrange</a>:</p>

<p>\[
 \begin{eqnarray}
     \lambda &amp;=&amp; \arg \max_{\lambda} g(\lambda)   \<br />
     \text{subject to:}~ &amp;&amp; \lambda \succeq 0~~~~~~~~~~ (9)\<br />
     &amp;&amp; \sum_{n=1}^N \lambda_ny_n = 0 
 \end{eqnarray}
 \] 
Ràng buộc thứ hai được lấy từ \((6)\).</p>

<p>Đây là một bài toán lồi vì ta đang đi tìm giá trị lớn nhất của một hàm mục tiêu là <em>concave</em> trên một <a href="/2017/03/12/convexity/#-giao-cua-cac-tap-loi-la-mot-tap-loi"><em>polyhedron</em></a>.</p>

<p>Bài toán này cũng được là một Quadratic Programming và cũng có thể được giải bằng các thư viện như CVXOPT.</p>

<p>Trong bài toán đối ngẫu này, số tham số (parameters) phải tìm là \(N\), là chiều của \(\lambda\), tức số điểm dữ liệu. Trong khi đó, với bài toán gốc \((3)\), số tham số phải tìm là \(d + 1\), là tổng số chiều của \(\mathbf{w}\) và \(b\), tức số chiều của mỗi điểm dữ liệu cộng với 1. Trong rất nhiều trường hợp, số điểm dữ liệu có được trong <em>training set</em> lớn hơn số chiều dữ liệu rất nhiều. Nếu giải trực tiếp bằng các công cụ giải Quadratic Programming, có thể bài toán đối ngẫu còn phức tạp hơn (tốn thời gian hơn) so với bài toàn gốc. Tuy nhiên, điều hấp dẫn của bài toán đối ngẫu này đến từ phần <em>Kernel Support Vector Machine (Kernel SVM)</em>, tức cho các bài toán mà dữ liệu không phải là <em>linearly separable</em> hoặc <em>gần linearly separable</em>. Phần <em>Kernel SVM</em> sẽ được tôi trình bày sau 1 hoặc 2 bài nữa. Ngoài ra, dựa vào tính chất đặc biệt của hệ điều kiện KKT mà SVM có thể được giải bằng nhiều phương pháp hiệu quả hơn.</p>

<p><a name="-dieu-kien-kkt"></a></p>

<h3 id="35-điều-kiện-kkt">3.5. Điều kiện KKT</h3>
<p>Quay trở lại bài toán, vì đây là một bài toán lồi và <em>strong duality</em> thoả mãn, nghiệm của bài toán sẽ thoả mãn hệ <a href="/2017/04/02/duality/#-kkt-optimality-conditions">điều kiện KKT</a> sau đây với biến số là \(\mathbf{w}, b\) và \(\lambda\): 
\[
\begin{eqnarray}
1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) &amp;\leq&amp; 0, ~ \forall n = 1, 2, \dots, N ~~~~(10) \<br />
\lambda_n &amp;\geq&amp; 0, ~\forall n = 1, 2, \dots, N  \<br />
\lambda_n (1 - y_n(\mathbf{w}^T\mathbf{x}_n + b)) &amp;=&amp; 0, ~\forall n = 1, 2, \dots, N ~~~~(11) \<br />
 \mathbf{w} &amp;=&amp; \sum_{n=1}^N \lambda_n y_n \mathbf{x}_n ~~~~~~~~~~~(12)\\ 
 \sum_{n=1}^N \lambda_ny_n &amp;=&amp; 0 ~~~~~~~~~~~~~~~~~~~(13)
\end{eqnarray}
\]</p>

<p>Trong những điều kiện trên, điều kiện \((11)\) là thú vị nhất. Từ đó ta có thể suy ra ngay, với \(n\) bất kỳ, hoặc \(\lambda_n =0\) hoặc \(1 - y_n(\mathbf{w}^T\mathbf{x}_n + b) = 0\). Trường hợp thứ hai chính là:
\[
\mathbf{w}^T\mathbf{x}_n + b = y_n~~~~ (14)
\] 
với chú ý rằng \(y_n^2 = 1, ~\forall n\).</p>

<p>Những điểm thoả mãn \((14)\) chính là những điểm nằm gần mặt phân chia nhất, là những điểm được khoanh tròn trong Hình 4 phía trên. Hai đường thẳng \(\mathbf{w}^T\mathbf{x}_n + b = \pm 1\) <em>tựa</em> lên các điểm thoả mãn \((14)\). Vậy nên những điểm (vectors) thoả mãn \((14)\) còn được gọi là các <em>Support Vectors</em>. Và từ đó, cái tên <em>Support Vector Machine</em> ra đời.</p>

<p>Một quan sát khác, số lượng những điểm thoả mãn \((14)\) thường chiếm số lượng rất nhỏ trong số \(N\) điểm. Chỉ cần dựa trên những <em>support vectors</em> này, chúng ta hoàn toàn có thể xác định được mặt phân cách cần tìm. Nhìn theo một cách khác, hầu hết các \(\lambda_n\) bằng 0. Vậy là mặc dù vector \(\lambda \in \mathbb{R}^N\) có số chiều có thể rất lớn, số lượng các phần tử khác 0 của nó rất ít. Nói cách khác, vector \(\lambda\) là một <em>sparse</em> vector. Support Vector Machine vì vậy còn được xếp vào <em>Sparse Models</em>. Các <em>Sparse Models</em> thường có cách giải hiệu quả (nhanh) hơn các mô hình tương tự với nghiệm là <em>dense</em> (hầu hết khác 0). Đây chính là lý do thứ hai của việc bài toán đối ngẫu SVM được quan tâm nhiều hơn là bài toán gốc.</p>

<p>Tiếp tục phân tích, với những bài toán có số điểm dữ liệu \(N\) nhỏ, ta có thể giải hệ điều kiện KKT phía trên bằng cách xét các trường hợp \(\lambda_n = 0\) hoặc \(\lambda_n \neq 0\). Tổng số trường hợp phải xét là \(2^N\). Với \(N &gt; 50\) (thường là như thế), đây là một con số rất lớn, giải bằng cách này sẽ không khả thi. Tôi sẽ không đi sâu tiếp vào việc giải hệ KKT như thế nào, trong phần tiếp theo chúng ta sẽ giải bài toán tối ưu \((9)\) bằng CVXOPT và bằng thư viện <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<p>Sau khi tìm được \(\lambda\) từ bài toán \((9)\), ta có thể suy ra được \(\mathbf{w}\) dựa vào \((12)\) và \(b\) dựa vào \((11)\) và \((13)\). Rõ ràng ta chỉ cần quan tâm tới \(\lambda_n \neq 0\).</p>

<p>Gọi tập hợp \(\mathcal{S} = \{n: \lambda_n \neq 0\}\) và \(N_{\mathcal{S}}\) là số phần tử của tập \(\mathcal{S}\). Với mỗi \(n \in \mathcal{S}\), ta có:
\[
1 = y_n(\mathbf{w}^T\mathbf{x}_n + b) \Leftrightarrow b + \mathbf{w}^T\mathbf{x}_n = y_n 
\]
Mặc dù từ chỉ một cặp \((\mathbf{x}_n, y_n)\), ta có thể suy ra ngay được \(b\) nếu đã biết \(\mathbf{w}\), một phiên bản khác để tính \(b\) thường được sử dụng và được cho là <em>ổn định hơn trong tính toán</em> (<em>numerically more stable</em>) là:</p>

<p>\[
b = \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}}(y_n - \mathbf{w}^T\mathbf{x}_n) = \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}} \left(y_n - \sum_{m\in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x}_n\right)~~~~~ (15)
\]</p>

<p>tức trung bình cộng của mọi cách tính \(b\).</p>

<p>Trước đó, \(\mathbf{w}\) đã được tính bằng: 
\[
\mathbf{w} = \sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m ~~~~~~ (16)
\]
theo \((12)\).</p>

<p>Quan sát quan trọng: Để xác định một điểm \(\mathbf{x}\) mới thuộc vào class nào, ta cần xác định dấu của biểu thức: 
\[
\mathbf{w}^T\mathbf{x} + b = \sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x} + \frac{1}{N_{\mathcal{S}}} \sum_{n \in \mathcal{S}} \left(y_n - \sum_{m\in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x}_n\right)
\]
Biểu thức này phụ thuộc vào cách tính tích vô hướng giữa các cặp vector \(\mathbf{x}\) và từng \(\mathbf{x}_n \in \mathcal{S}\). Nhận xét quan trọng này sẽ giúp ích cho chúng ta trong bài Kernal SVM.</p>

<p><a name="-lap-trinh-tim-nghiem-cho-svm"></a></p>

<h2 id="4-lập-trình-tìm-nghiệm-cho-svm">4. Lập trình tìm nghiệm cho SVM</h2>
<p>Trong mục này, tôi sẽ trình bày hai cách tính nghiệm cho SVM. Cách thứ nhất dựa theo bài toán \((9)\) và các công thức \((15)\) và \((16)\). Cách thứ hai sử dụng trực tiếp thư viện <code class="language-plaintext highlighter-rouge">sklearn</code>. Cách thứ nhất chỉ là để chứng minh nãy giờ tôi không <em>viết nhảm</em>, bằng cách minh hoạ kết quả tìm được và so sánh với nghiệm tìm được bằng cách thứ hai.</p>

<p><a name="-tim-nghiem-theo-cong-thuc"></a></p>

<h3 id="41-tìm-nghiệm-theo-công-thức">4.1. Tìm nghiệm theo công thức</h3>
<p>Trước tiên chúng ta gọi các <em>modules</em> cần dùng và tạo dữ liệu giả (dữ liệu này chính là dữ liệu tôi dùng trong các hình phía trên nên chúng ta biết chắc rằng hai classes là <em>linearly separable</em>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[.</span><span class="mi">3</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">],</span> <span class="p">[.</span><span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">]]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="c1"># class 1
</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="c1"># class -1 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># all data 
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># labels 
</span></code></pre></div></div>

<p>Tiếp theo, chúng ta giải bài toán \((9)\) bằng CVXOPT:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">cvxopt</span> <span class="kn">import</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">solvers</span>
<span class="c1"># build K
</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">))</span> <span class="c1"># see definition of V, K near eq (8)
</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="c1"># all-one vector 
# build A, b, G, h 
</span><span class="n">G</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">))</span> <span class="c1"># for all lambda_n &gt;= 0
</span><span class="n">h</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># the equality constrain is actually y^T lambda = 0
</span><span class="n">b</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> 
<span class="n">solvers</span><span class="p">.</span><span class="n">options</span><span class="p">[</span><span class="s">'show_progress'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">sol</span> <span class="o">=</span> <span class="n">solvers</span><span class="p">.</span><span class="n">qp</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="s">'x'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'lambda = '</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>Kết quả:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lambda = 
 [[  8.54018321e-01   2.89132533e-10   1.37095535e+00   6.36030818e-10
    4.04317408e-10   8.82390106e-10   6.35001881e-10   5.49567576e-10
    8.33359230e-10   1.20982928e-10   6.86678649e-10   1.25039745e-10
    2.22497367e+00   4.05417905e-09   1.26763684e-10   1.99008949e-10
    2.13742578e-10   1.51537487e-10   3.75329509e-10   3.56161975e-10]]
</code></pre></div></div>

<p>Ta nhận thấy rằng hầu hết các giá trị của <code class="language-plaintext highlighter-rouge">lambda</code> đều rất nhỏ, tới \(10^{-9}\) hoặc \(10^{-10}\). Đây chính là các giá trị bằng 0 nhưng vì sai số tính toán nên nó khác 0 một chút. Chỉ có 3 giá trị khác 0, ta dự đoán là sẽ có 3 điểm là <em>support vectors</em>.</p>

<p>Ta đi tìm <em>support set</em> \(\mathcal{S}\) rồi tìm nghiệm của bài toán:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span> <span class="c1"># just a small number, greater than 1e-9
</span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">l</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">VS</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">XS</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">yS</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">S</span><span class="p">]</span>
<span class="n">lS</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">S</span><span class="p">]</span>
<span class="c1"># calculate w and b
</span><span class="n">w</span> <span class="o">=</span> <span class="n">VS</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lS</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yS</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XS</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b = '</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w =  [[-2.00984381  0.64068336]]
b =  4.66856063387
</code></pre></div></div>

<p>Minh hoạ kết quả:</p>

<hr />

<div class="imgcap">
 <img src="/assets/19_svm/svm4.png" align="center" width="500" />
 <div class="thecap">Hình 5: Minh hoạ nghiệm tìm được bởi SVM.</div>
</div>
<hr />

<p>Đường màu đen đậm ở giữa chính là mặt phân cách tìm được bằng SVM. Từ đây có thể thấy <em>nhiều khả năng là các tính toán của ta là chính xác</em>. Để kiểm tra xem các tính toán phía trên có chính xác không, ta cần tìm nghiệm bằng các công cụ có sẵn, ví dụ như <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<p>Source code cho phần này có thể được tìm thấy <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/19_svm/plt/SVM-example.ipynb">ở đây</a>.</p>

<p><a name="-tim-nghiem-theo-thu-vien"></a></p>

<h3 id="42-tìm-nghiệm-theo-thư-viện">4.2. Tìm nghiệm theo thư viện</h3>
<p>Chúng ta sẽ sử dụng hàm <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code></a> ở đây. Các bài toán thực tế thường sử dụng thư viện <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> được viết trên ngôn ngữ C, có API cho Python và Matlab.</p>

<p>Nếu dùng thư viện thì sẽ như sau:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">,))</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="c1"># each sample is one row
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">)</span> <span class="c1"># just a big number 
</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span> 

<span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">coef_</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">intercept_</span>
<span class="k">print</span><span class="p">(</span><span class="s">'w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b = '</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w =  [[-2.00971102  0.64194082]]
b =  [ 4.66595309]
</code></pre></div></div>

<p>Kết quả này khá giống với kết quả chúng ta tìm được ở phần trên. Có rất nhiều tuỳ chọn cho SVM, các bạn sẽ dần thấy trong các bài sau.</p>

<p><a name="-tom-tat-va-thao-luan"></a></p>

<h2 id="5-tóm-tắt-và-thảo-luận">5. Tóm tắt và thảo luận</h2>

<ul>
  <li>
    <p>Với bài toán binary classification mà 2 classes là <em>linearly separable</em>, có vô số các siêu mặt phẳng giúp phân biệt hai classes, tức mặt phân cách. Với mỗi mặt phân cách, ta có một <em>classifier</em>. Khoảng cách gần nhất từ 1 điểm dữ liệu tới mặt phân cách ấy được gọi là <em>margin</em> của classifier đó.</p>
  </li>
  <li>
    <p>Support Vector Machine là bài toán đi tìm mặt phân cách sao cho <em>margin</em> tìm được là lớn nhất, đồng nghĩa với việc các điểm dữ liệu <em>an toàn nhất</em> so với mặt phân cách.</p>
  </li>
  <li>
    <p>Bài toán tối ưu trong SVM là một bài toán lồi với hàm mục tiêu là <em>stricly convex</em>, nghiệm của bài toán này là duy nhất. Hơn nữa, bài toán tối ưu đó là một Quadratic Programming (QP).</p>
  </li>
  <li>
    <p>Mặc dù có thể trực tiếp giải SVM qua bài toán tối ưu gốc này, thông thường người ta thường giải bài toán đối ngẫu. Bài toán đối ngẫu cũng là một QP nhưng nghiệm là <em>sparse</em> nên có những phương pháp giải hiệu quả hơn.</p>
  </li>
  <li>
    <p>Với các bài toán mà dữ liệu <em>gần linearly separable</em> hoặc <em>nonlinear separable</em>, có những cải tiền khác của SVM để thích nghi với dữ liệu đó. Mời bạn đón đọc bài tiếp theo.</p>
  </li>
  <li>
    <p><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/19_svm/plt/SVM-example.ipynb">Source code</a>.
<a name="-tai-lieu-tham-khao"></a></p>
  </li>
</ul>

<h2 id="6-tài-liệu-tham-khảo">6. Tài liệu tham khảo</h2>

<p>[1] Bishop, Christopher M. “Pattern recognition and Machine Learning.”, Springer  (2006). (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[2] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley &amp; Sons, 2012.</p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code></a></p>

<p>[4] <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">LIBSVM – A Library for Support Vector Machines</a></p>
:ET