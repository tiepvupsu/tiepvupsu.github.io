I"¿Â<p><strong>Trong trang nÃ y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#1-gi%e1%bb%9bi-thi%e1%bb%87u">1. Giá»›i thiá»‡u</a>
    <ul>
      <li><a href="#gradient-descent">Gradient Descent</a></li>
    </ul>
  </li>
  <li><a href="#2-gradient-descent-cho-h%c3%a0m-1-bi%e1%ba%bfn">2. Gradient Descent cho hÃ m 1 biáº¿n</a>
    <ul>
      <li><a href="#v%c3%ad-d%e1%bb%a5-%c4%91%c6%a1n-gi%e1%ba%a3n-v%e1%bb%9bi-python">VÃ­ dá»¥ Ä‘Æ¡n giáº£n vá»›i Python</a>
        <ul>
          <li><a href="#%c4%90i%e1%bb%83m-kh%e1%bb%9fi-t%e1%ba%a1o-kh%c3%a1c-nhau">Äiá»ƒm khá»Ÿi táº¡o khÃ¡c nhau</a></li>
          <li><a href="#learning-rate-kh%c3%a1c-nhau">Learning rate khÃ¡c nhau</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-gradient-descent-cho-h%c3%a0m-nhi%e1%bb%81u-bi%e1%ba%bfn">3. Gradient Descent cho hÃ m nhiá»u biáº¿n</a>
    <ul>
      <li><a href="#quay-l%e1%ba%a1i-v%e1%bb%9bi-b%c3%a0i-to%c3%a1n-linear-regression">Quay láº¡i vá»›i bÃ i toÃ¡n Linear Regression</a></li>
      <li><a href="#sau-%c4%91%c3%a2y-l%c3%a0-v%c3%ad-d%e1%bb%a5-tr%c3%aan-python-v%c3%a0-m%e1%bb%99t-v%c3%a0i-l%c6%b0u-%c3%bd-khi-l%e1%ba%adp-tr%c3%acnh">Sau Ä‘Ã¢y lÃ  vÃ­ dá»¥ trÃªn Python vÃ  má»™t vÃ i lÆ°u Ã½ khi láº­p trÃ¬nh</a>
        <ul>
          <li><a href="#ki%e1%bb%83m-tra-%c4%91%e1%ba%a1o-h%c3%a0m">Kiá»ƒm tra Ä‘áº¡o hÃ m</a>
            <ul>
              <li><a href="#gi%e1%ba%a3i-th%c3%adch-b%e1%ba%b1ng-h%c3%acnh-h%e1%bb%8dc">Giáº£i thÃ­ch báº±ng hÃ¬nh há»c</a></li>
              <li><a href="#gi%e1%ba%a3i-th%c3%adch-b%e1%ba%b1ng-gi%e1%ba%a3i-t%c3%adch">Giáº£i thÃ­ch báº±ng giáº£i tÃ­ch</a></li>
              <li><a href="#v%e1%bb%9bi-h%c3%a0m-nhi%e1%bb%81u-bi%e1%ba%bfn">Vá»›i hÃ m nhiá»u biáº¿n</a></li>
            </ul>
          </li>
          <li><a href="#%c4%90%c6%b0%e1%bb%9dng-%c4%91%e1%bb%93ng-m%e1%bb%a9c-level-sets">ÄÆ°á»ng Ä‘á»“ng má»©c (level sets)</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-m%e1%bb%99t-v%c3%ad-d%e1%bb%a5-kh%c3%a1c">4. Má»™t vÃ­ dá»¥ khÃ¡c</a></li>
  <li><a href="#5-th%e1%ba%a3o-lu%e1%ba%adn">5. Tháº£o luáº­n</a></li>
  <li><a href="#6-t%c3%a0i-li%e1%bb%87u-tham-kh%e1%ba%a3o">6. TÃ i liá»‡u tham kháº£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-giá»›i-thiá»‡u">1. Giá»›i thiá»‡u</h2>

<p>CÃ¡c báº¡n háº³n tháº¥y hÃ¬nh váº½ dÆ°á»›i Ä‘Ã¢y quen thuá»™c:</p>

<div class="imgcap">
 <img src="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/gradient_descent.png?raw=true" align="center" width="600" />
</div>

<p>Äiá»ƒm mÃ u xanh lá»¥c lÃ  Ä‘iá»ƒm local minimum (cá»±c tiá»ƒu), vÃ  cÅ©ng lÃ  Ä‘iá»ƒm lÃ m cho hÃ m
sá»‘ Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t. Tá»« Ä‘Ã¢y trá»Ÿ Ä‘i, tÃ´i sáº½ dÃ¹ng <em>local minimum</em> Ä‘á»ƒ thay cho
<em>Ä‘iá»ƒm cá»±c tiá»ƒu</em>, <em>global minimum</em> Ä‘á»ƒ thay cho <em>Ä‘iá»ƒm mÃ  táº¡i Ä‘Ã³ hÃ m sá»‘ Ä‘áº¡t giÃ¡ trá»‹
nhá» nháº¥t</em>. Global minimum lÃ  má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a local minimum.</p>

<p>Giáº£ sá»­ chÃºng ta Ä‘ang quan tÃ¢m Ä‘áº¿n má»™t hÃ m sá»‘ má»™t biáº¿n cÃ³ Ä‘áº¡o hÃ m má»i nÆ¡i. Xin
cho tÃ´i Ä‘Æ°á»£c nháº¯c láº¡i vÃ i Ä‘iá»u Ä‘Ã£ quÃ¡ quen thuá»™c:</p>

<ol>
  <li>
    <p>Äiá»ƒm local minimum \(x^*\) cá»§a hÃ m sá»‘ lÃ  Ä‘iá»ƒm cÃ³ Ä‘áº¡o hÃ m \(fâ€™(x^*)\)
báº±ng 0. HÆ¡n tháº¿ ná»¯a, trong lÃ¢n cáº­n cá»§a nÃ³, Ä‘áº¡o hÃ m cá»§a cÃ¡c Ä‘iá»ƒm phÃ­a bÃªn trÃ¡i
\(x^*\) lÃ  khÃ´ng dÆ°Æ¡ng, Ä‘áº¡o hÃ m cá»§a cÃ¡c Ä‘iá»ƒm phÃ­a bÃªn pháº£i \(x^*\) lÃ 
khÃ´ng Ã¢m.</p>
  </li>
  <li>
    <p>ÄÆ°á»ng tiáº¿p tuyáº¿n vá»›i Ä‘á»“ thá»‹ hÃ m sá»‘ Ä‘Ã³ táº¡i 1 Ä‘iá»ƒm báº¥t ká»³ cÃ³ há»‡ sá»‘ gÃ³c chÃ­nh
báº±ng Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ táº¡i Ä‘iá»ƒm Ä‘Ã³.</p>
  </li>
</ol>

<p>Trong hÃ¬nh phÃ­a trÃªn, cÃ¡c Ä‘iá»ƒm bÃªn trÃ¡i cá»§a Ä‘iá»ƒm local minimum mÃ u xanh lá»¥c cÃ³
Ä‘áº¡o hÃ m Ã¢m, cÃ¡c Ä‘iá»ƒm bÃªn pháº£i cÃ³ Ä‘áº¡o hÃ m dÆ°Æ¡ng. VÃ  Ä‘á»‘i vá»›i hÃ m sá»‘ nÃ y, cÃ ng xa
vá» phÃ­a trÃ¡i cá»§a Ä‘iá»ƒm local minimum thÃ¬ Ä‘áº¡o hÃ m cÃ ng Ã¢m, cÃ ng xa vá» phÃ­a pháº£i
thÃ¬ Ä‘áº¡o hÃ m cÃ ng dÆ°Æ¡ng.</p>

<p><a name="gradient-descent"></a></p>

<h3 id="gradient-descent">Gradient Descent</h3>
<p>Trong Machine Learning nÃ³i riÃªng vÃ  ToÃ¡n Tá»‘i Æ¯u nÃ³i chung, chÃºng ta thÆ°á»ng xuyÃªn
pháº£i tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t (hoáº·c Ä‘Ã´i khi lÃ  lá»›n nháº¥t) cá»§a má»™t hÃ m sá»‘ nÃ o Ä‘Ã³. VÃ­
dá»¥ nhÆ° cÃ¡c hÃ m máº¥t mÃ¡t trong hai bÃ i <a href="/2016/12/28/linearregression/">Linear Regression</a> 
vÃ  <a href="/2017/01/01/kmeans/">K-means Clustering</a>. NhÃ¬n chung, viá»‡c tÃ¬m global
minimum cá»§a cÃ¡c hÃ m máº¥t mÃ¡t trong Machine Learning lÃ  ráº¥t phá»©c táº¡p, tháº­m chÃ­ lÃ 
báº¥t kháº£ thi. Thay vÃ o Ä‘Ã³, ngÆ°á»i ta thÆ°á»ng cá»‘ gáº¯ng tÃ¬m cÃ¡c Ä‘iá»ƒm local minimum, vÃ 
á»Ÿ má»™t má»©c Ä‘á»™ nÃ o Ä‘Ã³, coi Ä‘Ã³ lÃ  nghiá»‡m cáº§n tÃ¬m cá»§a bÃ i toÃ¡n.</p>

<p>CÃ¡c Ä‘iá»ƒm local minimum lÃ  nghiá»‡m cá»§a phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m báº±ng 0. Náº¿u báº±ng má»™t
cÃ¡ch nÃ o Ä‘Ã³ cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c toÃ n bá»™ (há»¯u háº¡n) cÃ¡c Ä‘iá»ƒm cá»±c tiá»ƒu, ta chá»‰ cáº§n thay
tá»«ng Ä‘iá»ƒm local minimum Ä‘Ã³ vÃ o hÃ m sá»‘ rá»“i tÃ¬m Ä‘iá»ƒm lÃ m cho hÃ m cÃ³ giÃ¡ trá»‹ nhá»
nháº¥t (<em>Ä‘oáº¡n nÃ y nghe ráº¥t quen thuá»™c, Ä‘Ãºng khÃ´ng?</em>). Tuy nhiÃªn, trong háº§u háº¿t cÃ¡c
trÆ°á»ng há»£p, viá»‡c giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m báº±ng 0 lÃ  báº¥t kháº£ thi. NguyÃªn nhÃ¢n cÃ³
thá»ƒ Ä‘áº¿n tá»« sá»± phá»©c táº¡p cá»§a dáº¡ng cá»§a Ä‘áº¡o hÃ m, tá»« viá»‡c cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ sá»‘
chiá»u lá»›n, hoáº·c tá»« viá»‡c cÃ³ quÃ¡ nhiá»u Ä‘iá»ƒm dá»¯ liá»‡u.</p>

<p>HÆ°á»›ng tiáº¿p cáº­n phá»• biáº¿n nháº¥t lÃ  xuáº¥t phÃ¡t tá»« má»™t Ä‘iá»ƒm mÃ  chÃºng ta coi lÃ  <em>gáº§n</em>
vá»›i nghiá»‡m cá»§a bÃ i toÃ¡n, sau Ä‘Ã³ dÃ¹ng má»™t phÃ©p toÃ¡n láº·p Ä‘á»ƒ <em>tiáº¿n dáº§n</em> Ä‘áº¿n Ä‘iá»ƒm
cáº§n tÃ¬m, tá»©c Ä‘áº¿n khi Ä‘áº¡o hÃ m gáº§n vá»›i 0. Gradient Descent (viáº¿t gá»n lÃ  GD) vÃ  cÃ¡c
biáº¿n thá»ƒ cá»§a nÃ³ lÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c dÃ¹ng nhiá»u nháº¥t.</p>

<p><a name="large-scale"></a></p>

<p>VÃ¬ kiáº¿n thá»©c vá» GD khÃ¡ rá»™ng nÃªn tÃ´i xin phÃ©p Ä‘Æ°á»£c chia thÃ nh hai pháº§n. Pháº§n 1
nÃ y giá»›i thiá»‡u Ã½ tÆ°á»Ÿng phÃ­a sau thuáº­t toÃ¡n GD vÃ  má»™t vÃ i vÃ­ dá»¥ Ä‘Æ¡n giáº£n giÃºp cÃ¡c
báº¡n lÃ m quen vá»›i thuáº­t toÃ¡n nÃ y vÃ  vÃ i khÃ¡i niá»‡m má»›i. Pháº§n 2 sáº½ nÃ³i vá» cÃ¡c
phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n GD vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a GD trong cÃ¡c bÃ i toÃ¡n mÃ  sá»‘ chiá»u vÃ 
sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u lá»›n. Nhá»¯ng bÃ i toÃ¡n nhÆ° váº­y Ä‘Æ°á»£c gá»i lÃ  <em>large-scale</em>.</p>

<p><a name="-gradient-descent-cho-ham--bien"></a></p>

<h2 id="2-gradient-descent-cho-hÃ m-1-biáº¿n">2. Gradient Descent cho hÃ m 1 biáº¿n</h2>

<p>Quay trá»Ÿ láº¡i hÃ¬nh váº½ ban Ä‘áº§u vÃ  má»™t vÃ i quan sÃ¡t tÃ´i Ä‘Ã£ nÃªu. Giáº£ sá»­
\(x_{t}\) lÃ  Ä‘iá»ƒm ta tÃ¬m Ä‘Æ°á»£c sau vÃ²ng láº·p thá»© \(t\). Ta cáº§n tÃ¬m má»™t thuáº­t
toÃ¡n Ä‘á»ƒ Ä‘Æ°a \(x_{t}\) vá» cÃ ng gáº§n \(x^*\) cÃ ng tá»‘t.</p>

<p>Trong hÃ¬nh Ä‘áº§u tiÃªn, chÃºng ta láº¡i cÃ³ thÃªm hai quan sÃ¡t ná»¯a:</p>

<ol>
  <li>
    <p>Náº¿u Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ táº¡i \(x_{t}\): \(fâ€™(x_{t}) &gt; 0\) thÃ¬
\(x_{t}\) náº±m vá» bÃªn pháº£i so vá»›i \(x^*\) (vÃ  ngÆ°á»£c láº¡i). Äá»ƒ Ä‘iá»ƒm tiáº¿p
theo \(x_{t+1}\) gáº§n vá»›i \(x^*\) hÆ¡n, chÃºng ta cáº§n di chuyá»ƒn
\(x_{t}\) vá» phÃ­a bÃªn trÃ¡i, tá»©c vá» phÃ­a <em>Ã¢m</em>. NÃ³i cÃ¡c khÃ¡c, <strong>chÃºng ta cáº§n
di chuyá»ƒn ngÆ°á»£c dáº¥u vá»›i Ä‘áº¡o hÃ m</strong>:
\[
x_{t+1} = x_{t} + \Delta
\]
Trong Ä‘Ã³ \(\Delta\) lÃ  má»™t Ä‘áº¡i lÆ°á»£ng ngÆ°á»£c dáº¥u vá»›i Ä‘áº¡o hÃ m \(fâ€™(x_{t})\).</p>
  </li>
  <li>
    <p>\(x_{t}\) cÃ ng xa \(x^*\) vá» phÃ­a bÃªn pháº£i thÃ¬ \(fâ€™(x_{t})\) cÃ ng lá»›n
hÆ¡n 0 (vÃ  ngÆ°á»£c láº¡i). Váº­y, lÆ°á»£ng di chuyá»ƒn \(\Delta\), má»™t cÃ¡ch trá»±c quan
nháº¥t, lÃ  tá»‰ lá»‡ thuáº­n vá»›i \(-fâ€™(x_{t})\).</p>
  </li>
</ol>

<p>Hai nháº­n xÃ©t phÃ­a trÃªn cho chÃºng ta má»™t cÃ¡ch cáº­p nháº­t Ä‘Æ¡n giáº£n lÃ :
\[
x_{t+1} = x_{t} - \eta fâ€™(x_{t})
\]</p>

<p>Trong Ä‘Ã³ \(\eta\) (Ä‘á»c lÃ  <em>eta</em>) lÃ  má»™t sá»‘ dÆ°Æ¡ng Ä‘Æ°á»£c gá»i lÃ  <em>learning rate</em>
(tá»‘c Ä‘á»™ há»c). Dáº¥u trá»« thá»ƒ hiá»‡n viá»‡c chÃºng ta pháº£i <em>Ä‘i ngÆ°á»£c</em> vá»›i Ä‘áº¡o hÃ m (ÄÃ¢y
cÅ©ng chÃ­nh lÃ  lÃ½ do phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c gá»i lÃ  Gradient Descent - <em>descent</em>
nghÄ©a lÃ  <em>Ä‘i ngÆ°á»£c</em>). CÃ¡c quan sÃ¡t Ä‘Æ¡n giáº£n phÃ­a trÃªn, máº·c dÃ¹ khÃ´ng pháº£i Ä‘Ãºng
cho táº¥t cáº£ cÃ¡c bÃ i toÃ¡n, lÃ  ná»n táº£ng cho ráº¥t nhiá»u phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u nÃ³i chung
vÃ  thuáº­t toÃ¡n Machine Learning nÃ³i riÃªng.</p>

<p><a name="vi-du-don-gian-voi-python"></a></p>

<h3 id="vÃ­-dá»¥-Ä‘Æ¡n-giáº£n-vá»›i-python">VÃ­ dá»¥ Ä‘Æ¡n giáº£n vá»›i Python</h3>

<p>XÃ©t hÃ m sá»‘ \(f(x) = x^2 + 5\sin(x)\) vá»›i Ä‘áº¡o hÃ m \(fâ€™(x) = 2x + 5\cos(x)\)
(má»™t lÃ½ do tÃ´i chá»n hÃ m nÃ y vÃ¬ nÃ³ khÃ´ng dá»… tÃ¬m nghiá»‡m cá»§a Ä‘áº¡o hÃ m báº±ng 0 nhÆ° hÃ m
phÃ­a trÃªn). Giáº£ sá»­ báº¯t Ä‘áº§u tá»« má»™t Ä‘iá»ƒm \(x_{0}\) nÃ o Ä‘Ã³, táº¡i vÃ²ng láº·p thá»©
\(t\), chÃºng ta sáº½ cáº­p nháº­t nhÆ° sau:
\[
x_{t+1} = x_{t} - \eta(2x_{t} + 5\cos(x_{t}))
\]</p>

<p>NhÆ° thÆ°á»ng lá»‡, tÃ´i khai bÃ¡o vÃ i thÆ° viá»‡n quen thuá»™c</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To support both python 2 and python 3
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>Tiáº¿p theo, tÃ´i viáº¿t cÃ¡c hÃ m sá»‘ :</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">grad</code> Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m</li>
  <li><code class="language-plaintext highlighter-rouge">cost</code> Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ cá»§a hÃ m sá»‘. HÃ m nÃ y khÃ´ng sá»­ dá»¥ng trong thuáº­t toÃ¡n
nhÆ°ng thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ kiá»ƒm tra viá»‡c tÃ­nh Ä‘áº¡o hÃ m cá»§a Ä‘Ãºng khÃ´ng hoáº·c Ä‘á»ƒ
xem giÃ¡ trá»‹ cá»§a hÃ m sá»‘ cÃ³ giáº£m theo má»—i vÃ²ng láº·p hay khÃ´ng.</li>
  <li><code class="language-plaintext highlighter-rouge">myGD1</code> lÃ  pháº§n chÃ­nh thá»±c hiá»‡n thuáº­t toÃ¡n Gradient Desent nÃªu phÃ­a trÃªn. Äáº§u
vÃ o cá»§a hÃ m sá»‘ nÃ y lÃ  learning rate vÃ  Ä‘iá»ƒm báº¯t Ä‘áº§u. Thuáº­t toÃ¡n dá»«ng láº¡i khi
Ä‘áº¡o hÃ m cÃ³ Ä‘á»™ lá»›n Ä‘á»§ nhá».</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">myGD1</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">x_new</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">x_new</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">x</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="diem-khoi-tao-khac-nhau"></a></p>

<h4 id="Ä‘iá»ƒm-khá»Ÿi-táº¡o-khÃ¡c-nhau">Äiá»ƒm khá»Ÿi táº¡o khÃ¡c nhau</h4>

<p>Sau khi cÃ³ cÃ¡c hÃ m cáº§n thiáº¿t, tÃ´i thá»­ tÃ¬m nghiá»‡m vá»›i cÃ¡c Ä‘iá»ƒm khá»Ÿi táº¡o khÃ¡c nhau
lÃ  \(x_{0} = -5\) vÃ  \(x_{0} = 5\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">it1</span><span class="p">)</span> <span class="o">=</span> <span class="n">myGD1</span><span class="p">(.</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">)</span>
<span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">it2</span><span class="p">)</span> <span class="o">=</span> <span class="n">myGD1</span><span class="p">(.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Solution x1 = %f, cost = %f, obtained after %d iterations'</span><span class="o">%</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cost</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">it1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Solution x2 = %f, cost = %f, obtained after %d iterations'</span><span class="o">%</span><span class="p">(</span><span class="n">x2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cost</span><span class="p">(</span><span class="n">x2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">it2</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Solution x1 = -1.110667, cost = -3.246394, obtained after 11 iterations
Solution x2 = -1.110341, cost = -3.246394, obtained after 29 iterations
</code></pre></div></div>

<p>Váº­y lÃ  vá»›i cÃ¡c Ä‘iá»ƒm ban Ä‘áº§u khÃ¡c nhau, thuáº­t toÃ¡n cá»§a chÃºng ta tÃ¬m Ä‘Æ°á»£c nghiá»‡m
gáº§n giá»‘ng nhau, máº·c dÃ¹ vá»›i tá»‘c Ä‘á»™ há»™i tá»¥ khÃ¡c nhau. DÆ°á»›i Ä‘Ã¢y lÃ  hÃ¬nh áº£nh minh
há»a thuáº­t toÃ¡n GD cho bÃ i toÃ¡n nÃ y (<em>xem tá»‘t trÃªn Desktop á»Ÿ cháº¿ Ä‘á»™ full mÃ n
hÃ¬nh</em>).</p>

<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="50%" style="border: 0px solid white"> 
            <img style="display:block;" width="100%" src="/assets/GD/1dimg_5_0.1_-5.gif" />
        </td>
        <td width="50%" style="border: 0px solid white">
            <img style="display:block;" width="100%" src="/assets/GD/1dimg_5_0.1_5.gif" />
        </td>
    </tr>
</table>

<p>Tá»« hÃ¬nh minh há»a trÃªn ta tháº¥y ráº±ng á»Ÿ hÃ¬nh bÃªn trÃ¡i, tÆ°Æ¡ng á»©ng vá»›i \(x_{0} = -5\), nghiá»‡m há»™i tá»¥ nhanh hÆ¡n, vÃ¬ Ä‘iá»ƒm ban Ä‘áº§u \(x_0\) gáº§n vá»›i nghiá»‡m \( x^* \approx -1\)  hÆ¡n. HÆ¡n ná»¯a, vá»›i \(x_{0} = 5 \) á»Ÿ hÃ¬nh bÃªn pháº£i, <em>Ä‘Æ°á»ng Ä‘i</em> cá»§a nghiá»‡m cÃ³ chá»©a má»™t khu vá»±c cÃ³ Ä‘áº¡o hÃ m khÃ¡ nhá» gáº§n Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ báº±ng 2. Äiá»u nÃ y khiáº¿n cho thuáº­t toÃ¡n <em>la cÃ </em> á»Ÿ Ä‘Ã¢y khÃ¡ lÃ¢u. Khi vÆ°á»£t qua Ä‘Æ°á»£c Ä‘iá»ƒm nÃ y thÃ¬ má»i viá»‡c diá»…n ra ráº¥t tá»‘t Ä‘áº¹p.</p>

<p><a name="learning-rate-khac-nhau"></a></p>

<h4 id="learning-rate-khÃ¡c-nhau">Learning rate khÃ¡c nhau</h4>

<p>Tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a GD khÃ´ng nhá»¯ng phá»¥ thuá»™c vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o ban Ä‘áº§u mÃ  cÃ²n phá»¥ thuá»™c vÃ o <em>learning rate</em>. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ vá»›i cÃ¹ng Ä‘iá»ƒm khá»Ÿi táº¡o \(x_{0} = -5\) nhÆ°ng learning rate khÃ¡c nhau:</p>

<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="50%" style="border: 0px solid white"> 
            <img style="display:block;" width="100%" src="/assets/GD/1dimg_5_0.01_-5.gif" />
        </td>
        <td width="50%" style="border: 0px solid white">
            <img style="display:block;" width="100%" src="/assets/GD/1dimg_5_0.5_-5.gif" />
        </td>
        <!-- <td width="30%" style = "border: 0px solid white">
            <img src = "/assets/GD/1dimg_5_0.5_5.gif">
        </td> -->
    </tr>
</table>
<p>Ta quan sÃ¡t tháº¥y hai Ä‘iá»u:</p>

<ol>
  <li>Vá»›i <em>learning rate</em> nhá» \(\eta = 0.01\), tá»‘c Ä‘á»™ há»™i tá»¥ ráº¥t cháº­m. Trong vÃ­
dá»¥ nÃ y tÃ´i chá»n tá»‘i Ä‘a 100 vÃ²ng láº·p nÃªn thuáº­t toÃ¡n dá»«ng láº¡i trÆ°á»›c khi tá»›i
<em>Ä‘Ã­ch</em>, máº·c dÃ¹ Ä‘Ã£ ráº¥t gáº§n. Trong thá»±c táº¿, khi viá»‡c tÃ­nh toÃ¡n trá»Ÿ nÃªn phá»©c
táº¡p, <em>learning rate</em> quÃ¡ tháº¥p sáº½ áº£nh hÆ°á»Ÿng tá»›i tá»‘c Ä‘á»™ cá»§a thuáº­t toÃ¡n ráº¥t
nhiá»u, tháº­m chÃ­ khÃ´ng bao giá» tá»›i Ä‘Æ°á»£c Ä‘Ã­ch.</li>
  <li>Vá»›i <em>learning rate</em> lá»›n \(\eta = 0.5\), thuáº­t toÃ¡n tiáº¿n ráº¥t nhanh tá»›i <em>gáº§n
Ä‘Ã­ch</em> sau vÃ i vÃ²ng láº·p. Tuy nhiÃªn, thuáº­t toÃ¡n khÃ´ng há»™i tá»¥ Ä‘Æ°á»£c vÃ¬ <em>bÆ°á»›c
nháº£y</em> quÃ¡ lá»›n, khiáº¿n nÃ³ cá»© <em>quáº©n quanh</em> á»Ÿ Ä‘Ã­ch.</li>
</ol>

<p>Viá»‡c lá»±a chá»n <em>learning rate</em> ráº¥t quan trá»ng trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿. Viá»‡c
lá»±a chá»n giÃ¡ trá»‹ nÃ y phá»¥ thuá»™c nhiá»u vÃ o tá»«ng bÃ i toÃ¡n vÃ  pháº£i lÃ m má»™t vÃ i thÃ­
nghiá»‡m Ä‘á»ƒ chá»n ra giÃ¡ trá»‹ tá»‘t nháº¥t. NgoÃ i ra, tÃ¹y vÃ o má»™t sá»‘ bÃ i toÃ¡n, GD cÃ³ thá»ƒ
lÃ m viá»‡c hiá»‡u quáº£ hÆ¡n báº±ng cÃ¡ch chá»n ra <em>learning rate</em> phÃ¹ há»£p hoáº·c chá»n
<em>learning rate</em> khÃ¡c nhau á»Ÿ má»—i vÃ²ng láº·p. TÃ´i sáº½ quay láº¡i váº¥n Ä‘á» nÃ y á»Ÿ pháº§n 2.</p>

<p><a name="-gradient-descent-cho-ham-nhieu-bien"></a></p>

<h2 id="3-gradient-descent-cho-hÃ m-nhiá»u-biáº¿n">3. Gradient Descent cho hÃ m nhiá»u biáº¿n</h2>

<p>Giáº£ sá»­ ta cáº§n tÃ¬m global minimum cho hÃ m \(f(\mathbf{\theta})\) trong Ä‘Ã³
\(\mathbf{\theta}\) (<em>theta</em>) lÃ  má»™t vector, thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ kÃ½ hiá»‡u táº­p
há»£p cÃ¡c tham sá»‘ cá»§a má»™t mÃ´ hÃ¬nh cáº§n tá»‘i Æ°u (trong Linear Regression thÃ¬ cÃ¡c tham
sá»‘ chÃ­nh lÃ  há»‡ sá»‘ \(\mathbf{w}\)). Äáº¡o hÃ m cá»§a hÃ m sá»‘ Ä‘Ã³ táº¡i má»™t Ä‘iá»ƒm
\(\theta\) báº¥t ká»³ Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  \(\nabla_{\theta}f(\theta)\) (hÃ¬nh tam
giÃ¡c ngÆ°á»£c Ä‘á»c lÃ  <em>nabla</em>). TÆ°Æ¡ng tá»± nhÆ° hÃ m 1 biáº¿n, thuáº­t toÃ¡n GD cho hÃ m nhiá»u
biáº¿n cÅ©ng báº¯t Ä‘áº§u báº±ng má»™t Ä‘iá»ƒm dá»± Ä‘oÃ¡n \(\theta_{0}\), sau Ä‘Ã³, á»Ÿ vÃ²ng láº·p
thá»© \(t\), quy táº¯c cáº­p nháº­t lÃ :</p>

<p>\[
\theta_{t+1} = \theta_{t} - \eta \nabla_{\theta} f(\theta_{t})
\]</p>

<p>Hoáº·c viáº¿t dÆ°á»›i dáº¡ng Ä‘Æ¡n giáº£n hÆ¡n: \(\theta = \theta - \eta \nabla_{\theta} f(\theta)\).</p>

<p>Quy táº¯c cáº§n nhá»›: <strong>luÃ´n luÃ´n Ä‘i ngÆ°á»£c hÆ°á»›ng vá»›i Ä‘áº¡o hÃ m</strong>.</p>

<p>Viá»‡c tÃ­nh toÃ¡n Ä‘áº¡o hÃ m cá»§a cÃ¡c hÃ m nhiá»u biáº¿n lÃ  má»™t ká»¹ nÄƒng cáº§n thiáº¿t. Má»™t vÃ i Ä‘áº¡o hÃ m Ä‘Æ¡n giáº£n cÃ³ thá»ƒ Ä‘Æ°á»£c <a href="/math/#bang-cac-dao-ham-co-ban">tÃ¬m tháº¥y á»Ÿ Ä‘Ã¢y</a>.
<a name="quay-lai-voi-bai-toan-linear-regression"></a></p>

<h3 id="quay-láº¡i-vá»›i-bÃ i-toÃ¡n-linear-regression">Quay láº¡i vá»›i bÃ i toÃ¡n Linear Regression</h3>
<p>Trong má»¥c nÃ y, chÃºng ta quay láº¡i vá»›i bÃ i toÃ¡n <a href="/2016/12/28/linearregression/">Linear Regression</a> vÃ  thá»­ tá»‘i Æ°u hÃ m máº¥t mÃ¡t cá»§a nÃ³ báº±ng thuáº­t toÃ¡n GD.</p>

<p>HÃ m máº¥t mÃ¡t cá»§a Linear Regression lÃ : 
\[
\mathcal{L}(\mathbf{w}) = \frac{1}{2N}||\mathbf{y - \bar{X}w}||_2^2
\]</p>

<p><strong>ChÃº Ã½</strong>: hÃ m nÃ y cÃ³ khÃ¡c má»™t chÃºt so vá»›i hÃ m tÃ´i nÃªu trong bÃ i <a href="/2016/12/28/linearregression/">Linear Regression</a>. Máº«u sá»‘ cÃ³ thÃªm \(N\) lÃ  sá»‘ lÆ°á»£ng dá»¯ liá»‡u trong training set. Viá»‡c láº¥y trung bÃ¬nh cá»™ng cá»§a lá»—i nÃ y nháº±m giÃºp trÃ¡nh trÆ°á»ng há»£p hÃ m máº¥t mÃ¡t vÃ  Ä‘áº¡o hÃ m cÃ³ giÃ¡ trá»‹ lÃ  má»™t sá»‘ ráº¥t lá»›n, áº£nh hÆ°á»Ÿng tá»›i Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c phÃ©p toÃ¡n khi thá»±c hiá»‡n trÃªn mÃ¡y tÃ­nh. Vá» máº·t toÃ¡n há»c, nghiá»‡m cá»§a hai bÃ i toÃ¡n lÃ  nhÆ° nhau.</p>

<p>Äáº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t lÃ :
\[
\nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w}) = 
\frac{1}{N}\mathbf{\bar{X}}^T \mathbf{(\bar{X}w - y)} ~~~~~(1)
\]</p>

<p><a name="sau-day-la-vi-du-tren-python-va-mot-vai-luu-y-khi-lap-trinh"></a></p>

<h3 id="sau-Ä‘Ã¢y-lÃ -vÃ­-dá»¥-trÃªn-python-vÃ -má»™t-vÃ i-lÆ°u-Ã½-khi-láº­p-trÃ¬nh">Sau Ä‘Ã¢y lÃ  vÃ­ dá»¥ trÃªn Python vÃ  má»™t vÃ i lÆ°u Ã½ khi láº­p trÃ¬nh</h3>

<p>Load thÆ° viá»‡n</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To support both python 2 and python 3
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Tiáº¿p theo, chÃºng ta táº¡o 1000 Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c chá»n <em>gáº§n</em> vá»›i Ä‘Æ°á»ng tháº³ng \(y = 4 + 3x\), hiá»ƒn thá»‹ chÃºng vÃ  tÃ¬m nghiá»‡m theo cÃ´ng thá»©c:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="p">.</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># noise added
</span>
<span class="c1"># Building Xbar 
</span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Xbar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">one</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xbar</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Xbar</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xbar</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">w_lr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Solution found by formula: w = '</span><span class="p">,</span><span class="n">w_lr</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># Display result
</span><span class="n">w</span> <span class="o">=</span> <span class="n">w_lr</span>
<span class="n">w_0</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w_1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">w_0</span> <span class="o">+</span> <span class="n">w_1</span><span class="o">*</span><span class="n">x0</span>

<span class="c1"># Draw the fitting line 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="s">'b.'</span><span class="p">)</span>     <span class="c1"># data 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="s">'y'</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># the fitting line
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Solution found by formula: w =  [[ 4.00305242  2.99862665]]
</code></pre></div></div>

<div class="imgcap">
 <img src="/assets/GD/output_11_1.png" align="center" width="400" />
</div>

<p>ÄÆ°á»ng tháº³ng tÃ¬m Ä‘Æ°á»£c lÃ  Ä‘Æ°á»ng cÃ³ mÃ u vÃ ng cÃ³ phÆ°Æ¡ng trÃ¬nh \(y \approx 4 + 2.998x\).</p>

<p>Tiáº¿p theo ta viáº¿t Ä‘áº¡o hÃ m vÃ  hÃ m máº¥t mÃ¡t:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">Xbar</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">Xbar</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xbar</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">Xbar</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">.</span><span class="mi">5</span><span class="o">/</span><span class="n">N</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xbar</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">;</span>
</code></pre></div></div>

<p><a name="kiem-tra-dao-ham"></a></p>

<h4 id="kiá»ƒm-tra-Ä‘áº¡o-hÃ m">Kiá»ƒm tra Ä‘áº¡o hÃ m</h4>
<p>Viá»‡c tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m nhiá»u biáº¿n thÃ´ng thÆ°á»ng khÃ¡ phá»©c táº¡p vÃ  ráº¥t dá»… máº¯c lá»—i, náº¿u chÃºng ta tÃ­nh sai Ä‘áº¡o hÃ m thÃ¬ thuáº­t toÃ¡n GD khÃ´ng thá»ƒ cháº¡y Ä‘Ãºng Ä‘Æ°á»£c. Trong thá»±c nghiá»‡m, cÃ³ má»™t cÃ¡ch Ä‘á»ƒ kiá»ƒm tra liá»‡u Ä‘áº¡o hÃ m tÃ­nh Ä‘Æ°á»£c cÃ³ chÃ­nh xÃ¡c khÃ´ng. CÃ¡ch nÃ y dá»±a trÃªn Ä‘á»‹nh nghÄ©a cá»§a Ä‘áº¡o hÃ m (cho hÃ m 1 biáº¿n):
\[
fâ€™(x) = \lim_{\varepsilon \rightarrow 0}\frac{f(x + \varepsilon) - f(x)}{\varepsilon}
\]</p>

<p>Má»™t cÃ¡ch thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  láº¥y má»™t giÃ¡ trá»‹ \(\varepsilon \) ráº¥t nhá», vÃ­ dá»¥ \(10^{-6}\), vÃ  sá»­ dá»¥ng cÃ´ng thá»©c:
\[
fâ€™(x) \approx \frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon} ~~~~ (2)
\]</p>

<p>CÃ¡ch tÃ­nh nÃ y Ä‘Æ°á»£c gá»i lÃ  <em>numerical gradient</em>.</p>

<p><strong>CÃ¢u há»i: Táº¡i sao cÃ´ng thá»©c xáº¥p xá»‰ hai phÃ­a trÃªn Ä‘Ã¢y láº¡i Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, sao khÃ´ng sá»­ dá»¥ng cÃ´ng thá»©c xáº¥p xá»‰ Ä‘áº¡o hÃ m bÃªn pháº£i hoáº·c bÃªn trÃ¡i?</strong></p>

<p>CÃ³ hai cÃ¡c giáº£i thÃ­ch cho váº¥n Ä‘á» nÃ y, má»™t báº±ng hÃ¬nh há»c, má»™t báº±ng giáº£i tÃ­ch.</p>

<p><a name="giai-thich-bang-hinh-hoc"></a></p>

<h5 id="giáº£i-thÃ­ch-báº±ng-hÃ¬nh-há»c">Giáº£i thÃ­ch báº±ng hÃ¬nh há»c</h5>
<p>Quan sÃ¡t hÃ¬nh dÆ°á»›i Ä‘Ã¢y:</p>

<div class="imgcap">
 <img src="/assets/GD/check_grad.png" align="center" width="600" />
</div>
<p><br />
Trong hÃ¬nh, vector mÃ u Ä‘á» lÃ  Ä‘áº¡o hÃ m <em>chÃ­nh xÃ¡c</em> cá»§a hÃ m sá»‘ táº¡i Ä‘iá»ƒm cÃ³ hoÃ nh Ä‘á»™ báº±ng \(x_0\). Vector mÃ u xanh lam (cÃ³ váº» lÃ  hÆ¡i tÃ­m sau khi convert tá»« .pdf sang .png) thá»ƒ hiá»‡n cÃ¡ch xáº¥p xá»‰ Ä‘áº¡o hÃ m phÃ­a pháº£i. Vector mÃ u xanh lá»¥c thá»ƒ hiá»‡n cÃ¡ch xáº¥p xá»‰ Ä‘áº¡o hÃ m phÃ­a trÃ¡i. Vector mÃ u nÃ¢u thá»ƒ hiá»‡n cÃ¡ch xáº¥p xá»‰ Ä‘áº¡o hÃ m hai phÃ­a. Trong ba vector xáº¥p xá»‰ Ä‘Ã³, vector xáº¥p xá»‰ hai phÃ­a mÃ u nÃ¢u lÃ  gáº§n vá»›i vector Ä‘á» nháº¥t náº¿u xÃ©t theo hÆ°á»›ng.</p>

<p>Sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c cÃ¡ch xáº¥p xá»‰ cÃ²n lá»›n hÆ¡n ná»¯a náº¿u táº¡i Ä‘iá»ƒm x, hÃ m sá»‘ bá»‹ <em>báº» cong</em> máº¡nh hÆ¡n. Khi Ä‘Ã³, xáº¥p xá»‰ trÃ¡i vÃ  pháº£i sáº½ khÃ¡c nhau ráº¥t nhiá»u. Xáº¥p xá»‰ hai bÃªn sáº½ <em>á»•n Ä‘á»‹nh</em> hÆ¡n.</p>

<p><a name="giai-thich-bang-giai-tich"></a></p>

<h5 id="giáº£i-thÃ­ch-báº±ng-giáº£i-tÃ­ch">Giáº£i thÃ­ch báº±ng giáº£i tÃ­ch</h5>
<p>ChÃºng ta cÃ¹ng quay láº¡i má»™t chÃºt vá»›i Giáº£i tÃ­ch I nÄƒm thá»© nháº¥t Ä‘áº¡i há»c: <a href="http://mathworld.wolfram.com/TaylorSeries.html">Khai triá»ƒn Taylor</a>.</p>

<p>Vá»›i \(\varepsilon\) ráº¥t nhá», ta cÃ³ hai xáº¥p xá»‰ sau:</p>

<p>\[
f(x + \varepsilon) \approx f(x) + fâ€™(x)\varepsilon + \frac{fâ€(x)}{2} \varepsilon^2 + \dots
\]</p>

<p>vÃ :
\[
f(x - \varepsilon) \approx f(x) - fâ€™(x)\varepsilon + \frac{fâ€(x)}{2} \varepsilon^2 - \dots
\]</p>

<p>Tá»« Ä‘Ã³ ta cÃ³: 
\[
\frac{f(x + \varepsilon) - f(x)}{\varepsilon} \approx fâ€™(x) + \frac{fâ€(x)}{2}\varepsilon + \dots =  fâ€™(x) + O(\varepsilon) ~~ (3)
\]</p>

<p>\[
\frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon} \approx fâ€™(x) + \frac{f^{(3)}(x)}{6}\varepsilon^2 + \dots =  fâ€™(x) + O(\varepsilon^2) ~~(4)
\]</p>

<p>Tá»« Ä‘Ã³, náº¿u xáº¥p xá»‰ Ä‘áº¡o hÃ m báº±ng cÃ´ng thá»©c \((3)\) (xáº¥p xá»‰ Ä‘áº¡o hÃ m pháº£i), sai sá»‘ sáº½ lÃ  \(O(\varepsilon)\). Trong khi Ä‘Ã³, náº¿u xáº¥p xá»‰ Ä‘áº¡o hÃ m báº±ng cÃ´ng thá»©c \((4)\) (xáº¥p xá»‰ Ä‘áº¡o hÃ m hai phÃ­a), sai sá»‘ sáº½ lÃ  \(O(\varepsilon^2) \ll O(\varepsilon)\) náº¿u \(\varepsilon\) nhá».</p>

<p>Cáº£ hai cÃ¡ch giáº£i thÃ­ch trÃªn Ä‘Ã¢y Ä‘á»u cho chÃºng ta tháº¥y ráº±ng, xáº¥p xá»‰ Ä‘áº¡o hÃ m hai
phÃ­a lÃ  xáº¥p xá»‰ tá»‘t hÆ¡n.</p>

<p><a name="voi-ham-nhieu-bien"></a></p>

<h5 id="vá»›i-hÃ m-nhiá»u-biáº¿n">Vá»›i hÃ m nhiá»u biáº¿n</h5>

<p>Vá»›i hÃ m nhiá»u biáº¿n, cÃ´ng thá»©c \((2)\) Ä‘Æ°á»£c Ã¡p dá»¥ng cho tá»«ng biáº¿n khi cÃ¡c biáº¿n
khÃ¡c cá»‘ Ä‘á»‹nh. CÃ¡ch tÃ­nh nÃ y thÆ°á»ng cho giÃ¡ trá»‹ khÃ¡ chÃ­nh xÃ¡c. Tuy nhiÃªn, cÃ¡ch
nÃ y khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m vÃ¬ Ä‘á»™ phá»©c táº¡p quÃ¡ cao so vá»›i cÃ¡ch tÃ­nh
trá»±c tiáº¿p. Khi so sÃ¡nh Ä‘áº¡o hÃ m nÃ y vá»›i Ä‘áº¡o hÃ m chÃ­nh xÃ¡c tÃ­nh theo cÃ´ng thá»©c,
ngÆ°á»i ta thÆ°á»ng giáº£m sá»‘ chiá»u dá»¯ liá»‡u vÃ  giáº£m sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»ƒ thuáº­n tiá»‡n cho
tÃ­nh toÃ¡n. Má»™t khi Ä‘áº¡o hÃ m tÃ­nh Ä‘Æ°á»£c ráº¥t gáº§n vá»›i <em>numerical gradient</em>, chÃºng ta
cÃ³ thá»ƒ tá»± tin ráº±ng Ä‘áº¡o hÃ m tÃ­nh Ä‘Æ°á»£c lÃ  chÃ­nh xÃ¡c.</p>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  má»™t Ä‘oáº¡n code Ä‘Æ¡n giáº£n Ä‘á»ƒ kiá»ƒm tra Ä‘áº¡o hÃ m vÃ  cÃ³ thá»ƒ Ã¡p dá»¥ng vá»›i má»™t
hÃ m sá»‘ (cá»§a má»™t vector) báº¥t ká»³ vá»›i <code class="language-plaintext highlighter-rouge">cost</code> vÃ  <code class="language-plaintext highlighter-rouge">grad</code> Ä‘Ã£ tÃ­nh á»Ÿ phÃ­a trÃªn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">w_p</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">w_n</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">w_p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eps</span> 
        <span class="n">w_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">eps</span>
        <span class="n">g</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">w_p</span><span class="p">)</span> <span class="o">-</span> <span class="n">cost</span><span class="p">(</span><span class="n">w_n</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span> 

<span class="k">def</span> <span class="nf">check_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">grad1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">grad2</span> <span class="o">=</span> <span class="n">numerical_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad1</span> <span class="o">-</span> <span class="n">grad2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span> <span class="k">else</span> <span class="bp">False</span> 

<span class="k">print</span><span class="p">(</span> <span class="s">'Checking gradient...'</span><span class="p">,</span> <span class="n">check_grad</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Checking gradient... True
</code></pre></div></div>

<p>(<em>Vá»›i cÃ¡c hÃ m sá»‘ khÃ¡c, báº¡n Ä‘á»c chá»‰ cáº§n viáº¿t láº¡i hÃ m <code class="language-plaintext highlighter-rouge">grad</code> vÃ  <code class="language-plaintext highlighter-rouge">cost</code> á»Ÿ pháº§n trÃªn
rá»“i Ã¡p dá»¥ng Ä‘oáº¡n code nÃ y Ä‘á»ƒ kiá»ƒm tra Ä‘áº¡o hÃ m. Náº¿u hÃ m sá»‘ lÃ  hÃ m cá»§a má»™t ma tráº­n
thÃ¬ chÃºng ta thay Ä‘á»•i má»™t chÃºt trong hÃ m <code class="language-plaintext highlighter-rouge">numerical_grad</code>, tÃ´i hy vá»ng khÃ´ng quÃ¡
phá»©c táº¡p</em>).</p>

<p>Vá»›i bÃ i toÃ¡n Linear Regression, cÃ¡ch tÃ­nh Ä‘áº¡o hÃ m nhÆ° trong \((1)\) phÃ­a trÃªn
Ä‘Æ°á»£c coi lÃ  Ä‘Ãºng vÃ¬ sai sá»‘ giá»¯a hai cÃ¡ch tÃ­nh lÃ  ráº¥t nhá» (nhá» hÆ¡n
\(10^{-6}\)). Sau khi cÃ³ Ä‘Æ°á»£c Ä‘áº¡o hÃ m chÃ­nh xÃ¡c, chÃºng ta viáº¿t hÃ m cho GD:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">myGD</span><span class="p">(</span><span class="n">w_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_init</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">w_new</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">w_new</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>
            <span class="k">break</span> 
        <span class="n">w</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span> 

<span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">it1</span><span class="p">)</span> <span class="o">=</span> <span class="n">myGD</span><span class="p">(</span><span class="n">w_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Solution found by GD: w = '</span><span class="p">,</span> <span class="n">w1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="s">',</span><span class="se">\n</span><span class="s">after %d iterations.'</span> <span class="o">%</span><span class="p">(</span><span class="n">it1</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Solution found by GD: w =  [[ 4.01780793  2.97133693]] ,
after 49 iterations.
</code></pre></div></div>

<p>Sau 49 vÃ²ng láº·p, thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥ vá»›i má»™t nghiá»‡m khÃ¡ gáº§n vá»›i nghiá»‡m tÃ¬m Ä‘Æ°á»£c
theo cÃ´ng thá»©c.</p>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  hÃ¬nh Ä‘á»™ng minh há»a thuáº­t toÃ¡n GD.</p>

<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/GD/img1_1.gif" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/GD/img2_1.gif" />
        </td>
    </tr>
</table>

<p>Trong hÃ¬nh bÃªn trÃ¡i, cÃ¡c Ä‘Æ°á»ng tháº³ng mÃ u Ä‘á» lÃ  nghiá»‡m tÃ¬m Ä‘Æ°á»£c sau má»—i vÃ²ng láº·p.</p>

<p>Trong hÃ¬nh bÃªn pháº£i, tÃ´i xin giá»›i thiá»‡u má»™t thuáº­t ngá»¯ má»›i: <em>Ä‘Æ°á»ng Ä‘á»“ng má»©c</em>.
<a name="duong-dong-muc-level-sets"></a></p>

<h4 id="Ä‘Æ°á»ng-Ä‘á»“ng-má»©c-level-sets">ÄÆ°á»ng Ä‘á»“ng má»©c (level sets)</h4>

<p>Vá»›i Ä‘á»“ thá»‹ cá»§a má»™t hÃ m sá»‘ vá»›i hai biáº¿n Ä‘áº§u vÃ o cáº§n Ä‘Æ°á»£c váº½ trong khÃ´ng gian ba
chiá»u, nhá»u khi chÃºng ta khÃ³ nhÃ¬n Ä‘Æ°á»£c nghiá»‡m cÃ³ khoáº£ng tá»a Ä‘á»™ bao nhiÃªu. Trong
toÃ¡n tá»‘i Æ°u, ngÆ°á»i ta thÆ°á»ng dÃ¹ng má»™t cÃ¡ch váº½ sá»­ dá»¥ng khÃ¡i niá»‡m <em>Ä‘Æ°á»ng Ä‘á»“ng má»©c</em>
(level sets).</p>

<p>Náº¿u cÃ¡c báº¡n Ä‘á»ƒ Ã½ trong cÃ¡c báº£n Ä‘á»™ tá»± nhiÃªn, Ä‘á»ƒ miÃªu táº£ Ä‘á»™ cao cá»§a cÃ¡c dÃ£y nÃºi,
ngÆ°á»i ta dÃ¹ng nhiá»u Ä‘Æ°á»ng cong kÃ­n bao quanh nhau nhÆ° sau:</p>

<div class="imgcap">
 <img src="http://files.vforum.vn/2016/T06/img/vforum.vn-324944-hinh-44-lc6b0e1bba3c-c491e1bb93-c491e1bb8ba-hc3acnh-te1bb89-le1bb87-le1bb9bn.png" align="center" width="600" />
 <div class="thecap"> VÃ­ dá»¥ vá» Ä‘Æ°á»ng Ä‘á»“ng má»©c trong cÃ¡c báº£n Ä‘á»“ tá»± nhiÃªn. (Nguá»“n: <a href="http://vforum.vn/diendan/showthread.php?90166-Dia-ly-6-Duong-dong-muc-la-nhung-duong-nhu-the-nao-">Äá»‹a lÃ½ 6: ÄÆ°á»ng Ä‘á»“ng má»©c lÃ  nhá»¯ng Ä‘Æ°á»ng nhÆ° tháº¿ nÃ o?</a>)</div>
</div>

<p>CÃ¡c vÃ²ng nhá» mÃ u Ä‘á» hÆ¡n thá»ƒ hiá»‡n cÃ¡c Ä‘iá»ƒm á»Ÿ trÃªn cao hÆ¡n.</p>

<p>Trong toÃ¡n tá»‘i Æ°u, ngÆ°á»i ta cÅ©ng dÃ¹ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»ƒ thá»ƒ hiá»‡n cÃ¡c bá» máº·t
trong khÃ´ng gian hai chiá»u.</p>

<p>Quay trá»Ÿ láº¡i vá»›i hÃ¬nh minh há»a thuáº­t toÃ¡n GD cho bÃ i toÃ¡n Liner Regression bÃªn
trÃªn, hÃ¬nh bÃªn pháº£i lÃ  hÃ¬nh biá»ƒu diá»…n cÃ¡c level sets. Tá»©c lÃ  táº¡i cÃ¡c Ä‘iá»ƒm trÃªn
cÃ¹ng má»™t vÃ²ng, hÃ m máº¥t mÃ¡t cÃ³ giÃ¡ trá»‹ nhÆ° nhau. Trong vÃ­ dá»¥ nÃ y, tÃ´i hiá»ƒn thá»‹
giÃ¡ trá»‹ cá»§a hÃ m sá»‘ táº¡i má»™t sá»‘ vÃ²ng. CÃ¡c vÃ²ng mÃ u xanh cÃ³ giÃ¡ trá»‹ tháº¥p, cÃ¡c vÃ²ng
trÃ²n mÃ u Ä‘á» phÃ­a ngoÃ i cÃ³ giÃ¡ trá»‹ cao hÆ¡n. Äiá»ƒm nÃ y khÃ¡c má»™t chÃºt so vá»›i Ä‘Æ°á»ng
Ä‘á»“ng má»©c trong tá»± nhiÃªn lÃ  cÃ¡c vÃ²ng bÃªn trong thÆ°á»ng thá»ƒ hiá»‡n má»™t thung lÅ©ng hÆ¡n
lÃ  má»™t Ä‘á»‰nh nÃºi (vÃ¬ chÃºng ta Ä‘ang Ä‘i tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t).</p>

<p>TÃ´i thá»­ vá»›i <em>learning rate</em> nhá» hÆ¡n, káº¿t quáº£ nhÆ° sau:</p>

<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/GD/img1_0.1.gif" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/GD/img2_0.1.gif" />
        </td>
    </tr>
</table>

<p>Tá»‘c Ä‘á»™ há»™i tá»¥ Ä‘Ã£ cháº­m Ä‘i nhiá»u, tháº­m chÃ­ sau 99 vÃ²ng láº·p, GD váº«n chÆ°a tá»›i gáº§n
Ä‘Æ°á»£c nghiá»‡m tá»‘t nháº¥t. Trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿, chÃºng ta cáº§n nhiá»u vÃ²ng láº·p
hÆ¡n 99 ráº¥t nhiá»u, vÃ¬ sá»‘ chiá»u vÃ  sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u thÆ°á»ng lÃ  ráº¥t lá»›n.</p>

<p><a name="-mot-vi-du-khac"></a></p>

<h2 id="4-má»™t-vÃ­-dá»¥-khÃ¡c">4. Má»™t vÃ­ dá»¥ khÃ¡c</h2>

<p>Äá»ƒ káº¿t thÃºc pháº§n 1 cá»§a Gradient Descent, tÃ´i xin nÃªu thÃªm má»™t vÃ­ dá»¥ khÃ¡c.</p>

<div class="imgcap">
 <img src="/assets/GD/img3_0.015.gif" align="center" width="800" />
</div>

<p>HÃ m sá»‘ \(f(x, y) = (x^2 + y - 7)^2 + (x - y + 1)^2\) cÃ³ hai Ä‘iá»ƒm local minimum
mÃ u xanh lá»¥c táº¡i \((2, 3)\) vÃ  \((-3, -2)\), vÃ  chÃºng cÅ©ng lÃ  hai Ä‘iá»ƒm
global minimum. Trong vÃ­ dá»¥ nÃ y, tÃ¹y vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o mÃ  chÃºng ta thu Ä‘Æ°á»£c cÃ¡c
nghiá»‡m cuá»‘i cÃ¹ng khÃ¡c nhau.</p>

<p><a name="-thao-luan"></a></p>

<h2 id="5-tháº£o-luáº­n">5. Tháº£o luáº­n</h2>

<p>Dá»±a trÃªn GD, cÃ³ ráº¥t nhiá»u thuáº­t toÃ¡n phá»©c táº¡p vÃ  hiá»‡u quáº£ hÆ¡n Ä‘Æ°á»£c thiáº¿t káº¿ cho
nhá»¯ng loáº¡i bÃ i toÃ¡n khÃ¡c nhau. VÃ¬ bÃ i nÃ y Ä‘Ã£ Ä‘á»§ dÃ i, tÃ´i xin phÃ©p dá»«ng láº¡i á»Ÿ
Ä‘Ã¢y. Má»i cÃ¡c báº¡n Ä‘Ã³n Ä‘á»c bÃ i Gradient Descent pháº§n 2 vá»›i nhiá»u ká»¹ thuáº­t nÃ¢ng cao
hÆ¡n.</p>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-tÃ i-liá»‡u-tham-kháº£o">6. TÃ i liá»‡u tham kháº£o</h2>
<ol>
  <li><a href="http://sebastianruder.com/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a></li>
  <li><a href="http://www.benfrederickson.com/numerical-optimization/">An Interactive Tutorial on Numerical Optimization</a></li>
  <li><a href="https://www.youtube.com/watch?v=eikJboPQDT0">Gradient Descent by Andrew NG</a></li>
</ol>
:ET