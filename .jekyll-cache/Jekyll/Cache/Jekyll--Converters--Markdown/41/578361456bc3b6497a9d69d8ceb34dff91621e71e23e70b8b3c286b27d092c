I"æ%<p>TrÆ°á»›c Deep Learning, bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh (cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c cÅ©ng tÆ°Æ¡ng tá»±) thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh 2 bÆ°á»›c: Feature Engineering vÃ  Train a Classifier. Hai bÆ°á»›c nÃ y thÆ°á»ng Ä‘Æ°á»£c tÃ¡ch rá»i nhau. Vá»›i Feature Engineering, cÃ¡c phÆ°Æ¡ng phÃ¡p thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng cho áº£nh lÃ  <a href="http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html">SIFT</a> (Scale Invariant Feature Transform), <a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html">SURF</a> (Speeded-Up Robust Features), <a href="http://www.learnopencv.com/histogram-of-oriented-gradients/">HOG</a> (Histogram of Oriented Gradients), LBP (Local Binary Pattern), etc. CÃ¡c Classifier thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  <a href="/2017/04/28/multiclasssmv/">multi-class SVM</a>, <a href="/2017/02/17/softmax/">Softmax Regression</a>, Discriminative Dictionary Learning, Random Forest, etc.</p>

<p>CÃ¡c phÆ°Æ¡ng phÃ¡p Feature Engineering nÃªu trÃªn thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  cÃ¡c <em>hand-crafted features</em> (feature Ä‘Æ°á»£c táº¡o thá»§ cÃ´ng) vÃ¬ nÃ³ chá»§ yáº¿u dá»±a trÃªn cÃ¡c quan sÃ¡t vá» Ä‘áº·c tÃ­nh riÃªng cá»§a áº£nh. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cho káº¿t quáº£ khÃ¡ áº¥n tÆ°á»£ng trong má»™t sá»‘ trÆ°á»ng há»£p. Tuy nhiÃªn, chÃºng váº«n cÃ²n nhiá»u háº¡n cháº¿ vÃ¬ quÃ¡ trÃ¬nh tÃ¬m ra cÃ¡c features vÃ  cÃ¡c classifier phÃ¹ há»£p váº«n lÃ  riÃªng biá»‡t.</p>

<p>(TÃ´i Ä‘Ã£ tá»«ng Ä‘á» cáº­p tá»›i váº¥n Ä‘á» nÃ y trong <a href="/2017/04/28/multiclasssmv/#-mo-hinh-end-to-end">cÃ¡c mÃ´ hÃ¬nh end-to-end</a>)</p>

<p>Nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, Deep Learning phÃ¡t triá»ƒn cá»±c nhanh dá»±a trÃªn lÆ°á»£ng dá»¯ liá»‡u training khá»•ng lá»“ vÃ  kháº£ nÄƒng tÃ­nh toÃ¡n ngÃ y cÃ ng Ä‘Æ°á»£c cáº£i tiáº¿n cá»§a cÃ¡c mÃ¡y tÃ­nh. CÃ¡c káº¿t quáº£ cho bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh ngÃ y cÃ ng Ä‘Æ°á»£c nÃ¢ng cao. Bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng nháº¥t lÃ  <a href="https://www.image-net.org">ImageNet</a> vá»›i 1.2M áº£nh cho 1000 classes khÃ¡c nhau. Ráº¥t nhiá»u cÃ¡c mÃ´ hÃ¬nh Deep Learning Ä‘Ã£ giÃ nh chiáº¿n tháº¯ng trong cÃ¡c cuá»™c thi <a href="https://www.google.com/search?client=opera&amp;q=imagenet+results&amp;sourceid=opera&amp;ie=UTF-8&amp;oe=UTF-8#q=ILSVRC+">ILSVRC</a> (ImageNet Large Scale Visual Recognition Challenge). CÃ³ thá»ƒ ká»ƒ ra má»™t vÃ i: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>, <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">ZFNet</a>, <a href="https://arxiv.org/abs/1409.4842v1">GoogLeNet</a>, <a href="https://arxiv.org/pdf/1502.01852.pdf">ResNet</a>, <a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">VGG</a>.</p>

<hr />

<div class="imgcap">
<img src="/assets/q2_tl/multi_layers.png" align="center" width="500" />
</div>

<div class="thecap" style="text-align: justify">HÃ¬nh 1: MÃ´ hÃ¬nh chung cho cÃ¡c bÃ i toÃ¡n classification sá»­ dá»¥ng Deep Learning. Layer cuá»‘i cÃ¹ng thÆ°á»ng lÃ  má»™t Fully Connected Layer vÃ  thÆ°á»ng lÃ  má»™t Softmax Regression.</div>
<hr />

<p>NhÃ¬n chung, cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘á»u bao gá»“m ráº¥t nhiá»u layers. CÃ¡c layers phÃ­a trÆ°á»›c thÆ°á»ng lÃ  cÃ¡c Convolutional layers káº¿t há»£p vá»›i cÃ¡c nonlinear <a href="/2017/02/24/mlp/#-activation-functions">activation functions</a> vÃ  pooling layers (vÃ  Ä‘Æ°á»£c gá»i chung lÃ  ConvNet). Layer cuá»‘i cÃ¹ng lÃ  má»™t Fully Connected Layer vÃ  thÆ°á»ng lÃ  má»™t Softmax Regression (Xem HÃ¬nh 1). Sá»‘ lÆ°á»£ng units á»Ÿ layer cuá»‘i cÃ¹ng báº±ng vá»›i sá»‘ lÆ°á»£ng classes (vá»›i ImageNet lÃ  1000). VÃ¬ váº­y output á»Ÿ layer gáº§n cuá»‘i cÃ¹ng (second to last layer) cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  feature vectors vÃ  Softmax Regression chÃ­nh lÃ  Classifier Ä‘Æ°á»£c sá»­ dá»¥ng.</p>

<p>ChÃ­nh nhá» viá»‡c features vÃ  classifier Ä‘Æ°á»£c trained cÃ¹ng nhau qua deep networks khiáº¿n cho cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘áº¡t káº¿t quáº£ tá»‘t. Tuy nhiÃªn, nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘á»u lÃ  cÃ¡c Deep Networks vá»›i ráº¥t nhiá»u layers. Viá»‡c training dá»±a trÃªn 1.2M bá»©c áº£nh cá»§a ImageNet cÅ©ng tá»‘n ráº¥t nhiá»u thá»i gian (2-3 tuáº§n).</p>

<p>Vá»›i cÃ¡c bÃ i toÃ n dá»±a trÃªn táº­p dá»¯ liá»‡u khÃ¡c, ráº¥t Ã­t khi ngÆ°á»i ta xÃ¢y dá»±ng vÃ  train láº¡i toÃ n bá»™ Network tá»« Ä‘áº§u, bá»Ÿi vÃ¬ cÃ³ ráº¥t Ã­t cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c lá»›n. Thay vÃ o Ä‘Ã³, phÆ°Æ¡ng phÃ¡p thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng lÃ  sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh (nÃªu phÃ­a trÃªn) Ä‘Ã£ Ä‘Æ°á»£c trained tá»« trÆ°á»›c, vÃ  sá»­ dá»¥ng má»™t vÃ i ká»¹ thuáº­t khÃ¡c Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n. PhÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh cÃ³ sáºµn nhÆ° tháº¿ nÃ y Ä‘Æ°á»£c gá»i lÃ  <em>Transfer Learning</em>.</p>

<p>NhÆ° Ä‘Ã£ Ä‘á» cáº­p, toÃ n bá»™ cÃ¡c layer trá»« output layer cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t bá»™ Feature Extractor. Dá»±a trÃªn nháº­n xÃ©t ráº±ng cÃ¡c bá»©c áº£nh Ä‘á»u cÃ³ nhá»¯ng Ä‘áº·c tÃ­nh giá»‘ng nhau nÃ o Ä‘Ã³, vá»›i cÆ¡ sá»Ÿ dá»¯ liá»‡u khÃ¡c, ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng pháº§n Feature Extractor nÃ y Ä‘á»ƒ táº¡o ra cÃ¡c feature vectors. Sau Ä‘Ã³, ta thay output layer cÅ©ng báº±ng má»™t Softmax Regression (hoáº·c multi-class SVM) nhÆ°ng vá»›i sá»‘ lÆ°á»£ng units báº±ng vá»›i sá»‘ lÆ°á»£ng class á»Ÿ bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u má»›i. Ta chá»‰ cáº§n train layer cuá»‘i cÃ¹ng nÃ y. Kinh nghiá»‡m thá»±c táº¿ cá»§a tÃ´i cho tháº¥y, viá»‡c lÃ m nÃ y Ä‘Ã£ tÄƒng káº¿t quáº£ phÃ¢n lá»›p lÃªn ráº¥t nhiá»u so vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c <em>hand-crafted features</em>.</p>

<p>CÃ¡ch lÃ m nhÆ° trÃªn Ä‘Æ°á»£c gá»i lÃ  <strong>ConvNet as fixed feature extractor</strong>, tá»©c ta sá»­ dá»¥ng trá»±c tiáº¿p vector á»Ÿ <em>second to last layer</em> lÃ m feature vector. Náº¿u tiáº¿p tá»¥c tinh chá»‰nh (Fine-tuning) má»™t chÃºt ná»¯a, káº¿t quáº£ sáº½ cÃ³ thá»ƒ tá»‘t hÆ¡n.</p>

<p><strong>Fine-tuning the ConvNet</strong>. HÆ°á»›ng tiáº¿p cáº­n thá»© hai lÃ  sá»­ dá»¥ng cÃ¡c weights Ä‘Ã£ Ä‘Æ°á»£c trained tá»« má»™t trong cÃ¡c mÃ´ hÃ¬nh ConvNet nhÆ° lÃ  khá»Ÿi táº¡o cho mÃ´ hÃ¬nh má»›i vá»›i dá»¯ liá»‡u má»›i vÃ  sá»­ dá»¥ng <a href="/2017/02/24/mlp/#-backpropagation">Back Propagation</a> Ä‘á»ƒ train láº¡i toÃ n bá»™ mÃ´ hÃ¬nh má»›i hoáº·c train láº¡i má»™t sá»‘ layer cuá»‘i (cÅ©ng lÃ  Ä‘á»ƒ trÃ¡nh <a href="/2017/03/04/overfitting/">overfitting</a> khi mÃ  mÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p khi dá»¯ liá»‡u khÃ´ng Ä‘á»§ lá»›n). <em>Viá»‡c nÃ y Ä‘Æ°á»£c dá»±a trÃªn quan sÃ¡t ráº±ng nhá»¯ng layers Ä‘áº§u trong ConvNet thÆ°á»ng giÃºp extract nhá»¯ng Ä‘áº·c tÃ­nh chung cá»§a áº£nh (cÃ¡c cáº¡nh - edges, cÃ²n Ä‘Æ°á»£c gá»i lÃ  low-level features), cÃ¡c layers cuá»‘i thÆ°á»ng mang nhá»¯ng Ä‘áº·c trÆ°ng riÃªng cá»§a cÆ¡ sá»Ÿ dá»¯ liá»‡u (CSDL) (vÃ  Ä‘Æ°á»£c gá»i lÃ  high-level features). VÃ¬ váº­y, viá»‡c train cÃ¡c layer cuá»‘i mang nhiá»u giÃ¡ trá»‹ hÆ¡n.</em></p>

<p>Dá»±a trÃªn kÃ­ch thÆ°á»›c vÃ  Ä‘á»™ tÆ°Æ¡ng quan giá»¯a CSDL má»›i vÃ  CSDL gá»‘c (chá»§ yáº¿u lÃ  ImageNet) Ä‘á»ƒ train cÃ¡c mÃ´ hÃ¬nh cÃ³ sáºµn, <a href="http://cs231n.github.io/transfer-learning/#tf">CS231n Ä‘Æ°a ra má»™t vÃ i lá»i khuyÃªn</a>:</p>

<ul>
  <li>
    <p><em>CSDL má»›i lÃ  nhá» vÃ  tÆ°Æ¡ng tá»± nhÆ° CSDL gá»‘c.</em> VÃ¬ CSDL má»›i nhá», viá»‡c tiáº¿p tá»¥c train model dá»… dáº«n Ä‘áº¿n hiá»‡n tÆ°á»£ng overfitting. CÅ©ng vÃ¬ hai CSDL lÃ  tÆ°Æ¡ng tá»± nhau, ta dá»± Ä‘oÃ¡n ráº±ng cÃ¡c high-level features lÃ  tÆ°Æ¡ng tá»± nhau. Váº­y nÃªn ta khÃ´ng cáº§n train láº¡i model mÃ  chá»‰ cáº§n train má»™t classifer dá»±a trÃªn feature vectors á»Ÿ Ä‘áº§u ra á»Ÿ layer gáº§n cuá»‘i.</p>
  </li>
  <li>
    <p><em>CSDL má»›i lÃ  lá»›n vÃ  tÆ°Æ¡ng tá»± nhÆ° CSDL gá»‘c.</em> VÃ¬ CSDL nÃ y lá»›n, overfitting Ã­t cÃ³ kháº£ nÄƒng xáº£y ra hÆ¡n, ta cÃ³ thá»ƒ train mÃ´ hÃ¬nh thÃªm má»™t chÃºt ná»¯a (toÃ n bá»™ hoáº·c chá»‰ má»™t vÃ i layers cuá»‘i).</p>
  </li>
  <li>
    <p><em>CSDL má»›i lÃ  nhá» vÃ  ráº¥t khÃ¡c vá»›i CSDL gá»‘c.</em> VÃ¬ CSDL nÃ y nhá», tá»‘t hÆ¡n háº¿t lÃ  dÃ¹ng cÃ¡c classifier Ä‘Æ¡n giáº£n (cÃ¡c linear classifiers) Ä‘á»ƒ trÃ¡nh overfitting). Náº¿u muá»‘n train thÃªm, ta cÅ©ng chá»‰ nÃªn train cÃ¡c layer cuá»‘i. Hoáº·c cÃ³ má»™t ká»¹ thuáº­t khÃ¡c lÃ  coi Ä‘áº§u ra cá»§a má»™t layer xa layer cuá»‘i hÆ¡n lÃ m cÃ¡c feature vectors.</p>
  </li>
  <li>
    <p><em>CSDL má»›i lÃ  lá»›n vÃ  ráº¥t khÃ¡c CSDL gá»‘c.</em> Trong trÆ°á»ng há»£p nÃ y, ta váº«n cÃ³ thá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ train nhÆ° lÃ  Ä‘iá»ƒm khá»Ÿi táº¡o cho mÃ´ hÃ¬nh má»›i, khÃ´ng nÃªn train láº¡i tá»« Ä‘áº§u.</p>
  </li>
</ul>

<p>CÃ³ má»™t Ä‘iá»ƒm Ä‘Ã¡ng chÃº Ã½ ná»¯a lÃ  khi tiáº¿p tá»¥c train cÃ¡c mÃ´ hÃ¬nh nÃ y, ta chá»‰ nÃªn chá»n <em>learning rate</em> nhá» Ä‘á»ƒ cÃ¡c weights má»›i khÃ´ng Ä‘i quÃ¡ xa so vá»›i cÃ¡c weights Ä‘Ã£ Ä‘Æ°á»£c trained á»Ÿ cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c.</p>

<p><a name="tai-lieu-tham-khao"></a></p>

<h2 id="Ä‘á»c-thÃªm">Äá»c thÃªm</h2>
<p>[1] <a href="http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html">Introduction to SIFT (Scale-Invariant Feature Transform) - OpenCV</a></p>

<p>[2] <a href="Introduction to SURF (Speeded-Up Robust Features)">Introduction to SURF (Speeded-Up Robust Features) - OpenCV</a></p>

<p>[3] <a href="http://www.learnopencv.com/histogram-of-oriented-gradients/">Histogram of Oriented Gradients - OpenCV</a></p>

<p>[4] <a href="http://cs231n.github.io/transfer-learning/#tf">Transfer Learning</a></p>

<p>[5] <a href="http://sebastianruder.com/transfer-learning/">Transfer Learning - Machine Learningâ€™s Next Frontier</a></p>

<p>[6] <a href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/">Transfer learning &amp; The art of using Pre-trained Models in Deep Learning</a></p>
:ET