I"E™<p>Cá»© lÃ m Ä‘i, sai Ä‘Ã¢u sá»­a Ä‘áº¥y, cuá»‘i cÃ¹ng sáº½ thÃ nh cÃ´ng!</p>

<p>ÄÃ³ chÃ­nh lÃ  Ã½ tÆ°á»Ÿng chÃ­nh cá»§a má»™t thuáº­t toÃ¡n ráº¥t quan trá»ng trong Machine Learning - thuáº­t toÃ¡n Perceptron Learning Algorithm hay PLA.</p>

<p><strong>Trong trang nÃ y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li>
    <ol>
      <li>Giá»›i thiá»‡u
        <ul>
          <li>BÃ i toÃ¡n Perceptron</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <ol>
      <li>Thuáº­t toÃ¡n Perceptron (PLA)
        <ul>
          <li>Má»™t sá»‘ kÃ½ hiá»‡u</li>
          <li>XÃ¢y dá»±ng hÃ m máº¥t mÃ¡t</li>
          <li>TÃ³m táº¯t PLA</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <ol>
      <li>VÃ­ dá»¥ trÃªn Python
        <ul>
          <li>Load thÆ° viá»‡n vÃ  táº¡o dá»¯ liá»‡u</li>
          <li>CÃ¡c hÃ m sá»‘ cho PLA</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <ol>
      <li>Chá»©ng minh há»™i tá»¥</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>MÃ´ hÃ¬nh Neural Network Ä‘áº§u tiÃªn</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>Tháº£o Luáº­n
        <ul>
          <li>PLA cÃ³ thá»ƒ cho vÃ´ sá»‘ nghiá»‡m khÃ¡c nhau</li>
          <li>PLA Ä‘Ã²i há»i dá»¯ liá»‡u linearly separable</li>
          <li>Pocket Algorithm</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <ol>
      <li>Káº¿t luáº­n</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>TÃ i liá»‡u tham kháº£o</li>
    </ol>
  </li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-giá»›i-thiá»‡u">1. Giá»›i thiá»‡u</h2>

<p>Trong bÃ i nÃ y, tÃ´i sáº½ giá»›i thiá»‡u thuáº­t toÃ¡n Ä‘áº§u tiÃªn trong Classification cÃ³ tÃªn lÃ  Perceptron Learning Algorithm (PLA) hoáº·c Ä‘Ã´i khi Ä‘Æ°á»£c viáº¿t gá»n lÃ  Perceptron.</p>

<p>Perceptron lÃ  má»™t thuáº­t toÃ¡n Classification cho trÆ°á»ng há»£p Ä‘Æ¡n giáº£n nháº¥t: chá»‰ cÃ³ hai class (lá»›p) (<em>bÃ i toÃ¡n vá»›i chá»‰ hai class Ä‘Æ°á»£c gá»i lÃ  binary classification</em>) vÃ  cÅ©ng chá»‰ hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c trong má»™t trÆ°á»ng há»£p ráº¥t cá»¥ thá»ƒ. Tuy nhiÃªn, nÃ³ lÃ  ná»n táº£ng cho má»™t máº£ng lá»›n quan trá»ng cá»§a Machine Learning lÃ  Neural Networks vÃ  sau nÃ y lÃ  Deep Learning. (Táº¡i sao láº¡i gá»i lÃ  Neural Networks - tá»©c máº¡ng dÃ¢y tháº§n kinh - cÃ¡c báº¡n sáº½ Ä‘Æ°á»£c tháº¥y á»Ÿ cuá»‘i bÃ i).</p>

<p>Giáº£ sá»­ chÃºng ta cÃ³ hai táº­p há»£p dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘Æ°á»£c minh hoáº¡ trong HÃ¬nh 1 bÃªn trÃ¡i dÆ°á»›i Ä‘Ã¢y. Hai class cá»§a chÃºng ta lÃ  táº­p cÃ¡c Ä‘iá»ƒm mÃ u xanh vÃ  táº­p cÃ¡c Ä‘iá»ƒm mÃ u Ä‘á». BÃ i toÃ¡n Ä‘áº·t ra lÃ : tá»« dá»¯ liá»‡u cá»§a hai táº­p Ä‘Æ°á»£c gÃ¡n nhÃ£n cho trÆ°á»›c, hÃ£y xÃ¢y dá»±ng má»™t <em>classifier</em> (bá»™ phÃ¢n lá»›p) Ä‘á»ƒ khi cÃ³ má»™t Ä‘iá»ƒm dá»¯ liá»‡u hÃ¬nh tam giÃ¡c mÃ u xÃ¡m má»›i, ta cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c mÃ u (nhÃ£n) cá»§a nÃ³.</p>

<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/pla/pla1.png" />
         </td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/pla/pla2.png" />
        </td>
    </tr>
</table>
<div class="thecap">HÃ¬nh 1: BÃ i toÃ¡n Perceptron</div>

<p>Hiá»ƒu theo má»™t cÃ¡ch khÃ¡c, chÃºng ta cáº§n tÃ¬m <em>lÃ£nh thá»•</em> cá»§a má»—i class sao cho, vá»›i má»—i má»™t Ä‘iá»ƒm má»›i, ta chá»‰ cáº§n xÃ¡c Ä‘á»‹nh xem nÃ³ náº±m vÃ o lÃ£nh thá»• cá»§a class nÃ o rá»“i quyáº¿t Ä‘á»‹nh nÃ³ thuá»™c class Ä‘Ã³. Äá»ƒ tÃ¬m <em>lÃ£nh thá»•</em> cá»§a má»—i class, chÃºng ta cáº§n Ä‘i tÃ¬m biÃªn giá»›i (boundary) giá»¯a hai <em>lÃ£nh thá»•</em> nÃ y. Váº­y bÃ i toÃ¡n classification cÃ³ thá»ƒ coi lÃ  bÃ i toÃ¡n Ä‘i tÃ¬m boundary giá»¯a cÃ¡c class. VÃ  boundary Ä‘Æ¡n giáº£n nhÃ¢t trong khÃ´ng gian hai chiá»u lÃ  má»™t Ä‘Æ°á»ng tháº±ng, trong khÃ´ng gian ba chiá»u lÃ  má»™t máº·t pháº³ng, trong khÃ´ng gian nhiá»u chiá»u lÃ  má»™t siÃªu máº·t pháº³ng (hyperplane) (tÃ´i gá»i chung nhá»¯ng boundary nÃ y lÃ  <em>Ä‘Æ°á»ng pháº³ng</em>). Nhá»¯ng boundary pháº³ng nÃ y Ä‘Æ°á»£c coi lÃ  Ä‘Æ¡n giáº£n vÃ¬ nÃ³ cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i dáº¡ng toÃ¡n há»c báº±ng má»™t hÃ m sá»‘ Ä‘Æ¡n giáº£n cÃ³ dáº¡ng tuyáº¿n tÃ­nh, tá»©c linear. Táº¥t nhiÃªn, chÃºng ta Ä‘ang giáº£ sá»­ ráº±ng tá»“n táº¡i má»™t Ä‘Æ°á»ng pháº³ng Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n Ä‘á»‹nh <em>lÃ£nh thá»•</em> cá»§a hai class. HÃ¬nh 1 bÃªn pháº£i minh há»a má»™t Ä‘Æ°á»ng tháº³ng phÃ¢n chia hai class trong máº·t pháº³ng. Pháº§n cÃ³ ná»n mÃ u xanh Ä‘Æ°á»£c coi lÃ  <em>lÃ£nh thá»•</em> cá»§a lá»›p xanh, pháº§n cÃ³ nÃªn mÃ u Ä‘á» Ä‘Æ°á»£c coi lÃ  <em>lÃ£nh thá»•</em> cá»§a lá»›p Ä‘á». Trong trÆ°á»ng há»£p nÃ y, Ä‘iá»ƒm dá»¯ liá»‡u má»›i hÃ¬nh tam giÃ¡c Ä‘Æ°á»£c phÃ¢n vÃ o class Ä‘á».</p>

<p><a name="bai-toan-perceptron"></a></p>

<h3 id="bÃ i-toÃ¡n-perceptron">BÃ i toÃ¡n Perceptron</h3>
<p>BÃ i toÃ¡n Perceptron Ä‘Æ°á»£c phÃ¡t biá»ƒu nhÆ° sau: <em>Cho hai class Ä‘Æ°á»£c gÃ¡n nhÃ£n, hÃ£y tÃ¬m má»™t Ä‘Æ°á»ng pháº³ng sao cho toÃ n bá»™ cÃ¡c Ä‘iá»ƒm thuá»™c class 1 náº±m vá» 1 phÃ­a, toÃ n bá»™ cÃ¡c Ä‘iá»ƒm thuá»™c class 2 náº±m vá» phÃ­a cÃ²n láº¡i cá»§a Ä‘Æ°á»ng pháº³ng Ä‘Ã³. Vá»›i giáº£ Ä‘á»‹nh ráº±ng tá»“n táº¡i má»™t Ä‘Æ°á»ng pháº³ng nhÆ° tháº¿.</em></p>

<p>Náº¿u tá»“n táº¡i má»™t Ä‘Æ°á»ng pháº³ng phÃ¢n chia hai class thÃ¬ ta gá»i hai class Ä‘Ã³ lÃ  <em>linearly separable</em>. CÃ¡c thuáº­t toÃ¡n classification táº¡o ra cÃ¡c boundary lÃ  cÃ¡c Ä‘Æ°á»ng pháº³ng Ä‘Æ°á»£c gá»i chung lÃ  Linear Classifier.</p>

<p><a name="-thuat-toan-perceptron-pla"></a></p>

<h2 id="2-thuáº­t-toÃ¡n-perceptron-pla">2. Thuáº­t toÃ¡n Perceptron (PLA)</h2>
<p>CÅ©ng giá»‘ng nhÆ° cÃ¡c thuáº­t toÃ¡n láº·p trong <a href="/2017/01/01/kmeans/">K-means Clustering</a> vÃ  <a href="/2017/01/12/gradientdescent/">Gradient Descent</a>, Ã½ tÆ°á»Ÿng cÆ¡ báº£n cá»§a PLA lÃ  xuáº¥t phÃ¡t tá»« má»™t nghiá»‡m dá»± Ä‘oÃ¡n nÃ o Ä‘Ã³, qua má»—i vÃ²ng láº·p, nghiá»‡m sáº½ Ä‘Æ°á»£c cáº­p nháº­t tá»›i má»™t vÃ­ trÃ­ tá»‘t hÆ¡n. Viá»‡c cáº­p nháº­t nÃ y dá»±a trÃªn viá»‡c giáº£m giÃ¡ trá»‹ cá»§a má»™t hÃ m máº¥t mÃ¡t nÃ o Ä‘Ã³.</p>

<p><a name="mot-so-ky-hieu"></a></p>

<h3 id="má»™t-sá»‘-kÃ½-hiá»‡u">Má»™t sá»‘ kÃ½ hiá»‡u</h3>
<p>Giáº£ sá»­ \(\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N] \in \mathbb{R}^{d \times N}\) lÃ  ma tráº­n chá»©a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u mÃ  má»—i cá»™t \(\mathbf{x}_i \in \mathbb{R}^{d\times 1}\) lÃ  má»™t Ä‘iá»ƒm dá»¯ liá»‡u trong khÃ´ng gian \(d\) chiá»u. (<em>ChÃº Ã½: khÃ¡c vá»›i cÃ¡c bÃ i trÆ°á»›c tÃ´i thÆ°á»ng dÃ¹ng cÃ¡c vector hÃ ng Ä‘á»ƒ mÃ´ táº£ dá»¯ liá»‡u, trong bÃ i nÃ y tÃ´i dÃ¹ng vector cá»™t Ä‘á»ƒ biá»ƒu diá»…n. Viá»‡c biá»ƒu diá»…n dá»¯ liá»‡u á»Ÿ dáº¡ng hÃ ng hay cá»™t tÃ¹y thuá»™c vÃ o tá»«ng bÃ i toÃ¡n, miá»…n sao cÃ¡ch biá»…u diá»…n toÃ¡n há»c cá»§a nÃ³ khiáº¿n cho ngÆ°á»i Ä‘á»c tháº¥y dá»… hiá»ƒu</em>).</p>

<p>Giáº£ sá»­ thÃªm cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng vá»›i tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trong má»™t vector hÃ ng \(\mathbf{y} = [y_1, y_2, \dots, y_N] \in \mathbb{R}^{1\times N}\), vá»›i \(y_i = 1\) náº¿u \(\mathbf{x}_i\) thuá»™c class 1 (xanh) vÃ  \(y_i = -1\) náº¿u \(\mathbf{x}_i\) thuá»™c class 2 (Ä‘á»).</p>

<p>Táº¡i má»™t thá»i Ä‘iá»ƒm, giáº£ sá»­ ta tÃ¬m Ä‘Æ°á»£c boundary lÃ  Ä‘Æ°á»ng pháº³ng cÃ³ phÆ°Æ¡ng trÃ¬nh:
\[
\begin{eqnarray}
f_{\mathbf{w}}(\mathbf{x}) &amp;=&amp; w_1x_1 + \dots + w_dx_d + w_0 \newline 
&amp;=&amp;\mathbf{w}^T\mathbf{\bar{x}} = 0
\end{eqnarray}
\]</p>

<p>vá»›i \(\mathbf{\bar{x}}\) lÃ  Ä‘iá»ƒm dá»¯ liá»‡u má»Ÿ rá»™ng báº±ng cÃ¡ch thÃªm pháº§n tá»­ \(x_0 = 1\) lÃªn trÆ°á»›c vector \(\mathbf{x}\) tÆ°Æ¡ng tá»± nhÆ° trong <a href="/2016/12/28/linearregression/">Linear Regression</a>. VÃ  tá»« Ä‘Ã¢y, khi nÃ³i \(\mathbf{x}\), tÃ´i cÅ©ng ngáº§m hiá»ƒu lÃ  Ä‘iá»ƒm dá»¯ liá»‡u má»Ÿ rá»™ng.</p>

<p>Äá»ƒ cho Ä‘Æ¡n giáº£n, chÃºng ta hÃ£y cÃ¹ng lÃ m viá»‡c vá»›i trÆ°á»ng há»£p má»—i Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ sá»‘ chiá»u \(d = 2\). Giáº£ sá»­ Ä‘Æ°á»ng tháº³ng \(w_1 x_1 + w_2 x_2 + w_0 = 0\) chÃ­nh lÃ  nghiá»‡m cáº§n tÃ¬m nhÆ° HÃ¬nh 2 dÆ°á»›i Ä‘Ã¢y:</p>

<div class="imgcap">
<img src="\assets\pla\pla4.png" align="center" width="400" />
<div class="thecap">HÃ¬nh 2: PhÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng boundary.</div>
</div>

<p>Nháº­n xÃ©t ráº±ng cÃ¡c Ä‘iá»ƒm náº±m vá» cÃ¹ng 1 phÃ­a so vá»›i Ä‘Æ°á»ng tháº³ng nÃ y sáº½ lÃ m cho hÃ m sá»‘ \(f_{\mathbf{w}}(\mathbf{x})\) mang cÃ¹ng dáº¥u. Chá»‰ cáº§n Ä‘á»•i dáº¥u cá»§a \(\mathbf{w}\) náº¿u cáº§n thiáº¿t, ta cÃ³ thá»ƒ giáº£ sá»­ cÃ¡c Ä‘iá»ƒm náº±m trong ná»­a máº·t pháº³ng ná»n xanh mang dáº¥u dÆ°Æ¡ng (+), cÃ¡c Ä‘iá»ƒm náº±m trong ná»­a máº·t pháº³ng ná»n Ä‘á» mang dáº¥u Ã¢m (-). CÃ¡c dáº¥u nÃ y cÅ©ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i nhÃ£n \(y\) cá»§a má»—i class. Váº­y náº¿u \(\mathbf{w}\) lÃ  má»™t nghiá»‡m cá»§a bÃ i toÃ¡n Perceptron, vá»›i má»™t Ä‘iá»ƒm dá»¯ liá»‡u má»›i \(\mathbf{x}\) chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n, ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh class cá»§a nÃ³ báº±ng phÃ©p toÃ¡n Ä‘Æ¡n giáº£n nhÆ° sau:
\[
\text{label}(\mathbf{x}) = 1 ~\text{if}~ \mathbf{w}^T\mathbf{x} \geq 0, \text{otherwise} -1
\]</p>

<p>Ngáº¯n gá»n hÆ¡n: 
\[
\text{label}(\mathbf{x}) = \text{sgn}(\mathbf{w}^T\mathbf{x})
\]
trong Ä‘Ã³, \(\text{sgn}\) lÃ  hÃ m xÃ¡c Ä‘á»‹nh dáº¥u, vá»›i giáº£ sá»­ ráº±ng \(\text{sgn}(0) = 1\).</p>

<p><a name="xay-dung-ham-mat-mat"></a></p>

<h3 id="xÃ¢y-dá»±ng-hÃ m-máº¥t-mÃ¡t">XÃ¢y dá»±ng hÃ m máº¥t mÃ¡t</h3>
<p>Tiáº¿p theo, chÃºng ta cáº§n xÃ¢y dá»±ng hÃ m máº¥t mÃ¡t vá»›i tham sá»‘ \(\mathbf{w}\) báº¥t ká»³. Váº«n trong khÃ´ng gian hai chiá»u, giáº£ sá»­ Ä‘Æ°á»ng tháº³ng \(w_1x_1 + w_2x_2 + w_0 = 0\) Ä‘Æ°á»£c cho nhÆ° HÃ¬nh 3 dÆ°á»›i Ä‘Ã¢y:</p>
<div class="imgcap">
<img src="\assets\pla\pla3.png" align="center" width="400" />
<div class="thecap">HÃ¬nh 3: ÄÆ°á»ng tháº³ng báº¥t ká»³ vÃ  cÃ¡c Ä‘iá»ƒm bá»‹ misclassified Ä‘Æ°á»£c khoanh trÃ²n.</div>
</div>

<p>Trong trÆ°á»ng há»£p nÃ y, cÃ¡c Ä‘iá»ƒm Ä‘Æ°á»£c khoanh trÃ²n lÃ  cÃ¡c Ä‘iá»ƒm bá»‹ misclassified (phÃ¢n lá»›p lá»—i). Äiá»u chÃºng ta mong muá»‘n lÃ  khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o bá»‹ misclassified. HÃ m máº¥t mÃ¡t Ä‘Æ¡n giáº£n nháº¥t chÃºng ta nghÄ© Ä‘áº¿n lÃ  hÃ m <em>Ä‘áº¿m</em> sá»‘ lÆ°á»£ng cÃ¡c Ä‘iá»ƒm bá»‹ misclassied vÃ  tÃ¬m cÃ¡ch tá»‘i thiá»ƒu hÃ m sá»‘ nÃ y:
\[
J_1(\mathbf{w}) = \sum_{\mathbf{x}_i \in \mathcal{M}} (-y_i\text{sgn}(\mathbf{w}^T\mathbf{x_i}))
\]</p>

<p>trong Ä‘Ã³ \(\mathcal{M}\) lÃ  táº­p há»£p cÃ¡c Ä‘iá»ƒm bá»‹ misclassifed (<em>táº­p há»£p nÃ y thay Ä‘á»•i theo</em> \(\mathbf{w}\)). Vá»›i má»—i Ä‘iá»ƒm \(\mathbf{x}_i \in \mathcal{M}\), vÃ¬ Ä‘iá»ƒm nÃ y bá»‹ misclassified nÃªn \(y_i\) vÃ  \(\text{sgn}(\mathbf{w}^T\mathbf{x})\) khÃ¡c nhau, vÃ  vÃ¬ tháº¿ \(-y_i\text{sgn}(\mathbf{w}^T\mathbf{x_i}) = 1 \). Váº­y \(J_1(\mathbf{w})\) chÃ­nh lÃ  hÃ m <em>Ä‘áº¿m</em> sá»‘ lÆ°á»£ng cÃ¡c Ä‘iá»ƒm bá»‹ misclassified. Khi hÃ m sá»‘ nÃ y Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t báº±ng 0 thÃ¬ ta khÃ´ng cÃ²n Ä‘iá»ƒm nÃ o bá»‹ misclassified.</p>

<p>Má»™t Ä‘iá»ƒm quan trá»ng, hÃ m sá»‘ nÃ y lÃ  rá»i ráº¡c, khÃ´ng tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m theo \(\mathbf{w}\) nÃªn ráº¥t khÃ³ tá»‘i Æ°u. ChÃºng ta cáº§n tÃ¬m má»™t hÃ m máº¥t mÃ¡t khÃ¡c Ä‘á»ƒ viá»‡c tá»‘i Æ°u kháº£ thi hÆ¡n.</p>

<p>XÃ©t hÃ m máº¥t mÃ¡t sau Ä‘Ã¢y:</p>

<p>\[
J(\mathbf{w}) = \sum_{\mathbf{x}_i \in \mathcal{M}} (-y_i\mathbf{w}^T\mathbf{x_i})
\]</p>

<p>HÃ m \(J()\) khÃ¡c má»™t chÃºt vá»›i hÃ m \(J_1()\) á»Ÿ viá»‡c bá» Ä‘i hÃ m \(\text{sgn}\). Nháº­n xÃ©t ráº±ng khi má»™t Ä‘iá»ƒm misclassified \(\mathbf{x}_i\) náº±m cÃ ng xa boundary thÃ¬ giÃ¡ trá»‹ \(-y_i\mathbf{w}^T\mathbf{x}_i\) sáº½ cÃ ng lá»›n, nghÄ©a lÃ  sá»± sai lá»‡ch cÃ ng lá»›n. GiÃ¡ trá»‹ nhá» nháº¥t cá»§a hÃ m máº¥t mÃ¡t nÃ y cÅ©ng báº±ng 0 náº¿u khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o bá»‹ misclassifed. HÃ m máº¥t mÃ¡t nÃ y cÅ©ng Ä‘Æ°á»£c cho lÃ  tá»‘t hÆ¡n hÃ m \(J_1()\) vÃ¬ nÃ³ <em>trá»«ng pháº¡t</em> ráº¥t náº·ng nhá»¯ng Ä‘iá»ƒm <em>láº¥n sÃ¢u sang lÃ£nh thá»• cá»§a class kia</em>. Trong khi Ä‘Ã³, \(J_1()\) <em>trá»«ng pháº¡t</em> cÃ¡c Ä‘iá»ƒm misclassified nhÆ° nhau (Ä‘á»u = 1), báº¥t ká»ƒ chÃºng xa hay gáº§n vá»›i Ä‘Æ°á»ng biÃªn giá»›i.</p>

<p>Táº¡i má»™t thá»i Ä‘iá»ƒm, náº¿u chÃºng ta chá»‰ quan tÃ¢m tá»›i cÃ¡c Ä‘iá»ƒm bá»‹ misclassified thÃ¬ hÃ m sá»‘ \(J(\mathbf{w})\) kháº£ vi (tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m), váº­y chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng <a href="/2017/01/12/gradientdescent/">Gradient Descent</a> hoáº·c <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">Stochastic Gradient Descent (SGD)</a> Ä‘á»ƒ tá»‘i Æ°u hÃ m máº¥t mÃ¡t nÃ y. Vá»›i Æ°u Ä‘iá»ƒm cá»§a SGD cho cÃ¡c bÃ i toÃ¡n <a href="/2017/01/12/gradientdescent/#large-scale">large-scale</a>, chÃºng ta sáº½ lÃ m theo thuáº­t toÃ¡n nÃ y.</p>

<p>Vá»›i <em>má»™t</em> Ä‘iá»ƒm dá»¯ liá»‡u \(\mathbf{x}_i\) bá»‹ misclassified, hÃ m máº¥t mÃ¡t trá»Ÿ thÃ nh:</p>

<p>\[
J(\mathbf{w}; \mathbf{x}_i; y_i) = -y_i\mathbf{w}^T\mathbf{x}_i
\]</p>

<p>Äáº¡o hÃ m tÆ°Æ¡ng á»©ng:</p>

<p>\[
\nabla_{\mathbf{w}}J(\mathbf{w}; \mathbf{x}_i; y_i) = -y_i\mathbf{x}_i
\]</p>

<p>Váº­y quy táº¯c cáº­p nháº­t lÃ :</p>

<p>\[
\mathbf{w} = \mathbf{w} + \eta y_i\mathbf{x}_i
\]</p>

<p>vá»›i \(\eta\) lÃ  learning rate Ä‘Æ°á»£c chá»n báº±ng 1. Ta cÃ³ má»™t quy táº¯c cáº­p nháº­t ráº¥t gá»n lÃ : \(\mathbf{w}_{t+1} = \mathbf{w}_{t} + y_i\mathbf{x}_i\). NÃ³i cÃ¡ch khÃ¡c, vá»›i má»—i Ä‘iá»ƒm \(\mathbf{x}_i\) bá»‹ misclassifed, ta chá»‰ cáº§n nhÃ¢n Ä‘iá»ƒm Ä‘Ã³ vá»›i nhÃ£n \(y_i\) cá»§a nÃ³, láº¥y káº¿t quáº£ cá»™ng vÃ o \(\mathbf{w}\) ta sáº½ Ä‘Æ°á»£c \(\mathbf{w}\) má»›i.</p>

<p>Ta cÃ³ má»™t quan sÃ¡t nhá» á»Ÿ Ä‘Ã¢y:
\[
\mathbf{w}_{t+1}^T\mathbf{x}_i = (\mathbf{w}_{t} + y_i\mathbf{x}_i)^T\mathbf{x}_{i} \newline
= \mathbf{w}_{t}^T\mathbf{x}_i + y_i ||\mathbf{x}_i||_2^2
\]</p>

<p>Náº¿u \(y_i = 1\), vÃ¬ \(\mathbf{x}_i\) bá»‹ misclassifed nÃªn \(\mathbf{w}_{t}^T\mathbf{x}_i &lt; 0\). CÅ©ng vÃ¬ \(y_i = 1\) nÃªn \(y_i ||\mathbf{x}_i||_2^2 = ||\mathbf{x}_i||_2^2 \geq 1\) (chÃº Ã½ \(x_0 = 1\)), nghÄ©a lÃ  \(\mathbf{w}_{t+1}^T\mathbf{x}_i &gt; \mathbf{w}_{t}^T\mathbf{x}_i\). LÃ½ giáº£i báº±ng lá»i, \(\mathbf{w}_{t+1}\) tiáº¿n vá» phÃ­a lÃ m cho \(\mathbf{x}_i\) Ä‘Æ°á»£c phÃ¢n lá»›p Ä‘Ãºng. Äiá»u tÆ°Æ¡ng tá»± xáº£y ra náº¿u \(y_i = -1\).</p>

<p>Äáº¿n Ä‘Ã¢y, cáº£m nháº­n cá»§a chÃºng ta vá»›i thuáº­t toÃ¡n nÃ y lÃ : cá»© chá»n Ä‘Æ°á»ng boundary Ä‘i. XÃ©t tá»«ng Ä‘iá»ƒm má»™t, náº¿u Ä‘iá»ƒm Ä‘Ã³ bá»‹ misclassified thÃ¬ tiáº¿n Ä‘Æ°á»ng boundary vá» phÃ­a lÃ m cho Ä‘iá»ƒm Ä‘Ã³ Ä‘Æ°á»£c classifed Ä‘Ãºng. CÃ³ thá»ƒ tháº¥y ráº±ng, khi di chuyá»ƒn Ä‘Æ°á»ng boundary nÃ y, cÃ¡c Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c classified Ä‘Ãºng cÃ³ thá»ƒ láº¡i bá»‹ misclassified. Máº·c dÃ¹ váº­y, PLA váº«n Ä‘Æ°á»£c Ä‘áº£m báº£o sáº½ há»™i tá»¥ sau má»™t sá»‘ há»¯u háº¡n bÆ°á»›c (tÃ´i sáº½ chá»©ng minh viá»‡c nÃ y á»Ÿ phÃ­a sau cá»§a bÃ i viáº¿t). Tá»©c lÃ  cuá»‘i cÃ¹ng, ta sáº½ tÃ¬m Ä‘Æ°á»£c Ä‘Æ°á»ng pháº³ng phÃ¢n chia hai lá»›p, miá»…n lÃ  hai lá»›p Ä‘Ã³ lÃ  linearly separable. ÄÃ¢y cÅ©ng chÃ­nh lÃ  lÃ½ do cÃ¢u Ä‘áº§u tiÃªn trong bÃ i nÃ y tÃ´i nÃ³i vá»›i cÃ¡c báº¡n lÃ : â€œCá»© lÃ m Ä‘i, sai Ä‘Ã¢u sá»­a Ä‘áº¥y, cuá»‘i cÃ¹ng sáº½ thÃ nh cÃ´ng!â€.</p>

<p>TÃ³m láº¡i, thuáº­t toÃ¡n Perceptron cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° sau:</p>

<p><a name="tom-tat-pla"></a></p>

<h3 id="tÃ³m-táº¯t-pla">TÃ³m táº¯t PLA</h3>

<ol>
  <li>Chá»n ngáº«u nhiÃªn má»™t vector há»‡ sá»‘ \(\mathbf{w}\) vá»›i cÃ¡c pháº§n tá»­ gáº§n 0.</li>
  <li>Duyá»‡t ngáº«u nhiÃªn qua tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u \(\mathbf{x}_i\):
    <ul>
      <li>Náº¿u \(\mathbf{x}_i\) Ä‘Æ°á»£c phÃ¢n lá»›p Ä‘Ãºng, tá»©c \(\text{sgn}(\mathbf{w}^T\mathbf{x}_i) = y_i\), chÃºng ta khÃ´ng cáº§n lÃ m gÃ¬.</li>
      <li>Náº¿u \(\mathbf{x}_i\) bá»‹ misclassifed, cáº­p nháº­t \(\mathbf{w}\) theo cÃ´ng thá»©c:
 \[
 \mathbf{w} = \mathbf{w} + y_i\mathbf{x}_i
 \]</li>
    </ul>
  </li>
  <li>Kiá»ƒm tra xem cÃ³ bao nhiÃªu Ä‘iá»ƒm bá»‹ misclassifed. Náº¿u khÃ´ng cÃ²n Ä‘iá»ƒm nÃ o, dá»«ng thuáº­t toÃ¡n. Náº¿u cÃ²n, quay láº¡i bÆ°á»›c 2.</li>
</ol>

<p><a name="-vi-du-tren-python"></a></p>

<h2 id="3-vÃ­-dá»¥-trÃªn-python">3. VÃ­ dá»¥ trÃªn Python</h2>
<p>NhÆ° thÆ°á»ng lá»‡, chÃºng ta sáº½ thá»­ má»™t vÃ­ dá»¥ nhá» vá»›i Python.</p>

<p><a name="load-thu-vien-va-tao-du-lieu"></a></p>

<h3 id="load-thÆ°-viá»‡n-vÃ -táº¡o-dá»¯-liá»‡u">Load thÆ° viá»‡n vÃ  táº¡o dá»¯ liá»‡u</h3>
<p>ChÃºng ta sáº½ táº¡o hai nhÃ³m dá»¯ liá»‡u, má»—i nhÃ³m cÃ³ 10 Ä‘iá»ƒm, má»—i Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ hai chiá»u Ä‘á»ƒ thuáº­n tiá»‡n cho viá»‡c minh há»a. Sau Ä‘Ã³, táº¡o dá»¯ liá»‡u má»Ÿ rá»™ng báº±ng cÃ¡ch thÃªm 1 vÃ o Ä‘áº§u má»—i Ä‘iá»ƒm dá»¯ liá»‡u.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate data
# list of points 
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[.</span><span class="mi">3</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">],</span> <span class="p">[.</span><span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">]]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">T</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">T</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Xbar 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)),</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Sau khi thá»±c hiá»‡n Ä‘oáº¡n code nÃ y, biáº¿n <code class="language-plaintext highlighter-rouge">X</code> sáº½ chá»©a dá»¯ liá»‡u input (má»Ÿ rá»™ng), biáº¿n <code class="language-plaintext highlighter-rouge">y</code> sáº½ chá»©a nhÃ£n cá»§a má»—i Ä‘iá»ƒm dá»¯ liá»‡u trong <code class="language-plaintext highlighter-rouge">X</code>.
<a name="cac-ham-so-cho-pla"></a></p>

<h3 id="cÃ¡c-hÃ m-sá»‘-cho-pla">CÃ¡c hÃ m sá»‘ cho PLA</h3>
<p>Tiáº¿p theo chÃºng ta cáº§n viáº¿t 3 hÃ m sá»‘ cho PLA:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">h(w, x)</code>: tÃ­nh Ä‘áº§u ra khi biáº¿t Ä‘áº§u vÃ o <code class="language-plaintext highlighter-rouge">x</code> vÃ  weights <code class="language-plaintext highlighter-rouge">w</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">has_converged(X, y, w)</code>: kiá»ƒm tra xem thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥ chÆ°a. Ta chá»‰ cáº§n so sÃ¡nh <code class="language-plaintext highlighter-rouge">h(w, X)</code> vá»›i <em>ground truth</em> <code class="language-plaintext highlighter-rouge">y</code>. Náº¿u giá»‘ng nhau thÃ¬ dá»«ng thuáº­t toÃ¡n.</li>
  <li><code class="language-plaintext highlighter-rouge">perceptron(X, y, w_init)</code>: hÃ m chÃ­nh thá»±c hiá»‡n PLA.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">has_converged</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> 

<span class="k">def</span> <span class="nf">perceptron</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_init</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_init</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mis_points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># mix data 
</span>        <span class="n">mix_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">xi</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">mix_id</span><span class="p">[</span><span class="n">i</span><span class="p">]].</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mix_id</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">h</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xi</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">yi</span><span class="p">:</span> <span class="c1"># misclassified point
</span>                <span class="n">mis_points</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mix_id</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">w_new</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">yi</span><span class="o">*</span><span class="n">xi</span> 
                <span class="n">w</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span>
                
        <span class="k">if</span> <span class="n">has_converged</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mis_points</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_init</span><span class="p">)</span>
</code></pre></div></div>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  hÃ¬nh minh há»a thuáº­t toÃ¡n PLA cho bÃ i toÃ¡n nhá» nÃ y:</p>

<div class="imgcap">
<img src="\assets\pla\pla_vis.gif" align="center" width="400" />
<div class="thecap"> HÃ¬nh 4: Minh há»a thuáº­t toÃ¡n PLA </div>
</div>
<p>Sau khi cáº­p nháº­t 18 láº§n, PLA Ä‘Ã£ há»™i tá»¥. Äiá»ƒm Ä‘Æ°á»£c khoanh trÃ²n mÃ u Ä‘en lÃ  Ä‘iá»ƒm misclassified tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c chá»n Ä‘á»ƒ cáº­p nháº­t Ä‘Æ°á»ng boundary.</p>

<p>Source code cho pháº§n nÃ y (bao gá»“m hÃ¬nh Ä‘á»™ng) <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/pla/perceptron.py">cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y á»Ÿ Ä‘Ã¢y</a>.</p>

<p><a name="-chung-minh-hoi-tu"></a></p>

<h2 id="4-chá»©ng-minh-há»™i-tá»¥">4. Chá»©ng minh há»™i tá»¥</h2>

<p>Giáº£ sá»­ ráº±ng \(\mathbf{w}^*\) lÃ  má»™t nghiá»‡m cá»§a bÃ i toÃ¡n (ta cÃ³ thá»ƒ giáº£ sá»­ viá»‡c nÃ y Ä‘Æ°á»£c vÃ¬ chÃºng ta Ä‘Ã£ cÃ³ giáº£ thiáº¿t hai class lÃ  linearly separable - tá»©c tá»“n táº¡i nghiá»‡m). CÃ³ thá»ƒ tháº¥y ráº±ng, vá»›i má»i \(\alpha &gt; 0\), náº¿u \(\mathbf{w}^*\) lÃ  nghiá»‡m, \(\alpha\mathbf{w}^*\) cÅ©ng lÃ  nghiá»‡m cá»§a bÃ i toÃ¡n. XÃ©t dÃ£y sá»‘ khÃ´ng Ã¢m \( u_{\alpha}(t) = ||\mathbf{w}_{t} - \alpha\mathbf{w}^*||_2^2\). Vá»›i \(\mathbf{x}_i\) lÃ  má»™t Ä‘iá»ƒm bá»‹ misclassified náº¿u dÃ¹ng nghiá»‡m \(\mathbf{w}_t\) ta cÃ³:</p>

<p>\[
\begin{eqnarray}
&amp;&amp;u_{\alpha}(t+1) = ||\mathbf{w}_{t+1} - \alpha \mathbf{w}^*||_2^2 \newline
&amp;=&amp; ||\mathbf{w}_{t} + y_i\mathbf{x}_i - \alpha\mathbf{w}^*||_2^2 \newline
&amp;=&amp; ||\mathbf{w}_{t} -\alpha\mathbf{w}^*||_2^2 + y_i^2||\mathbf{x}_i||_2^2 + 2y_i\mathbf{x}_i^T(\mathbf{w} - \alpha\mathbf{w}^*) \newline
&amp;&lt;&amp; u_{\alpha}(t) \ + ||\mathbf{x}_i||_2^2 - 2\alpha y_i\mathbf{x}_i^T \mathbf{w}^*
\end{eqnarray}
\]</p>

<p>Dáº¥u nhá» hÆ¡n á»Ÿ dÃ²ng cuá»‘i lÃ  vÃ¬ \(y_i^2 = 1\) vÃ  \(2y_i\mathbf{x}_i^T\mathbf{w}_{t} &lt; 0\). Náº¿u ta Ä‘áº·t:</p>

<p>\[
\begin{eqnarray}
\beta^2 &amp;=&amp; \max_{i=1, 2, \dots, N}||\mathbf{x}_i||_2^2 \newline
\gamma &amp;=&amp; \min_{i=1, 2, \dots, N} y_i\mathbf{x}_i^T\mathbf{w}^*
\end{eqnarray}
\]</p>

<p>vÃ  chá»n \(\alpha = \frac{\beta^2}{\gamma}\), ta cÃ³:
\[
0 \leq u_{\alpha}(t+1) &lt; u_{\alpha}(t) + \beta^2 - 2\alpha\gamma = u_{\alpha}(t) - \beta^2
\]</p>

<p>Äiá»u nÃ y nghÄ©a lÃ : náº¿u luÃ´n luÃ´n cÃ³ cÃ¡c Ä‘iá»ƒm bá»‹ misclassified thÃ¬ dÃ£y \(u_{\alpha}(t)\) lÃ  dÃ£y giáº£m, bá»‹ cháº·n dÆ°á»›i bá»Ÿi 0, vÃ  pháº§n tá»­ sau kÃ©m pháº§n tá»­ trÆ°á»›c Ã­t nháº¥t má»™t lÆ°á»£ng lÃ  \(\beta^2&gt;0\). Äiá»u vÃ´ lÃ½ nÃ y chá»©ng tá» Ä‘áº¿n má»™t lÃºc nÃ o Ä‘Ã³ sáº½ khÃ´ng cÃ²n Ä‘iá»ƒm nÃ o bá»‹ misclassified. NÃ³i cÃ¡ch khÃ¡c, thuáº­t toÃ¡n PLA há»™i tá»¥ sau má»™t sá»‘ há»¯u háº¡n bÆ°á»›c.</p>

<p><a name="-mo-hinh-neural-network-dau-tien"></a></p>

<h2 id="5-mÃ´-hÃ¬nh-neural-network-Ä‘áº§u-tiÃªn">5. MÃ´ hÃ¬nh Neural Network Ä‘áº§u tiÃªn</h2>
<p>HÃ m sá»‘ xÃ¡c Ä‘á»‹nh class cá»§a Perceptron \(\text{label}(\mathbf{x}) = \text{sgn}(\mathbf{w}^T\mathbf{x})\) cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° hÃ¬nh váº½ (Ä‘Æ°á»£c gá»i lÃ  network) dÆ°á»›i Ä‘Ã¢y:</p>

<div class="imgcap">
<img src="\assets\pla\pla_nn.png" align="center" width="800" />
<div class="thecap"> HÃ¬nh 5: Biá»ƒu diá»…n cá»§a Perceptron dÆ°á»›i dáº¡ng Neural Network.</div>
</div>

<p>Äáº§u vÃ o cá»§a network \(\mathbf{x}\) Ä‘Æ°á»£c minh há»a báº±ng cÃ¡c node mÃ u xanh lá»¥c vá»›i node \(x_0\) luÃ´n luÃ´n báº±ng 1. Táº­p há»£p cÃ¡c node mÃ u xanh lá»¥c Ä‘Æ°á»£c gá»i lÃ  <em>Input layer</em>. Trong vÃ­ dá»¥ nÃ y, tÃ´i giáº£ sá»­ sá»‘ chiá»u cá»§a dá»¯ liá»‡u \(d = 4\). Sá»‘ node trong input layer luÃ´n luÃ´n lÃ  \(d + 1\) vá»›i má»™t node lÃ  1 Ä‘Æ°á»£c thÃªm vÃ o. Node \(x_0 = 1\) nÃ y Ä‘Ã´i khi Ä‘Æ°á»£c áº©n Ä‘i.</p>

<p>CÃ¡c trá»ng sá»‘ (<em>weights</em>) \(w_0, w_1, \dots, w_d\) Ä‘Æ°á»£c gÃ¡n vÃ o cÃ¡c mÅ©i tÃªn Ä‘i tá»›i node \(\displaystyle z = \sum_{i=0}^dw_ix_i = \mathbf{w}^T\mathbf{x}\). Node \(y = \text{sgn}(z)\) lÃ  <em>output</em> cá»§a network. KÃ½ hiá»‡u hÃ¬nh chá»¯ Z ngÆ°á»£c mÃ u xanh trong node \(y\) thá»ƒ hiá»‡n Ä‘á»“ thá»‹ cá»§a hÃ m sá»‘ \(\text{sgn}\).</p>

<p>Trong thuáº­t toÃ¡n PLA, ta pháº£i tÃ¬m cÃ¡c weights trÃªn cÃ¡c mÅ©i tÃªn sao cho vá»›i má»—i \(\mathbf{x}_i\) á»Ÿ táº­p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã£ biáº¿t Ä‘Æ°á»£c Ä‘áº·t á»Ÿ Input layer, output cá»§a network nÃ y trÃ¹ng vá»›i nhÃ£n \(y_i\) tÆ°Æ¡ng á»©ng.</p>

<p>HÃ m sá»‘ \(y = \text{sgn}(z)\) cÃ²n Ä‘Æ°á»£c gá»i lÃ  <em>activation function</em>. ÄÃ¢y chÃ­nh lÃ  dáº¡ng Ä‘Æ¡n giáº£n nháº¥t cá»§a Neural Network.</p>

<p>CÃ¡c Neural Networks sau nÃ y cÃ³ thá»ƒ cÃ³ nhiá»u node á»Ÿ output táº¡o thÃ nh má»™t <em>output layer</em>, hoáº·c cÃ³ thá»ƒ cÃ³ thÃªm cÃ¡c layer trung gian giá»¯a <em>input layer</em> vÃ  <em>output layer</em>. CÃ¡c layer trung gian Ä‘Ã³ Ä‘Æ°á»£c gá»i lÃ  <em>hidden layer</em>. Khi biá»ƒu diá»…n cÃ¡c Networks lá»›n, ngÆ°á»i ta thÆ°á»ng giáº£n lÆ°á»£c hÃ¬nh bÃªn trÃ¡i thÃ nh hÃ¬nh bÃªn pháº£i. Trong Ä‘Ã³ node \(x_0 = 1\) thÆ°á»ng Ä‘Æ°á»£c áº©n Ä‘i. Node \(z\) cÅ©ng Ä‘Æ°á»£c áº©n Ä‘i vÃ  viáº¿t gá»™p vÃ o trong node \(y\). Perceptron thÆ°á»ng Ä‘Æ°á»£c váº½ dÆ°á»›i dáº¡ng Ä‘Æ¡n giáº£n nhÆ° HÃ¬nh 5 bÃªn pháº£i.</p>

<p>Äá»ƒ Ã½ ráº±ng náº¿u ta thay <em>activation function</em> bá»Ÿi \(y = z\), ta sáº½ cÃ³ Neural Network mÃ´ táº£ thuáº­t toÃ¡n Linear Regression nhÆ° hÃ¬nh dÆ°á»›i. Vá»›i Ä‘Æ°á»ng tháº³ng chÃ©o mÃ u xanh thá»ƒ hiá»‡n Ä‘á»“ thá»‹ hÃ m sá»‘ \(y = z\). CÃ¡c trá»¥c tá»a Ä‘á»™ Ä‘Ã£ Ä‘Æ°á»£c lÆ°á»£c bá».</p>

<div class="imgcap">
<img src="\assets\pla\lr_nn.png" align="center" width="300" />
<div class="thecap"> HÃ¬nh 6: Biá»ƒu diá»…n cá»§a Linear Regression dÆ°á»›i dáº¡ng Neural Network.</div>
</div>

<p>MÃ´ hÃ¬nh perceptron á»Ÿ trÃªn khÃ¡ giá»‘ng vá»›i má»™t node nhá» cá»§a dÃ¢y thÃ¢n kinh sinh há»c nhÆ° hÃ¬nh sau Ä‘Ã¢y:</p>

<div class="imgcap">
<img src="http://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_neuron.png" align="center" width="600" />
<div class="thecap">HÃ¬nh 7: MÃ´ táº£ má»™t neuron tháº§n kinh sinh há»c. (Nguá»“n: <a href="http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html">Single-Layer Neural Networks and Gradient Descent</a>)</div>
</div>

<p>Dá»¯ liá»‡u tá»« nhiá»u dÃ¢y tháº§n kinh Ä‘i vá» má»™t <em>cell nucleus</em>. ThÃ´ng tin Ä‘Æ°á»£c tá»•ng há»£p vÃ  Ä‘Æ°á»£c Ä‘Æ°a ra á»Ÿ output. Nhiá»u bá»™ pháº­n nhÆ° tháº¿ nÃ y káº¿t há»£p vá»›i nhau táº¡o nÃªn há»‡ tháº§n kinh sinh há»c. ChÃ­nh vÃ¬ váº­y mÃ  cÃ³ tÃªn Neural Networks trong Machine Learning. ÄÃ´i khi máº¡ng nÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  Artificial Neural Networks (ANN) tá»©c <em>há»‡ neuron nhÃ¢n táº¡o</em>.</p>

<p><a name="-thao-luan"></a></p>

<h2 id="6-tháº£o-luáº­n">6. Tháº£o Luáº­n</h2>
<p><a name="pla-co-the-cho-vo-so-nghiem-khac-nhau"></a></p>

<h3 id="pla-cÃ³-thá»ƒ-cho-vÃ´-sá»‘-nghiá»‡m-khÃ¡c-nhau">PLA cÃ³ thá»ƒ cho vÃ´ sá»‘ nghiá»‡m khÃ¡c nhau</h3>
<p>RÃµ rÃ ng ráº±ng, náº¿u hai class lÃ  linearly separable thÃ¬ cÃ³ vÃ´ sá»‘ Ä‘Æ°á»ng tháº±ng phÃ¢n cÃ¡ch 2 class Ä‘Ã³. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥:</p>

<div class="imgcap">
<img src="/assets/pla/pla6.png" align="center" width="400" />
<div class="thecap">HÃ¬nh 8: PLA cÃ³ thá»ƒ cho vÃ´ sá»‘ nghiá»‡m khÃ¡c nhau.</div>
</div>

<p>Táº¥t cáº£ cÃ¡c Ä‘Æ°á»ng tháº³ng mÃ u Ä‘en Ä‘á»u thá»a mÃ£n. Tuy nhiÃªn, cÃ¡c Ä‘Æ°á»ng khÃ¡c nhau sáº½ quyáº¿t Ä‘á»‹nh Ä‘iá»ƒm hÃ¬nh tam giÃ¡c thuá»™c cÃ¡c lá»›p khÃ¡c nhau. Trong cÃ¡c Ä‘Æ°á»ng Ä‘Ã³, Ä‘Æ°á»ng nÃ o lÃ  tá»‘t nháº¥t? VÃ  Ä‘á»‹nh nghÄ©a â€œtá»‘t nháº¥tâ€ Ä‘Æ°á»£c hiá»ƒu theo nghÄ©a nÃ o? CÃ³ má»™t thuáº­t toÃ¡n khÃ¡c Ä‘á»‹nh nghÄ©a vÃ  tÃ¬m Ä‘Æ°á»ng tá»‘t nháº¥t nhÆ° tháº¿, tÃ´i sáº½ giá»›i thiá»‡u trong 1 vÃ i bÃ i tá»›i. Má»i cÃ¡c báº¡n Ä‘Ã³n Ä‘á»c.</p>

<p><a name="pla-doi-hoi-du-lieu-linearly-separable"></a></p>

<h3 id="pla-Ä‘Ã²i-há»i-dá»¯-liá»‡u-linearly-separable">PLA Ä‘Ã²i há»i dá»¯ liá»‡u linearly separable</h3>

<p>Hai class trong vÃ­ dá»¥ dÆ°á»›i Ä‘Ã¢y <em>tÆ°Æ¡ng Ä‘á»‘i</em> linearly separable. Má»—i class cÃ³ 1 Ä‘iá»ƒm coi nhÆ° <em>nhiá»…u</em> náº±m trong khu vá»±c cÃ¡c Ä‘iá»ƒm cá»§a class kia. PLA sáº½ khÃ´ng lÃ m viá»‡c trong trÆ°á»ng há»£p nÃ y vÃ¬ luÃ´n luÃ´n cÃ³ Ã­t nháº¥t 2 Ä‘iá»ƒm bá»‹ misclassified.</p>

<div class="imgcap">
<img src="/assets/pla/pla7.png" align="center" width="400" />
<div class="thecap">Hinhf 9: PLA khÃ´ng lÃ m viá»‡c náº¿u chá»‰ cÃ³ má»™t nhiá»…u nhá».</div>
</div>

<p>Trong má»™t chá»«ng má»±c nÃ o Ä‘Ã³, Ä‘Æ°á»ng tháº³ng mÃ u Ä‘en váº«n cÃ³ thá»ƒ coi lÃ  má»™t nghiá»‡m tá»‘t vÃ¬ nÃ³ Ä‘Ã£ giÃºp phÃ¢n loáº¡i chÃ­nh xÃ¡c háº§u háº¿t cÃ¡c Ä‘iá»ƒm. Viá»‡c khÃ´ng há»™i tá»¥ vá»›i dá»¯ liá»‡u <em>gáº§n</em> linearly separable chÃ­nh lÃ  má»™t nhÆ°á»£c Ä‘iá»ƒm lá»›n cá»§a PLA.</p>

<p>Äá»ƒ kháº¯c phá»¥c nhÆ°á»£c Ä‘iá»ƒm nÃ y, cÃ³ má»™t cáº£i tiáº¿n nhá» nhÆ° thuáº­t toÃ¡n Pocket Algorithm dÆ°á»›i Ä‘Ã¢y:
<a name="pocket-algorithm"></a></p>

<h3 id="pocket-algorithm">Pocket Algorithm</h3>
<p>Má»™t cÃ¡ch tá»± nhiÃªn, náº¿u cÃ³ má»™t vÃ i <em>nhiá»…u</em>, ta sáº½ Ä‘i tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng phÃ¢n chia hai class sao cho cÃ³ Ã­t Ä‘iá»ƒm bá»‹ misclassified nháº¥t. Viá»‡c nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua PLA vá»›i má»™t chÃºt thay Ä‘á»•i nhá» nhÆ° sau:</p>

<ol>
  <li>Giá»›i háº¡n sá»‘ lÆ°á»£ng vÃ²ng láº·p cá»§a PLA.</li>
  <li>Má»—i láº§n cáº­p nháº­t nghiá»‡m \(\mathbf{w}\) má»›i, ta Ä‘áº¿m xem cÃ³ bao nhiÃªu Ä‘iá»ƒm bá»‹ misclassified. Náº¿u lÃ  láº§n Ä‘áº§u tiÃªn, giá»¯ láº¡i nghiá»‡m nÃ y trong <em>pocket</em> (tÃºi quáº§n). Náº¿u khÃ´ng, so sÃ¡nh sá»‘ Ä‘iá»ƒm misclassified nÃ y vá»›i sá»‘ Ä‘iá»ƒm misclassified cá»§a nghiá»‡m trong <em>pocket</em>, náº¿u nhá» hÆ¡n thÃ¬ <em>lÃ´i</em> nghiá»‡m cÅ© ra, Ä‘áº·t nghiá»‡m má»›i nÃ y vÃ o.</li>
</ol>

<p>Thuáº­t toÃ¡n nÃ y giá»‘ng vá»›i thuáº­t toÃ¡n tÃ¬m pháº§n tá»­ nhá» nháº¥t trong 1 máº£ng.</p>

<p><a name="-ket-luan"></a></p>

<h2 id="7-káº¿t-luáº­n">7. Káº¿t luáº­n</h2>

<p>Hy vá»ng ráº±ng bÃ i viáº¿t nÃ y sáº½ giÃºp cÃ¡c báº¡n pháº§n nÃ o hiá»ƒu Ä‘Æ°á»£c má»™t sá»‘ khÃ¡i niá»‡m trong Neural Networks. Trong má»™t sá»‘ bÃ i tiáº¿p theo, tÃ´i sáº½ tiáº¿p tá»¥c nÃ³i vá» cÃ¡c thuáº­t toÃ¡n cÆ¡ báº£n khÃ¡c trong Neural Networks trÆ°á»›c khi chuyá»ƒn sang pháº§n khÃ¡c.</p>

<p>Trong tÆ°Æ¡ng lai, náº¿u cÃ³ thá»ƒ, tÃ´i sáº½ viáº¿t tiáº¿p vá» Deep Learning vÃ  chÃºng ta sáº½ láº¡i quay láº¡i vá»›i Neural Networks.</p>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="8-tÃ i-liá»‡u-tham-kháº£o">8. TÃ i liá»‡u tham kháº£o</h2>

<p>[1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.</p>

<p>[2] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115â€“133, 1943.</p>

<p>[3] B. Widrow et al. Adaptive â€Adalineâ€ neuron using chemical â€memistorsâ€. Number Technical Report 1553-2. Stanford Electron. Labs., Stanford, CA, October 1960.</p>

<p>[3] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. (<a href="http://work.caltech.edu/telecourse.html">link to course</a>)</p>

<p>[4] Bishop, Christopher M. â€œPattern recognition and Machine Learning.â€, Springer  (2006). (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[5] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley &amp; Sons, 2012.</p>

:ET