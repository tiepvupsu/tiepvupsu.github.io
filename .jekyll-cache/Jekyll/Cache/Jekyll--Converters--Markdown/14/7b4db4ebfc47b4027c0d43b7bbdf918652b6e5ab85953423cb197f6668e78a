I"°<p><em>BÃ i viáº¿t nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i sá»± Ä‘Ã³ng gÃ³p cá»§a Nguyá»…n Tiáº¿n CÆ°á»ng, Cao Thanh HÃ , Äinh Duy KhÃ¡nh, vÃ  Nguyá»…n VÄƒn TÃ i. CÃ¡c báº¡n nÃ y cÅ©ng sáº½ giÃºp tÃ´i trong cÃ¡c bÃ i vá» deep learning tiáº¿p theo.</em></p>

<p>Trong trang nÃ y: 
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Giá»›i thiá»‡u</a></li>
  <li><a href="#-linear-regression-va-logistic-regression-voi-keras">2. Linear regression vÃ  logistic regression vá»›i Keras</a>
    <ul>
      <li><a href="#-keras-voi-linear-regression">2.1. Keras vá»›i linear regression</a></li>
      <li><a href="#-keras-voi-logistic-regression">2.1. Keras vá»›i logistic regression</a></li>
    </ul>
  </li>
  <li><a href="#-keras-cho-multi-layer-perceptron">3. Keras cho multi-layer perceptron</a>
    <ul>
      <li><a href="#-gioi-thieu-ve-fashion-mnist">3.1. Giá»›i thiá»‡u vá» Fashion-MNIST</a></li>
      <li><a href="#-xay-dung-mot-multi-layer-perceptron-de-giai-quyet-bai-toan">3.2. XÃ¢y dá»±ng má»™t multi-layer perceptron Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n</a></li>
      <li><a href="#-nhan-xet">3.3. Nháº­n xÃ©t</a></li>
    </ul>
  </li>
  <li><a href="#-ket-luan">4. Káº¿t luáº­n</a></li>
  <li><a href="#-tai-lieu-tham-khao">5. TÃ i liá»‡u tham kháº£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-giá»›i-thiá»‡u">1. Giá»›i thiá»‡u</h2>

<p>Ká»ƒ tá»« 2012 khi <a href="/2018/06/22/deeplearning/#dot-pha-">deep learning cÃ³ bÆ°á»›c Ä‘á»™t phÃ¡ lá»›n</a>, hÃ ng loáº¡t cÃ¡c thÆ° viá»‡n há»— trá»£ deep learning ra Ä‘á»i. CÃ¹ng vá»›i Ä‘Ã³, ngÃ y cÃ ng nhiá»u kiáº¿n trÃºc deep learning ra Ä‘á»i, khiáº¿n cho sá»‘ lÆ°á»£ng á»©ng dá»¥ng vÃ  cÃ¡c bÃ i bÃ¡o liÃªn quan tá»›i deep learning tÄƒng lÃªn chÃ³ng máº·t.</p>

<p>CÃ¡c thÆ° viá»‡n deep learning thÆ°á»ng Ä‘Æ°á»£c â€˜chá»‘ng lÆ°ngâ€™ bá»Ÿi nhá»¯ng hÃ£ng cÃ´ng nghá»‡ lá»›n: Google (Keras, TensorFlow), Facebook (Caffe2, Pytorch), Microsoft (CNTK), Amazon (Mxnet), Microsoft vÃ  Amazon cÅ©ng Ä‘ang báº¯t tay xÃ¢y dá»±ng Gluon (phiÃªn báº£n tÆ°Æ¡ng tá»± nhÆ° Keras). (CÃ¡c hÃ£ng nÃ y Ä‘á»u cÃ³ cÃ¡c dá»‹ch vá»¥ cloud computing vÃ  muá»‘n thu hÃºt ngÆ°á»i dÃ¹ng).</p>

<hr />

<div class="imgcap">
<div>
    <img src="/assets/36_keras/dlp0.png" width="600" />
</div>
<div class="thecap">CÃ¡c thÆ° viá»‡n deep learning vÃ  cÃ¡c hÃ£ng cÃ´ng nghá»‡ lá»›n. <br /> (Nguá»“n: <a href="https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750">Battle of the Deep Learning frameworksâ€Šâ€”â€ŠPart I: 2017, even more frameworks and interfaces</a>)</div>
</div>
<hr />

<p>Váº­y thÆ° viá»‡n nÃ o lÃ  tá»‘t nháº¥t? CÃ¢u tráº£ lá»i tuá»³ thuá»™c vÃ o viá»‡c báº¡n quen vá»›i há»‡ Ä‘iá»u hÃ nh, ngÃ´n ngá»¯ láº­p trÃ¬nh nÃ o, báº¡n sá»­ dá»¥ng deep learning vÃ o má»¥c Ä‘Ã­ch nghiÃªn cá»©u hay ra sáº£n pháº©m, báº¡n sá»­ dá»¥ng trÃªn ná»n táº£ng pháº§n cá»©ng nÃ o, v.v.</p>

<p>NhÃ¬n chung, má»™t thÆ° viá»‡n deep learning tá»‘t cáº§n cÃ³ cÃ¡c Ä‘áº·c Ä‘iá»ƒm sau:</p>

<ol>
  <li>
    <p>Há»— trá»£ tÃ­nh toÃ¡n vá»›i GPU vÃ  cÃ¡c há»‡ thá»‘ng phÃ¢n tÃ¡n. Äiá»u nÃ y lÃ  tá»‘i quan trá»ng vÃ¬ viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh deep learning yÃªu cáº§u kháº£ nÄƒng tÃ­nh toÃ¡n ráº¥t máº¡nh.</p>
  </li>
  <li>
    <p>Há»— trá»£ cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n: C/C++, Python, Java, R, â€¦</p>
  </li>
  <li>
    <p>CÃ³ thá»ƒ cháº¡y Ä‘Æ°á»£c trÃªn nhiá»u há»‡ Ä‘iá»u hÃ nh.</p>
  </li>
  <li>
    <p>Thá»i gian tá»« Ã½ tÆ°á»Ÿng tá»›i xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh ngáº¯n.</p>
  </li>
  <li>
    <p>CÃ³ thá»ƒ cháº¡y trÃªn trÃ¬nh duyá»‡t vÃ  cÃ¡c thiáº¿t bá»‹ di Ä‘á»™ng.</p>
  </li>
  <li>
    <p>CÃ³ kháº£ nÄƒng giÃºp ngÆ°á»i láº­p trÃ¬nh can thiá»‡p sÃ¢u vÃ o mÃ´ hÃ¬nh vÃ  táº¡o ra cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p.</p>
  </li>
  <li>
    <p>Chá»©a nhiá»u <code class="language-plaintext highlighter-rouge">model zoo</code>, tá»©c cÃ¡c mÃ´ hÃ¬nh deep learning thÃ´ng dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n.</p>
  </li>
  <li>
    <p>Há»— trá»£ tÃ­nh toÃ¡n backpropagation tá»± Ä‘á»™ng.</p>
  </li>
  <li>
    <p>CÃ³ cá»™ng Ä‘á»“ng há»i Ä‘Ã¡p lá»›n.</p>
  </li>
</ol>

<p>Tháº­t khÃ³ cÃ³ thá»ƒ chá»‰ ra má»™t thÆ° viá»‡n Ä‘Ã¡p á»©ng tá»‘t táº¥t cáº£ cÃ¡c yÃªu cáº§u phÃ­a trÃªn. CÃ¡c báº¡n cÃ³ thá»ƒ xem cÃ¡c bÃ i so sÃ¡nh cÃ¡c thÆ° viá»‡n nÃ y á»Ÿ pháº§n <a href="#-tai-lieu-tham-khao">TÃ i liá»‡u tham kháº£o</a>. TÃ´i chá»‰ xin giá»›i thiá»‡u má»™t vÃ i thá»‘ng kÃª giÃºp cÃ¡c báº¡n cÃ³ cÃ¡i nhÃ¬n nhanh nháº¥t vá» thÆ° viá»‡n nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u nháº¥t.</p>

<hr />

<div class="imgcap">
<div>
    <img src="/assets/36_keras/dlp1.png" width="600" />
</div>
<div class="thecap">Sá»‘ lÆ°á»£ng 'stars' trÃªn GitHub repo. <br /> (Xem thÃªm: <a href="https://blog.paperspace.com/which-ml-framework-should-i-use/">Machine Learning Frameworks Comparison</a>)</div>
</div>
<ol>
  <li>TensorFlow, 2. Caffe (nÄƒm 2015-2016).</li>
</ol>
<hr />

<hr />

<div class="imgcap">
<div>
    <img src="/assets/36_keras/dlp2.jpg" width="600" />
</div>
<div class="thecap">Sá»‘ lÆ°á»£ng 'stars' trÃªn GitHub repo, sá»‘ lÆ°á»£ng 'contributors', vÃ  'tuá»•i' cá»§a thÆ° viá»‡n. <br /> (Xem thÃªm: <a href="https://blog.paperspace.com/which-ml-framework-should-i-use/">Top 16 Open Source Deep Learning Libraries and Platforms</a>)</div>
</div>
<ol>
  <li>TensorFlow, 2. Keras, 3. Caffe</li>
</ol>
<hr />

<hr />

<div class="imgcap">
<div>
    <img src="/assets/36_keras/dlp3.png" width="600" />
</div>
<div class="thecap">Sá»‘ lÆ°á»£ng cÃ¡c bÃ i bÃ¡o trÃªn arxiv cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»—i thÆ° viá»‡n. <br /> (Xem thÃªm: <a href="https://keras.io/why-use-keras/">Why use Keras?</a>)</div>
</div>
<ol>
  <li>TensorFlow, 2. Keras, 3. Caffe</li>
</ol>
<hr />

<p>Nhá»¯ng so sÃ¡nh trÃªn Ä‘Ã¢y chá»‰ ra ráº±ng TensorFlow, Keras vÃ  Caffe lÃ  cÃ¡c thÆ° viá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u nháº¥t (gáº§n Ä‘Ã¢y cÃ³ thÃªm PyTorch ráº¥t dá»… sá»­ dá»¥ng vÃ  Ä‘ang thu hÃºt thÃªm nhiá»u ngÆ°á»i dÃ¹ng).</p>

<p>Keras Ä‘Æ°á»£c coi lÃ  má»™t thÆ° viá»‡n â€˜high-levelâ€™ vá»›i pháº§n â€˜low-levelâ€™ (cÃ²n Ä‘Æ°á»£c gá»i lÃ  <em>backend</em>) cÃ³ thá»ƒ lÃ  TensorFlow, CNTK, hoáº·c Theano (<a href="https://syncedreview.com/2017/09/29/rip-theano/">sáº¯p tá»›i Theano sáº½ khÃ´ng Ä‘Æ°á»£c duy trÃ¬ nÃ¢ng cáº¥p ná»¯a</a>). Keras cÃ³ cÃº phÃ¡p Ä‘Æ¡n giáº£n hÆ¡n TensorFlow ráº¥t nhiá»u. Vá»›i má»¥c Ä‘Ã­ch giá»›i thiá»‡u vá» cÃ¡c mÃ´ hÃ¬nh nhiá»u hÆ¡n lÃ  cÃ¡c sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n deep learning, tÃ´i sáº½ chá»n Keras vá»›i TensorFlow lÃ  â€˜backendâ€™.</p>

<p>(Báº£n thÃ¢n tÃ´i khi lÃ m nghiÃªn cá»©u thÆ°á»ng dÃ¹ng TensorFlow vÃ  Pytorch.)</p>

<p>CÃ¡c báº¡n cÃ³ thá»ƒ Ä‘á»c thÃªm bÃ i <a href="https://keras.io/why-use-keras/">Why use Keras?</a> trÃªn trang chá»§ cá»§a Keras (<em>Táº¥t nhiÃªn trÃªn trang chá»§ cá»§a thÆ° viá»‡n nÃ o cÅ©ng sáº½ cÃ³ má»™t bÃ i tÆ°Æ¡ng tá»± kiá»ƒu â€˜Why use â€¦?â€™</em>). TÃ´i xin nÃªu láº¡i má»™t vÃ i gáº¡ch Ä‘áº§u dÃ²ng:</p>

<ul>
  <li>
    <p>Keras Æ°u tiÃªn  tráº£i nghiá»‡m cá»§a ngÆ°á»i láº­p trÃ¬nh</p>
  </li>
  <li>
    <p>Keras Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong doanh nghiá»‡p vÃ  cá»™ng Ä‘á»“ng nghiÃªn cá»©u</p>
  </li>
  <li>
    <p>Keras giÃºp dá»… dÃ ng biáº¿n cÃ¡c thiáº¿t káº¿ thÃ nh sáº£n pháº©m</p>
  </li>
  <li>
    <p>Keras há»— trá»£ huáº¥n luyá»‡n trÃªn nhiá»u GPU phÃ¢n tÃ¡n</p>
  </li>
  <li>
    <p>Keras há»— trá»£ Ä‘a backend engines vÃ  khÃ´ng giá»›i háº¡n báº¡n vÃ o má»™t há»‡ sinh thÃ¡i</p>
  </li>
</ul>

<p>Amazon hiá»‡n cÅ©ng Ä‘ang lÃ m viá»‡c Ä‘á»ƒ phÃ¡t triá»ƒn MXNet backend cho Keras.
MÃ´ hÃ¬nh Keras cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t sá»‘ ná»n táº£ng pháº§n cá»©ng khÃ¡c nhau ngoÃ i CPU:</p>

<ul>
  <li>NVIDIA GPU</li>
  <li>Google TPUs, thÃ´ng qua TensorFlow backend vÃ  Google Cloud</li>
  <li>CÃ¡c OpenCL GPU, cháº³ng háº¡n nhÆ° cÃ¡c sáº£m pháº§m tá»« AMD, thÃ´ng qua PlaidML Keras backend.</li>
</ul>

<p>Hy vá»ng chá»«ng Ä‘Ã³ Ä‘Ã£ Ä‘á»§ Ä‘á»ƒ chÃºng ta cÃ¹ng báº¯t Ä‘áº§u vá»›i Keras. CÃ¡ch cÃ i Ä‘áº·t Keras cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trÃªn <a href="https://keras.io/">trang chá»§ cá»§a nÃ³</a>.</p>

<p>Tiáº¿p theo, chÃºng ta sáº½ lÃ m quen vá»›i Keras qua ba vÃ­ dá»¥ Ä‘Æ¡n giáº£n: <a href="/2016/12/28/linearregression/">linear regression</a>, <a href="/2017/01/27/logisticregression/">logistic regression</a>, vÃ  <a href="/2017/02/24/mlp/">multi-layer perceptron</a>.</p>

<p><a name="-linear-regression-va-logistic-regression-voi-keras"></a></p>

<h2 id="2-linear-regression-vÃ -logistic-regression-vá»›i-keras">2. Linear regression vÃ  logistic regression vá»›i Keras</h2>
<hr />

<p>Viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh deep learning hay neural network nÃ³i chung bao gá»“m cÃ¡c bÆ°á»›c:</p>

<ol>
  <li>Chuáº©n bá»‹ dá»¯ liá»‡u</li>
  <li>XÃ¢y dá»±ng network</li>
  <li>Chá»n thuáº­t toÃ¡n cáº­p nháº­t nghiá»‡m, xÃ¢y dá»±ng loss vÃ  phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh</li>
  <li>Huáº¥n luyá»‡n mÃ´ hÃ¬nh.</li>
  <li>ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh</li>
</ol>
<hr />

<p>ChÃºng ta cÃ¹ng xem Keras thá»±c hiá»‡n cÃ¡c bÆ°á»›c nÃ y thÃ´ng qua hai vÃ­ dá»¥ dÆ°á»›i Ä‘Ã¢y.</p>

<p><a name="-keras-voi-linear-regression"></a></p>

<h3 id="21-keras-vá»›i-linear-regression">2.1. Keras vá»›i linear regression</h3>

<p>Ta cÃ¹ng lÃ m má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n. Dá»¯ liá»‡u Ä‘áº§u <code class="language-plaintext highlighter-rouge">X</code> vÃ o cÃ³ sá»‘ chiá»u lÃ  2, Ä‘áº§u ra <code class="language-plaintext highlighter-rouge">y = 2*X[0] + 3*X[1] + 4 + e</code> vá»›i <code class="language-plaintext highlighter-rouge">e</code> lÃ  nhiá»…u tuÃ¢n theo má»™t phÃ¢n phá»‘i chuáº©n cÃ³ ká»³ vá»ng báº±ng 0, phÆ°Æ¡ng sai báº±ng 0.2.</p>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n code vÃ­ dá»¥ vá» huáº¥n luyá»‡n mÃ´ hÃ¬nh linear regression báº±ng Keras:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>

<span class="c1"># 1. create pseudo data y = 2*x0 + 3*x1 + 4
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="mi">2</span><span class="o">*</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">+</span> <span class="p">.</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># noise added
</span>
<span class="c1"># 2. Build model 
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)])</span>

<span class="c1"># 3. gradient descent optimizer and loss function 
</span><span class="n">sgd</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">)</span>

<span class="c1"># 4. Train the model 
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Káº¿t quáº£:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/100
100/100 [==============================] - 0s 5ms/step - loss: 1.7199
Epoch 2/100
100/100 [==============================] - 0s 709us/step - loss: 0.0388
Epoch 3/100
100/100 [==============================] - 0s 675us/step - loss: 0.0415
Epoch 4/100
100/100 [==============================] - 0s 774us/step - loss: 0.0392
Epoch 5/100
.....
Epoch 100/100
100/100 [==============================] - 0s 823us/step - loss: 0.0393
</code></pre></div></div>

<p>Ta tháº¥y rÄƒng thuáº­t toÃ¡n há»™i tá»¥ khÃ¡ nhanh vÃ  MSE loss khÃ¡ nhá» sau khi huáº¥n luyá»‡n xong.</p>

<p>ChÃºng ta cÃ¹ng xem xÃ©t tá»«ng bÆ°á»›c:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2. Build model 
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)])</span>

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Sequantial([&lt;a list&gt;])</code> lÃ  thá»ƒ hiá»‡n viá»‡c cÃ¡c layer Ä‘Æ°á»£c xÃ¢y dá»±ng theo Ä‘Ãºng thá»© tá»± trong <code class="language-plaintext highlighter-rouge">[&lt;a list&gt;]</code>. Pháº§n tá»­ Ä‘áº§u tiÃªn cá»§a list thá»ƒ hiá»‡n káº¿t ná»‘i giÆ°a input layer vÃ  layer tiáº¿p theo, cÃ¡c pháº§n tá»­ tiáº¿p theo cá»§a list thá»ƒ hiá»‡n káº¿t ná»‘i cá»§a cÃ¡c layer tiáº¿p theo.</p>

<p><code class="language-plaintext highlighter-rouge">Dense</code> thá»ƒ hiá»‡n má»™t <em>fully connected layer</em>, tá»©c toÃ n bá»™ cÃ¡c unit cá»§a layer trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c ná»‘i vá»›i toÃ n bá»™ cÃ¡c unit cá»§a layer hiá»‡n táº¡i. GiÃ¡ trá»‹ Ä‘áº§u tiÃªn trong <code class="language-plaintext highlighter-rouge">Dense</code> báº±ng <code class="language-plaintext highlighter-rouge">1</code> thá»ƒ hiá»‡n viá»‡c chá»‰ cÃ³ 1 unit á»Ÿ layer nÃ y (Ä‘áº§u ra cá»§a linear regression trong trÆ°á»ng há»£p nÃ y báº±ng 1). <code class="language-plaintext highlighter-rouge">input_shape = (2,)</code> chÃ­nh lÃ  kÃ­ch thÆ°á»›c cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o. KÃ­ch thÆ°á»›c nÃ y lÃ  má»™t tuple nÃªn ta cáº§n viáº¿t dÆ°á»›i dáº¡ng <code class="language-plaintext highlighter-rouge">(2,)</code>. Vá» sau, khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u nhiá»u chiá»u, ta sáº½ cÃ³ cÃ¡c tuple nhiá»u chiá»u. VÃ­ dá»¥, náº¿u input lÃ  áº£nh RGB vá»›i kÃ­ch thÆ°á»›c 224x224x3 pixel thÃ¬ <code class="language-plaintext highlighter-rouge">input_shape = (224, 224, 3)</code>.</p>

<p>CÃ¡c layer cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c thÃªm láº§n lÆ°á»£t vÃ o <code class="language-plaintext highlighter-rouge">model</code> báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m <code class="language-plaintext highlighter-rouge">.add()</code>. Äoáº¡n code phÃ­a trÃªn vÃ  Ä‘oáº¡n code ngay dÆ°á»›i Ä‘Ã¢y lÃ  tÆ°Æ¡ng Ä‘Æ°Æ¡ng.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
</code></pre></div></div>

<p>Activation cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¡ch ra thÃ nh riÃªng má»™t layer nhÆ° sau:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2. Build model 
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'linear'</span><span class="p">))</span>
</code></pre></div></div>

<p>Báº¡n Ä‘á»c cÃ³ thá»ƒ Ä‘á»c vá» cÃ¡c activation cá»§a Keras <a href="https://keras.io/activations/">táº¡i Ä‘Ã¢y</a>.</p>

<p>Äoáº¡n code tiáº¿p theo:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 3. gradient descent optimizer and loss function 
</span><span class="n">sgd</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">)</span>
</code></pre></div></div>

<p>Thá»ƒ hiá»‡n viá»‡c chá»n phÆ°Æ¡ng phÃ¡p cáº­p nháº­t nghiá»‡m, á»Ÿ Ä‘Ã¢u ta sá»­ dá»¥ng <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">Stochastic Gradient Descent (SGD)</a> vá»›i learning rate <code class="language-plaintext highlighter-rouge">lr=0.1</code>. CÃ¡c phÆ°Æ¡ng phÃ¡p cáº­p nháº­t nghiá»‡m khÃ¡c cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y táº¡i <a href="https://keras.io/optimizers/">Keras-Usage of optimizers</a>.
<code class="language-plaintext highlighter-rouge">loss='mse'</code> chÃ­nh lÃ  <a href="https://en.wikipedia.org/wiki/Mean_squared_error"><em>mean squared error</em></a>, lÃ  hÃ m máº¥t mÃ¡t cá»§a linear regression.</p>

<p>Sau khi xÃ¢y dá»±ng Ä‘Æ°á»£c mÃ´ hÃ¬nh vÃ  chá»‰ ra phÆ°Æ¡ng phÃ¡p cáº­p nháº­t cÅ©ng nhÆ° hÃ m máº¥t mÃ¡t, ta huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>
<p>(Keras khÃ¡ giá»‘ng vá»›i scikit-learn á»Ÿ chá»— cÃ¹ng huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh báº±ng phÆ°Æ¡ng thá»©c <code class="language-plaintext highlighter-rouge">.fit()</code>). á» Ä‘Ã¢y, <code class="language-plaintext highlighter-rouge">epochs</code> chÃ­nh lÃ  sá»‘ lÆ°á»£ng <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">epoch</a> vÃ  <code class="language-plaintext highlighter-rouge">batch_size</code> chÃ­nh lÃ  kÃ­ch thÆ°á»›c cá»§a má»™t <a href="\/2017/01/16/gradientdescent2/#-mini-batch-gradient-descent">mini-batch</a>.</p>

<p>Äá»ƒ xem há»‡ sá»‘ tÃ¬m Ä‘Æ°á»£c cá»§a linear regression, ta sá»­ dá»¥ng:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>
</code></pre></div></div>

<p>Káº¿t quáº£:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[array([[1.996118 ],
        [3.0239758]], dtype=float32), array([3.963116], dtype=float32)]
</code></pre></div></div>

<p>á»Ÿ Ä‘Ã³, pháº§n tá»­ thá»© nháº¥t cá»§a list nÃ y chÃ­nh lÃ  há»‡ sá»‘ tÃ¬m Ä‘Æ°á»£c, pháº©n tá»­ thá»© hai chÃ­nh lÃ  bias. Káº¿t quáº£ nÃ y gáº§n vá»›i nghiá»‡m mong Ä‘á»£i cá»§a bÃ i toÃ¡n (<code class="language-plaintext highlighter-rouge">y = 2*X[0] + 3*X[1] + 4</code>).</p>

<p><em>TÆ°Æ¡ng Ä‘á»‘i Ä‘Æ¡n giáº£n.</em></p>

<p><a name="-keras-voi-logistic-regression"></a></p>

<h3 id="21-keras-vá»›i-logistic-regression">2.1. Keras vá»›i logistic regression</h3>
<p>Quay láº¡i <a href="https://machinelearningcoban.com/2017/01/27/logisticregression/#-vi-du-voi-python">vÃ­ dá»¥ vá» má»‘i liÃªn há»‡ giá»¯a sá»‘ giá» Ã´n táº­p vÃ  káº¿t quáº£ thi trong bÃ i logistic regression</a>, bÃ i toÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng keras nhÆ° sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>

<span class="c1"># 1. Prepare data 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">1.50</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="mf">2.00</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">,</span> <span class="mf">2.50</span><span class="p">,</span> 
              <span class="mf">2.75</span><span class="p">,</span> <span class="mf">3.00</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">,</span> <span class="mf">3.50</span><span class="p">,</span> <span class="mf">4.00</span><span class="p">,</span> <span class="mf">4.25</span><span class="p">,</span> <span class="mf">4.50</span><span class="p">,</span> <span class="mf">4.75</span><span class="p">,</span> <span class="mf">5.00</span><span class="p">,</span> <span class="mf">5.50</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># 2. Build model 
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'sigmoid'</span><span class="p">))</span>

<span class="c1"># 3. gradient descent optimizer and loss function 
</span><span class="n">sgd</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="n">binary_crossentropy</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">)</span>

<span class="c1"># 4. Train the model 
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
</code></pre></div></div>

<p>CÃ³ hai sá»± khÃ¡c biá»‡t á»Ÿ <code class="language-plaintext highlighter-rouge">Activation</code> vÃ  <code class="language-plaintext highlighter-rouge">loss</code> vÃ¬ logistic regression sá»­ dá»¥ng hÃ m activation lÃ  sigmoid, hÃ m máº¥t mÃ¡t lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a cross entropy vá»›i hai class.</p>

<p>Káº¿t quáº£ tÃ¬m Ä‘Æ°á»£c tÆ°Æ¡ng Ä‘á»‘i giá»‘ng vá»›i káº¿t quáº£ tÃ¬m Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³ vá»›i numpy.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[array([[1.5141923]], dtype=float32), array([-4.1248693], dtype=float32)]
</code></pre></div></div>
<p>vÃ  ta cÃ³ <code class="language-plaintext highlighter-rouge">y ~ 1.51*x - 4.12</code>.</p>

<p><a name="-keras-cho-multi-layer-perceptron"></a></p>

<h2 id="3-keras-cho-multi-layer-perceptron">3. Keras cho multi-layer perceptron</h2>

<p>ChÃºng ta cÃ¹ng xÃ¢y dá»±ng má»™t máº¡ng MLP Ä‘Æ¡n giáº£n vá»›i Keras Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n phÃ¢n loáº¡i áº£nh.
<a name="-gioi-thieu-ve-fashion-mnist"></a></p>

<h3 id="31-giá»›i-thiá»‡u-vá»-fashion-mnist">3.1. Giá»›i thiá»‡u vá» Fashion-MNIST</h3>

<p>CÆ¡ sá»Ÿ dá»¯ liá»‡u áº£nh Ä‘Æ°á»£c dÃ¹ng lÃ  <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a>.</p>

<p>ChÃºng ta Ä‘Ã£ quÃ¡ quen vá»›i viá»‡c sá»­ dá»¥ng <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. MNIST lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u vá» chá»¯ sá»‘ viáº¿t tay, Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t rá»™ng rÃ£i trong cá»“ng Ä‘á»“ng AI/ML. MNIST thÆ°á»ng Ä‘Æ°á»£c thá»­ Ä‘áº§u tiÃªn khi cÃ³ má»™t thuáº­t toÃ¡n phÃ¢n loáº¡i áº£nh má»›i. Má»™t sá»‘ ngÆ°á»i nÃ³i â€œ<em>If it doesnâ€™t work on MNIST, it wonâ€™t work at all</em>â€. NhÆ°ng Ä‘á»“ng thá»i, â€œ<em>Well, if it does work on MNIST, it may still fail on others.</em>â€</p>

<p>Fashion-MNIST Ä‘Æ°á»£c táº¡o ra gáº§n Ä‘Ã¢y vá»›i kÃ­ch thÆ°á»›c tÆ°Æ¡ng tá»± nhÆ° MNIST nhÆ°ng cÃ¡c áº£nh lÃ  cÃ¡c áº£nh xÃ¡m cá»§a trang phá»¥c vá»›i cÃ¡c nhÃ£n: (0) T-shirt/top, (1) Trouser, (2) Pullover, (3) Dress, (4) Coat, (5) Sandal, (6) Shirt, (7) Sneaker, (8) Bag, (9) Ankle boot. Fashion-MNIST cÅ©ng cÃ³ 10 class, 60000 áº£nh cho training, 10000 áº£nh cho test, má»—i áº£nh cÃ³ kÃ­ch thÆ°á»›c 28x28 pixel vÃ  lÃ  cÃ¡c áº£nh xÃ¡m vá»›i chá»‰ má»™t channel. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ vá» áº£nh cá»§a class (2) Pullover.</p>

<hr />

<div class="imgcap">
<div>
    <img src="/assets/36_keras/mlp1.png" width="255" />
</div>
<div class="thecap">HÃ¬nh áº£nh má»™t máº«u dá»¯ liá»‡u Ä‘áº§u vÃ o vá»›i kÃ­ch thÆ°á»›c 28x28, tÆ°Æ¡ng á»©ng vá»›i nhÃ£n "pullover" - Ã¡o len chui Ä‘áº§u <br /> </div>
</div>
<hr />

<p>TÃ¡c giáº£ cá»§a Fashion-MNIST cho ráº±ng cáº§n pháº£i thay tháº¿ MNIST vÃ¬:</p>

<ul>
  <li>
    <p><strong>MNIST Ä‘Ã£ trá»Ÿ nÃªn quÃ¡ dá»…</strong>. Ráº¥t nhiá»u thuáº­t toÃ¡n deep learning Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c lÃªn tá»›i 99.7%, ngay cáº£ KNN cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c trÃªn 96%. Ráº¥t khÃ³ Ä‘á»ƒ Ä‘Ã¡nh báº¡i con sá»‘ 99.7%, vÃ  náº¿u mÃ´ hÃ¬nh cá»§a báº¡n Ä‘áº¡t Ä‘Æ°á»£c con sá»‘ nÃ y, ta váº«n khÃ³ cÃ³ thá»ƒ káº¿t luáº­n ngay Ä‘Ã³ lÃ  má»™t mÃ´ hÃ¬nh tá»‘t.</p>
  </li>
  <li>
    <p><strong>MNIST Ä‘Æ°á»£c sá»­ dá»¥ng quÃ¡ nhiá»u</strong> Ä‘áº¿n má»©c nhÃ m chÃ¡n.</p>
  </li>
  <li>
    <p><strong>MNIST khÃ´ng Ä‘áº¡i diá»‡n cho cÃ¡c bÃ i toÃ¡n computer vision hiá»‡n Ä‘áº¡i</strong>.</p>
  </li>
</ul>

<p>CÆ¡ sá»Ÿ dá»¯ liá»‡u Fashion-MNIST cÃ³ thá»ƒ Ä‘Æ°á»£c táº£i vá» thÃ´ng qua <code class="language-plaintext highlighter-rouge">keras.datasets</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. prepare data 
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span> 
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">fashion_mnist</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x_train shape:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x_test shape:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_train shape:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_test shape:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_train shape:   (60000, 28, 28)
x_test shape:    (10000, 28, 28)
y_train shape:   (60000,)
y_test shape:    (10000,)
</code></pre></div></div>

<p>á» Ä‘Ã¢y, <code class="language-plaintext highlighter-rouge">x_train, x_test</code> mang cÃ¡c giÃ¡ trá»‹ nguyÃªn tá»« 0 Ä‘áº¿n 255, <code class="language-plaintext highlighter-rouge">y_train, y_test</code> chá»©a cÃ¡c sá»‘ nguyÃªn tá»« 0 Ä‘áº¿n 9 thá»ƒ hiá»‡n class cá»§a <code class="language-plaintext highlighter-rouge">x</code> tÆ°Æ¡ng á»©ng. Náº¿u sá»­ dá»¥ng má»™t neural network vá»›i <a href="/2017/02/17/softmax/">softmax layer</a> á»Ÿ cuá»‘i, ta cáº§n chuáº©n hoÃ¡ dá»¯ liá»‡u Ä‘áº§u vÃ o <code class="language-plaintext highlighter-rouge">x_train, x_test</code> vá» Ä‘oáº¡n [0, 1] vÃ  chuyá»ƒn <code class="language-plaintext highlighter-rouge">y_train, y_test</code> vá» dáº¡ng one-hot coding.</p>

<p>Viá»‡c nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° sau:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># data normalization
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">/</span><span class="mf">255.</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">/</span><span class="mf">255.</span> 
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="c1"># convert class vectors to binary class matrices
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="-xay-dung-mot-multi-layer-perceptron-de-giai-quyet-bai-toan"></a></p>

<h3 id="32-xÃ¢y-dá»±ng-má»™t-multi-layer-perceptron-Ä‘á»ƒ-giáº£i-quyáº¿t-bÃ i-toÃ¡n">3.2. XÃ¢y dá»±ng má»™t multi-layer perceptron Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n</h3>
<p>Ta Ä‘Ã£ thá»±c hiá»‡n bÆ°á»›c Ä‘áº§u tiÃªn trong bÃ i toÃ¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh neural network cho bÃ i toÃ¡n classification â€“ bÆ°á»›c chuáº©n bá»‹ dá»¯ liá»‡u.</p>

<p>Trong má»¥c nÃ y, chÃºng ta sáº½ Ä‘i xÃ¢y dá»±ng má»™t neural network Ä‘Æ¡n giáº£n. Má»™t mÃ´ hÃ¬nh Ä‘á»§ Ä‘Æ¡n giáº£n giÃºp chÃºng ta tiáº¿p tá»¥c lÃ m quen vá»›i Keras lÃ  <a href="/2017/02/24/mlp/">multi-layer perceptron</a>.</p>

<p>Ta sáº½ xÃ¢y dá»±ng má»™t MLP vá»›i 3 hidden layers. CÃ¡c layer cá»¥ thá»ƒ nhÆ° sau:</p>
<ol>
  <li>Input layer vá»›i sá»‘ units lÃ  \(28\times 28 = 784\). Má»—i input lÃ  má»™t bá»©c áº£nh cá»§a Fashion-MNIST Ä‘Æ°á»£c <em>kÃ©o dÃ i ra</em> thÃ nh má»™t vector.</li>
  <li>Hidden layer thá»© nháº¥t vá»›i 128 units, activation lÃ  ReLU.</li>
  <li>Hidden layer thá»© hai vá»›i 256 units, activation lÃ  ReLU.</li>
  <li>Hidden layer thá»© ba vá»›i 512 units, activation lÃ  ReLU.</li>
  <li>Output layer lÃ  má»™t softmax layer vá»›i 10 units.</li>
</ol>

<p>HÃ m máº¥t mÃ¡t lÃ  cross entropy, ta táº¡m thá»i chÆ°a quan tÃ¢m Ä‘áº¿n <a href="/2017/03/04/overfitting/#-regularization">regularization</a> (á»Ÿ Ä‘Ã¢y lÃ  <a href="/2017/03/04/overfitting/#-%5C%5Cl%5C%5C-regularization">weight decay</a>).</p>

<p>PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ há»‡ thá»‘ng phÃ¢n lá»›p nÃ y lÃ  <code class="language-plaintext highlighter-rouge">'accuracy'</code>, tá»©c sá»‘ lÆ°á»£ng Ä‘iá»ƒm Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng trong toÃ n bá»™ sá»‘ Ä‘iá»ƒm.</p>

<p>Network Ä‘Æ¡n giáº£n nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn Keras nhÆ° sau:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">metrics</span> 
<span class="c1"># 2. buid model 
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="c1"># 3. loss, metrics 
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">categorical_crossentropy</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<p>á» Ä‘Ã¢y cÃ³ má»™t hÃ m má»›i lÃ  <code class="language-plaintext highlighter-rouge">Flatten()</code>, hÃ m nÃ y biáº¿n má»—i Ä‘iá»ƒm dá»¯ liá»ƒu á»Ÿ dáº¡ng má»™t máº£ng nhiá»u chiá»u thÃ nh má»™t máº£ng má»™t chiá»u. <em>VÃ¬ ta Ä‘ang sá»­ dá»¥ng MLP nÃªn ta cáº§n lÃ m cÃ´ng viá»‡c nÃ y. Vá» sau, khi sá»­ dá»¥ng CNN, viá»‡c giá»¯ nguyÃªn máº£ng hai chiá»u sáº½ cho káº¿t quáº£ tá»‘t hÆ¡n.</em></p>

<p>Káº¿t quáº£ sau khi thá»±c hiá»‡n Ä‘oáº¡n code trÃªn</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/20
60000/60000 [==============================] - 3s 47us/step - loss: 0.6784 - acc: 0.7582
....
Epoch 20/20
60000/60000 [==============================] - 3s 54us/step - loss: 0.2140 - acc: 0.9192
</code></pre></div></div>

<p>Sau 20 epochs, mÃ´ hÃ¬nh cho Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n <code class="language-plaintext highlighter-rouge">x_train</code> khÃ¡ cao.</p>

<p>Náº¿u chÃºng ta muá»‘n Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p <code class="language-plaintext highlighter-rouge">x_test</code>, ta cÃ³ thá»ƒ thá»±c hiá»‡n nhÆ° sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">metrics</span> 
<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test loss: %.4f'</span><span class="o">%</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy %.4f'</span><span class="o">%</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test loss: 0.3312
Test accuracy 0.8840
</code></pre></div></div>

<p>NhÆ° váº­y, Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm thá»­ Ä‘áº¡t 88.4%.</p>

<p><em>Báº¡n Ä‘á»c cÃ³ thá»ƒ thá»­ giáº£i quyáº¿t bÃ i toÃ¡n nÃ y báº±ng cÃ¡c thuáº­t toÃ¡n phÃ¢n loáº¡i khÃ¡c vÃ  so sÃ¡nh káº¿t quáº£.</em></p>

<p><a name="-nhan-xet"></a></p>

<h3 id="33-nháº­n-xÃ©t">3.3. Nháº­n xÃ©t</h3>

<ul>
  <li>
    <p>Dá»¯ liá»‡u Ä‘áº§u vÃ o chá»‰ lÃ  784 chiá»u vÃ  Ä‘áº·c trÆ°ng cho dá»¯ liá»‡u thá»i trang cÅ©ng khÃ´ng quÃ¡ phá»©c táº¡p trÃªn ná»n lÃ  má»™t mÃ u Ä‘en vÃ  áº£nh Ä‘Ã£ Ä‘Æ°á»£c cÄƒn chá»‰nh Ä‘Ãºng tÃ¢m, Ä‘Ãºng kÃ­ch thÆ°á»›c. MLP máº·c dÃ¹ cho káº¿t quáº£ tÆ°Æ¡ng Ä‘á»‘i tá»‘t trong bÃ i toÃ¡n nÃ y, nÃ³ khÃ´ng pháº£i lÃ  má»™t mÃ´ hÃ¬nh tá»‘i Æ°u cho dá»¯ liá»‡u dáº¡ng áº£nh vÃ¬ Ã­t nháº¥t, dá»¯ liá»‡u áº£nh hai chiá»u ban Ä‘áº§u Ä‘Ã£ Ä‘Æ°á»£c <em>dÃ n pháº³ng</em> ra thÃ nh dá»¯ liá»‡u má»™t chiá»u, lÃ m máº¥t Ä‘i nhá»¯ng thÃ´ng tin vá» khÃ´ng gian trong áº£nh (spatial information). Convolutional neural network thÃ´ng thÆ°á»ng sáº½ cho káº¿t quáº£ tá»‘t hÆ¡n. TÃ´i sáº½ sá»›m giá»›i thiá»‡u vá»›i báº¡n Ä‘á»c vá» kiáº¿n trÃºc nÃ y.</p>
  </li>
  <li>
    <p>Máº¡ng MLP Ä‘Æ¡n giáº£n nÃ y chÆ°a dÃ¹ng nhiá»u ká»¹ thuáº­t cá»§a deep learning, chÃºng ta sáº½ dáº§n lÃ m quen vá»›i cÃ¡c ká»¹ thuáº­t giÃºp tÄƒng Ä‘á»™ chÃ­nh xÃ¡c vÃ  giáº£m thá»i gian huáº¥n luyá»‡n trong cÃ¡c bÃ i sau.</p>
  </li>
</ul>

<p><a name="-ket-luan"></a></p>
<h2 id="4-káº¿t-luáº­n">4. Káº¿t luáº­n</h2>

<ul>
  <li>
    <p>Keras lÃ  má»™t thÆ° viá»‡n tÆ°Æ¡ng Ä‘á»‘i dá»… sá»­ dá»¥ng Ä‘á»‘i vá»›i ngÆ°á»i má»›i báº¯t Ä‘áº§u. NÃ³ cung cáº¥p cÃ¡c hÃ m sá»‘ cáº§n thiáº¿t vá»›i cÃº phÃ¡p Ä‘Æ¡n giáº£n.</p>
  </li>
  <li>
    <p>Khi Ä‘i sÃ¢u hÆ¡n vÃ o deep learning trong cÃ¡c bÃ i sau, chÃºng ta sáº½ dáº§n lÃ m quen vá»›i cÃ¡c ká»¹ thuáº­t láº­p trÃ¬nh vá»›i Keras. Má»i cÃ¡c báº¡n Ä‘Ã³n Ä‘á»c.</p>
  </li>
  <li>
    <p>Source code trong bÃ i nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/36_keras/Keras.ipynb">táº¡i Ä‘Ã¢y</a>.
<a name="-tai-lieu-tham-khao"></a></p>
  </li>
</ul>

<h2 id="5-tÃ i-liá»‡u-tham-kháº£o">5. TÃ i liá»‡u tham kháº£o</h2>

<p>[1] <a href="https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750">Battle of the Deep Learning frameworksâ€Šâ€”â€ŠPart I: 2017, even more frameworks and interfaces</a></p>

<p>[2] <a href="https://keras.io/">Keras hompage</a></p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">Comparison of deep learning software â€“ Wikipedia</a></p>
:ET