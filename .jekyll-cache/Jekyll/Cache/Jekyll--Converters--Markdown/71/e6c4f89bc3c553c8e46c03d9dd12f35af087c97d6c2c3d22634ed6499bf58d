I"j†<p><strong>Trong trang nÃ y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-moi-quan-he-giua-pca-va-svd">1. Má»‘i quan há»‡ giá»¯a PCA vÃ  SVD</a>
    <ul>
      <li><a href="#-svd-cho-bai-toan-xap-xi-low-rank-tot-nhat">1.1. SVD cho bÃ i toÃ¡n xáº¥p xá»‰ low-rank tá»‘t nháº¥t</a></li>
      <li><a href="#-y-tuong-cua-pca">1.2. Ã tÆ°á»Ÿng cá»§a PCA</a></li>
      <li><a href="#-quan-he-giua-pca-va-svd">1.3. Quan há»‡ giá»¯a PCA vÃ  SVD</a></li>
    </ul>
  </li>
  <li><a href="#-lam-the-nao-de-chon-chieu-cua-du-lieu-moi">2. LÃ m tháº¿ nÃ o Ä‘á»ƒ chá»n chiá»u cá»§a dá»¯ liá»‡u má»›i</a></li>
  <li><a href="#-luu-y-ve-tinh-pca-trong-cac-bai-toan-thuc-te">3. LÆ°u Ã½ vá» tÃ­nh PCA trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿</a>
    <ul>
      <li><a href="#-so-chieu-du-lieu-nhieu-hon-so-diem-du-lieu">3.1. Sá»‘ chiá»u dá»¯ liá»‡u nhiá»u hÆ¡n sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u</a>
        <ul>
          <li><a href="#-chuan-hoa-cac-vector-rieng">3.2. Chuáº©n hoÃ¡ cÃ¡c vector riÃªng</a></li>
        </ul>
      </li>
      <li><a href="#-voi-cac-bai-toan-large-scale">3.3. Vá»›i cÃ¡c bÃ i toÃ¡n large-scale</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-tren-python">4. VÃ­ dá»¥ trÃªn Python</a>
    <ul>
      <li><a href="#-eigenface">4.1. Eigenface</a></li>
      <li><a href="#--unsupervised-abnormal-detection">4.2.  Unsupervised Abnormal Detection</a></li>
    </ul>
  </li>
  <li><a href="#-thao-luan">5. Tháº£o luáº­n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. TÃ i liá»‡u tham kháº£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p>Trong <a href="/2017/06/15/pca/">pháº§n 1 cá»§a Principal Component Analysis</a> (PCA), má»™t phÆ°Æ¡ng phÃ¡p giáº£m chiá»u dá»¯ liá»‡u ráº¥t quan trá»ng, chÃºng ta Ä‘Ã£ cÃ¹ng Ã´n láº¡i má»™t vÃ i kiáº¿n thá»©c vá» Äáº¡i sá»‘ tuyáº¿n tÃ­nh vÃ  Thá»‘ng kÃª, Ä‘á»“ng thá»i, Ã½ nghÄ©a toÃ¡n há»c vÃ  cÃ¡c bÆ°á»›c thá»±c hiá»‡n PCA cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c trÃ¬nh bÃ y. Trong pháº§n 2 nÃ y, chÃºng ta cÃ¹ng tÃ¬m hiá»ƒu thÃªm má»™t vÃ i tÃ­nh cháº¥t quan trá»ng cá»§a PCA cÅ©ng nhÆ° cÃ¡c á»©ng dá»¥ng ná»•i báº­t cá»§a PCA trong cÃ¡c bÃ i toÃ¡n Machine Learning.</p>

<p><em>CÃ¡c báº¡n Ä‘Æ°á»£c khuyáº¿n khÃ­ch Ä‘á»c <a href="/2017/06/07/svd/">BÃ i 26</a> vÃ  <a href="/2017/06/15/pca/">BÃ i 27</a> trÆ°á»›c khi Ä‘á»c bÃ i nÃ y.</em></p>

<p><a name="-moi-quan-he-giua-pca-va-svd"></a></p>

<h2 id="1-má»‘i-quan-há»‡-giá»¯a-pca-vÃ -svd">1. Má»‘i quan há»‡ giá»¯a PCA vÃ  SVD</h2>
<p>Giá»¯a PCA vÃ  SVD cÃ³ má»—i quan há»‡ Ä‘áº·c biá»‡t vá»›i nhau. Äá»ƒ nháº­n ra Ä‘iá»u nÃ y, tÃ´i xin Ä‘Æ°á»£c nháº¯c láº¡i hai Ä‘iá»ƒm Ä‘Ã£ trÃ¬nh bÃ y sau Ä‘Ã¢y:</p>

<p><a name="-svd-cho-bai-toan-xap-xi-low-rank-tot-nhat"></a></p>

<h3 id="11-svd-cho-bÃ i-toÃ¡n-xáº¥p-xá»‰-low-rank-tá»‘t-nháº¥t">1.1. SVD cho bÃ i toÃ¡n xáº¥p xá»‰ low-rank tá»‘t nháº¥t</h3>
<p>Nghiá»‡m \(\mathbf{A}\) cá»§a bÃ i toÃ¡n xáº¥p xá»‰ má»™t ma tráº­n bá»Ÿi má»™t ma tráº­n khÃ¡c cÃ³ rank khÃ´ng vÆ°á»£t quÃ¡ \(k\):
\[
\begin{eqnarray}
\min_{\mathbf{A}} &amp;&amp;||\mathbf{X} - \mathbf{A}||_F ~~~~~~~~~~~~~~ (1)\newline
\text{s.t.} &amp;&amp; \text{rank}(\mathbf{A}) = K
\end{eqnarray}
\]</p>

<p>chÃ­nh lÃ  <a href="http://machinelearningcoban.com/2017/06/07/svd/#-truncated-svd">Truncated SVD</a> cá»§a \(\mathbf{A}\). Cá»¥ thá»ƒ, náº¿u SVD cá»§a \(\mathbf{X} \in\mathbb{R}^{D\times N}\) lÃ :
\[
  \mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T
\]
vá»›i \(\mathbf{U} \in \mathbb{R}^{D \times D}\) vÃ  \(\mathbf{V}\in \mathbb{R}^{N\times N}\) lÃ  cÃ¡c ma tráº­n trá»±c giao, vÃ  \(\mathbf{\Sigma} \in \mathbb{R}^{D \times N}\) lÃ  ma tráº­n Ä‘Æ°á»ng chÃ©o (khÃ´ng nháº¥t thiáº¿t vuÃ´ng) vá»›i cÃ¡c pháº§n tá»­ trÃªn Ä‘Æ°á»ng chÃ©o khÃ´ng Ã¢m giáº£m dáº§n, thÃ¬ nghiá»‡m cá»§a bÃ i toÃ¡n \((1)\) chÃ­nh lÃ :
\[
  \mathbf{A} = \mathbf{U}_K \mathbf{\Sigma}_K \mathbf{V}_K^T ~~~ (2)
\]</p>

<p>vá»›i \(\mathbf{U} \in \mathbb{R}^{D \times K}\) vÃ  \(\mathbf{V}\in \mathbb{R}^{N\times K}\) lÃ  cÃ¡c ma tráº­n táº¡o bá»Ÿi \(K\) cá»™t Ä‘áº§u tiÃªn cá»§a \(\mathbf{U}\) vÃ  \(\mathbf{V}\), vÃ  \(\mathbf{\Sigma}_K \in \mathbb{R}^{K \times K}\) lÃ  ma tráº­n Ä‘Æ°á»ng chÃ©o con á»©ng vá»›i \(K\) hÃ ng Ä‘áº§u tiÃªn vÃ  \(K\) cá»™t Ä‘áº§u tiÃªn cá»§a \(\mathbf{\Sigma}\).</p>

<p><a name="-y-tuong-cua-pca"></a></p>

<h3 id="12-Ã½-tÆ°á»Ÿng-cá»§a-pca">1.2. Ã tÆ°á»Ÿng cá»§a PCA</h3>
<p>Trong PCA, nhÆ° Ä‘Ã£ chá»©ng minh á»Ÿ <a href="/2017/06/15/pca/#eqn10">biá»ƒu thá»©c \((10)\) trong BÃ i 27</a>, PCA lÃ  bÃ i toÃ¡n Ä‘i tÃ¬m ma tráº­n trá»±c giao \(\mathbf{U}\) vÃ  ma tráº­n mÃ´ táº£ dá»¯ liá»‡u á»Ÿ khÃ´ng gian tháº¥p chiá»u \(\mathbf{Z}\) sao cho viá»‡c xáº¥p xá»‰ sau Ä‘Ã¢y lÃ  tá»‘t nháº¥t:
\[
\mathbf{X} \approx \tilde{\mathbf{X}} = \mathbf{U}_K \mathbf{Z} + \bar{\mathbf{U}}_K \bar{\mathbf{U}}_K^T\bar{\mathbf{x}}\mathbf{1}^T ~~~ (3)
\]
vá»›i \(\mathbf{U}_K, \bar{\mathbf{U}}_K\) láº§n lÆ°á»£t lÃ  cÃ¡c ma tráº­n Ä‘Æ°á»£c táº¡o bá»Ÿi \(K\) cá»™t Ä‘áº§u tiÃªn vÃ  \(D-K\) cá»™t cuá»‘i cÃ¹ng cá»§a ma tráº­n trá»±c giao \(\mathbf{U}\), vÃ  \(\bar{\mathbf{x}}\) lÃ  vector ká»³ vá»ng cá»§a dá»¯ liá»‡u.</p>

<p><strong>Giáº£ sá»­ ráº±ng vector ká»³ vá»ng \(\bar{\mathbf{x}} = \mathbf{0}\)</strong>. Khi Ä‘Ã³, \((3)\) tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i:
\[
\mathbf{X} \approx \tilde{\mathbf{X}} = \mathbf{U}_K \mathbf{Z}~~~ (4)
\]</p>

<p>BÃ i toÃ¡n tá»‘i Æ°u cá»§a PCA sáº½ trá»Ÿ thÃ nh:
\[
\begin{eqnarray}
  \mathbf{U}_K, \mathbf{Z} &amp;=&amp; \min_{\mathbf{U}_K, \mathbf{Z} } ||\mathbf{X} - \mathbf{U}_K \mathbf{Z}||_F&amp; (5)\newline
  \text{s.t.:}&amp;&amp; \mathbf{U}_K^T \mathbf{U}_K = \mathbf{I}_K &amp;
\end{eqnarray}
\]
vá»›i \(\mathbf{I}_K \in \mathbb{R}^{K\times K}\) lÃ  ma tráº­n Ä‘Æ¡n vá»‹ trong khÃ´ng gian \(K\) chiá»u, vÃ  Ä‘iá»u kiá»‡n rÃ ng buá»™c lÃ  Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c cá»™t cá»§a \(\mathbf{U}_K\) táº¡o thÃ nh má»™t há»‡ trá»±c chuáº©n.</p>

<p><a name="-quan-he-giua-pca-va-svd"></a></p>

<h3 id="13-quan-há»‡-giá»¯a-pca-vÃ -svd">1.3. Quan há»‡ giá»¯a PCA vÃ  SVD</h3>
<p>Báº¡n cÃ³ nháº­n ra Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai bÃ i toÃ¡n tá»‘i Æ°u \((1)\) vÃ  \((5)\) vá»›i nghiá»‡m cá»§a bÃ i toÃ¡n Ä‘áº§u tiÃªn Ä‘Æ°á»£c cho trong \((2)\)? Báº¡n cÃ³ thá»ƒ nháº­n ra ngay nghiá»‡m cá»§a bÃ i toÃ¡n \((5)\) chÃ­nh lÃ :
\[
\begin{eqnarray}
  \mathbf{U}_K \quad \text{in}\quad (5) &amp;=&amp; \mathbf{U}_K\quad \text{in} \quad(2) \newline
  \mathbf{Z} \quad\text{in}\quad (5) &amp;=&amp; \mathbf{\Sigma}_K \mathbf{V}_K^T \quad \text{in} \quad (2)
\end{eqnarray}
\]</p>

<p>NhÆ° váº­y, náº¿u cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c biá»…u diá»…n bá»Ÿi cÃ¡c cá»™t cá»§a má»™t ma tráº­n, vÃ  trung bÃ¬nh cá»™ng cá»§a má»—i hÃ ng cá»§a ma tráº­n Ä‘Ã³ báº±ng 0 (Ä‘á»ƒ cho vector ká»³ vá»ng báº±ng 0), thÃ¬ nghiá»‡m cá»§a bÃ i toÃ¡n PCA Ä‘Æ°á»£c rÃºt ra trá»±c tiáº¿p tá»« Truncated SVD cá»§a ma tráº­n Ä‘Ã³. NÃ³i cÃ¡ch khÃ¡c, nghiá»‡m cá»§a PCA chÃ­nh lÃ  má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a bÃ i toÃ¡n Matrix Factorization giáº£i báº±ng SVD.</p>

<p><a name="-lam-the-nao-de-chon-chieu-cua-du-lieu-moi"></a></p>

<h2 id="2-lÃ m-tháº¿-nÃ o-Ä‘á»ƒ-chá»n-chiá»u-cá»§a-dá»¯-liá»‡u-má»›i">2. LÃ m tháº¿ nÃ o Ä‘á»ƒ chá»n chiá»u cá»§a dá»¯ liá»‡u má»›i</h2>

<p>Má»™t cÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra lÃ , lÃ m tháº¿ nÃ o Ä‘á»ƒ chá»n ra giÃ¡ trá»‹ \(K\) - chiá»u cá»§a dá»¯ liá»‡u má»›i - vá»›i tá»«ng loáº¡i dá»¯ liá»‡u khÃ¡c nhau?</p>

<p>CÃ³ má»™t cÃ¡ch xÃ¡c Ä‘á»‹nh \(K\) lÃ  dá»±a trÃªn viá»‡c <em>lÆ°á»£ng thÃ´ng tin muá»‘n giá»¯ láº¡i</em>. NhÆ° Ä‘Ã£ trÃ¬nh bÃ y, PCA cÃ²n Ä‘Æ°á»£c gá»i lÃ  phÆ°Æ¡ng phÃ¡p tá»‘i Ä‘a <em>tá»•ng phÆ°Æ¡ng sai Ä‘Æ°á»£c giá»¯ láº¡i</em>. Váº­y ta cÃ³ thá»ƒ coi tá»•ng cÃ¡c phÆ°Æ¡ng sai Ä‘Æ°á»£c giá»¯ láº¡i lÃ  lÆ°á»£ng thÃ´ng tin Ä‘Æ°á»£c giá»¯ láº¡i. Vá»›i phÆ°Æ¡ng sai cÃ ng lá»›n, tá»©c dá»¯ liá»‡u cÃ³ Ä‘á»™ phÃ¢n tÃ¡n cao, thá»ƒ hiá»‡n lÆ°á»£ng thÃ´ng tin cÃ ng lá»›n.</p>

<p>Nháº¯c láº¡i ráº±ng trong má»i há»‡ trá»¥c toáº¡ Ä‘á»™, tá»•ng phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u lÃ  nhÆ° nhau vÃ  báº±ng tá»•ng cÃ¡c trá»‹ riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai \(\sum_{i=1}^D \lambda_i\). ThÃªm ná»¯a, PCA giÃºp giá»¯ láº¡i lÆ°á»£ng thÃ´ng tin (tá»•ng cÃ¡c phÆ°Æ¡ng sai) lÃ : \(\sum_{i=1}^K \lambda_i\). Váº­y ta cÃ³ thá»ƒ coi biá»ƒu thá»©c:
 \[
   r_K = \frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^D \lambda_j} \quad \quad (6)
 \]
lÃ  lÆ°á»£ng thÃ´ng tin Ä‘Æ°á»£c giá»¯ láº¡i khi sá»‘ chiá»u dá»¯ liá»‡u má»›i sau PCA lÃ  \(K\).</p>

<p>NhÆ° váº­y, giáº£ sá»­ ta muá»‘n giá»¯ láº¡i 99% dá»¯ liá»‡u, ta chá»‰ cáº§n chá»n \(K\) lÃ  sá»‘ tá»± nhiÃªn nhá» nháº¥t sao cho \(r_K \geq 0.99\).</p>

<p>Khi dá»¯ liá»‡u phÃ¢n bá»‘ quanh má»™t khÃ´ng gian con, cÃ¡c giÃ¡ trá»‹ phÆ°Æ¡ng sai lá»›n nháº¥t á»©ng vá»›i cÃ¡c \(\lambda_i\) Ä‘áº§u tiÃªn lá»›n hÆ¡n nhiá»u so vá»›i cÃ¡c phÆ°Æ¡ng sai cÃ²n láº¡i. Khi Ä‘Ã³, ta cÃ³ thá»ƒ chá»n Ä‘Æ°á»£c \(K\) khÃ¡ nhá» Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c \(r_K \geq 0.99\).</p>

<p><a name="-luu-y-ve-tinh-pca-trong-cac-bai-toan-thuc-te"></a></p>

<h2 id="3-lÆ°u-Ã½-vá»-tÃ­nh-pca-trong-cÃ¡c-bÃ i-toÃ¡n-thá»±c-táº¿">3. LÆ°u Ã½ vá» tÃ­nh PCA trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿</h2>
<p>CÃ³ hai trÆ°á»ng há»£p trong thá»±c táº¿ mÃ  chÃºng ta cáº§n lÆ°u Ã½ vá» PCA. TrÆ°á»ng há»£p thá»© nháº¥t lÃ  lÆ°á»£ng dá»¯ liá»‡u cÃ³ Ä‘Æ°á»£c nhá» hÆ¡n ráº¥t nhiá»u so vá»›i sá»‘ chiá»u dá»¯ liá»‡u. TrÆ°á»ng há»£p thá»© hai lÃ  khi lÆ°á»£ng dá»¯ liá»‡u trong táº­p training lÃ  ráº¥t lá»›n, cÃ³ thá»ƒ lÃªn tá»›i cáº£ triá»‡u. Viá»‡c tÃ­nh toÃ¡n ma tráº­n hiá»‡p phÆ°Æ¡ng sai vÃ  trá»‹ riÃªng Ä‘Ã´i khi trá»Ÿ nÃªn báº¥t kháº£ thi. CÃ³ nhá»¯ng hÆ°á»›ng giáº£i quyáº¿t hiá»‡u quáº£ cho cÃ¡c trÆ°á»ng há»£p nÃ y.</p>

<p><strong>Trong má»¥c nÃ y, ta sáº½ coi nhÆ° dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hoÃ¡, tá»©c Ä‘Ã£ Ä‘Æ°á»£c trá»« Ä‘i vector ká»³ vá»ng. Khi Ä‘Ã³, ma tráº­n hiá»‡p phÆ°Æ¡ng sai sáº½ lÃ  \(\mathbf{S} = \frac{1}{N}\mathbf{X}\mathbf{X}^T\).</strong>
<a name="-so-chieu-du-lieu-nhieu-hon-so-diem-du-lieu"></a></p>

<h3 id="31-sá»‘-chiá»u-dá»¯-liá»‡u-nhiá»u-hÆ¡n-sá»‘-Ä‘iá»ƒm-dá»¯-liá»‡u">3.1. Sá»‘ chiá»u dá»¯ liá»‡u nhiá»u hÆ¡n sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u</h3>

<p>ÄÃ³ lÃ  trÆ°á»ng há»£p \(D &gt; N\), tá»©c ma tráº­n dá»¯ liá»‡u \(\mathbf{X}\) lÃ  má»™t â€˜ma tráº­n caoâ€™. Khi Ä‘Ã³, sá»‘ trá»‹ riÃªng khÃ¡c khÃ´ng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai \(\mathbf{S}\) sáº½ khÃ´ng vÆ°á»£t quÃ¡ rank cá»§a nÃ³, tá»©c khÃ´ng vÆ°á»£t quÃ¡ \(N\). Váº­y ta cáº§n chá»n \(K \leq N\) vÃ¬ khÃ´ng thá»ƒ chá»n ra Ä‘Æ°á»£c \(K &gt; N\) trá»‹ riÃªng khÃ¡c 0 cá»§a má»™t ma tráº­n cÃ³ rank báº±ng \(N\).</p>

<p>Viá»‡c tÃ­nh toÃ¡n cÃ¡c trá»‹ riÃªng vÃ  vector riÃªng cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n má»™t cÃ¡ch hiá»‡u quáº£ dá»±a trÃªn cÃ¡c tÃ­nh cháº¥t sau Ä‘Ã¢y:</p>

<p><strong>TÃ­nh cháº¥t 1:</strong> Trá»‹ riÃªng cá»§a \(\mathbf{A}\) cÅ©ng lÃ  trá»‹ riÃªng cá»§a \(k\mathbf{A}\) vá»›i \(k \neq 0\) báº¥t ká»³. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c suy ra trá»±c tiáº¿p tá»« Ä‘á»‹nh nghÄ©a cá»§a trá»‹ riÃªng vÃ  vector riÃªng.</p>

<p><strong>TÃ­nh chÃ¢t 2:</strong> Trá»‹ riÃªng cá»§a \(\mathbf{AB}\) cÅ©ng lÃ  trá»‹ riÃªng cá»§a \(\mathbf{BA}\) vá»›i \(\mathbf{A} \in \mathbb{R}^{d_1 \times d_2}, \mathbf{B} \in \mathbb{R} ^{d_2 \times d_1}\) lÃ  cÃ¡c ma tráº­n báº¥t ká»³ vÃ  \(d_1, d_2\) lÃ  cÃ¡c sá»‘ tá»± nhiÃªn khÃ¡c khÃ´ng báº¥t ká»³. TÃ´i xin khÃ´ng chá»©ng minh quan sÃ¡t nÃ y.</p>

<p>NhÆ° váº­y, thay vÃ¬ tÃ¬m trá»‹ riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai \(\mathbf{S} \in \mathbb{R}^{D\times D}\), ta Ä‘i tÃ¬m trá»‹ riÃªng cá»§a ma tráº­n \(\mathbf{T} = \mathbf{X}^T \mathbf{X} \in \mathbb{R}^{N \times N}\) cÃ³ sá»‘ chiá»u nhá» hÆ¡n (vÃ¬ \(N &lt; D\)).</p>

<p><strong>TÃ­nh cháº¥t 3:</strong> Giáº£ sá»­ \((\lambda, \mathbf{u})\) lÃ  má»™t cáº·p trá»‹ riÃªng - vector riÃªng cá»§a \(\mathbf{T}\), tháº¿ thÃ¬ \((\lambda, \mathbf{Xu})\) lÃ  má»™t cáº·p trá»‹ riÃªng - vector riÃªng cá»§a \(\mathbf{S}\).</p>

<p>Tháº­t váº­y:
\[
\begin{eqnarray}
  \mathbf{X}^T \mathbf{Xu} &amp;=&amp; \lambda \mathbf{u}&amp; \quad (7) \newline
  \Rightarrow (\mathbf{X}\mathbf{X}^T)(\mathbf{Xu}) &amp;=&amp; \lambda \mathbf{Xu} &amp; \quad (8)
\end{eqnarray}
\]</p>

<p>Biá»ƒu thá»©c \((7)\) lÃ  theo Ä‘á»‹nh nghÄ©a cá»§a trá»‹ riÃªng vÃ  vector riÃªng. Biá»ƒu thá»©c \((8)\) thu Ä‘Æ°á»£c tá»« \((7)\) báº±ng cÃ¡ch nhÃ¢n bÃªn trÃ¡i cáº£ hai váº¿ vá»›i ma tráº­n \(\mathbf{X}\). Tá»« \((8)\) ta suy ra <strong>Quan sÃ¡t 3</strong>.</p>

<p>NhÆ° váº­y, ta cÃ³ thá»ƒ hoÃ n toÃ n tÃ­nh Ä‘Æ°á»£c trá»‹ riÃªng vÃ  vector riÃªng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai dá»±a trÃªn má»™t ma tráº­n nhá» hÆ¡n.</p>

<p><a name="-chuan-hoa-cac-vector-rieng"></a></p>

<h4 id="32-chuáº©n-hoÃ¡-cÃ¡c-vector-riÃªng">3.2. Chuáº©n hoÃ¡ cÃ¡c vector riÃªng</h4>

<p><em>Nháº¯c láº¡i Ä‘á»‹nh nghÄ©a khÃ´ng gian riÃªng: KhÃ´ng gian riÃªng á»©ng vá»›i trá»‹ riÃªng cá»§a má»™t ma tráº­n lÃ  khÃ´ng gian sinh (span subspace) táº¡o bá»Ÿi toÃ n bá»™ cÃ¡c vector riÃªng á»©ng vá»›i trá»‹ riÃªng Ä‘Ã³.</em></p>

<p>Viá»‡c cuá»‘i cÃ¹ng pháº£i lÃ m lÃ  chuáº©n hoÃ¡ cÃ¡c vector riÃªng tÃ¬m Ä‘Æ°á»£c sao cho chÃºng táº¡o thÃ nh má»™t há»‡ trá»±c chuáº©n. Viá»‡c nÃ y cÃ³ thá»ƒ dá»±a trÃªn hai Ä‘iá»ƒm sau Ä‘Ã¢y:</p>

<p><strong>Thá»© nháº¥t</strong>, náº¿u \(\mathbf{A}\) lÃ  má»™t ma tráº­n Ä‘á»‘i xá»©ng, \((\lambda_1, \mathbf{x}_1), (\lambda_2, \mathbf{x}_2)\) lÃ  cÃ¡c cÄƒp trá»‹ riÃªng - vector riÃªng cá»§a \(\mathbf{A}\) vá»›i \(\lambda_1 \neq \lambda_2\), tháº¿ thÃ¬ \(\mathbf{x}_1^T\mathbf{x}_2 = 0\). NÃ³i cÃ¡ch khÃ¡c, hai vector báº¥t ká»³ trong hai khÃ´ng gian riÃªng khÃ¡c nhau cá»§a má»™t ma tráº­n Ä‘á»‘i xá»©ng thÃ¬ trá»±c giao vá»›i nhau. Chá»©ng minh cho tÃ­nh cháº¥t nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tháº¥y trong má»™t dÃ²ng dÆ°á»›i Ä‘Ã¢y:
\[
\begin{eqnarray}
  \mathbf{x}_2^T \mathbf{Ax}_1 = \mathbf{x}_1^T \mathbf{Ax}_2 = \lambda_1 \mathbf{x}_2^T \mathbf{x}_1 = \lambda_2 \mathbf{x}_1^T \mathbf{x}_2 \Rightarrow \mathbf{x}_1^T \mathbf{x}_2 = 0
\end{eqnarray}
\]
Dáº¥u báº±ng cuá»‘i cÃ¹ng xáº£y ra vÃ¬ \(\lambda_1 \neq \lambda_2\).</p>

<p><strong>Thá»© hai</strong>, vá»›i cÃ¡c trá»‹ riÃªng Ä‘á»™c láº­p tÃ¬m Ä‘Æ°á»£c trong má»™t khÃ´ng gian riÃªng, ta cÃ³ thá»ƒ dÃ¹ng Gram-Schmit process Ä‘á»ƒ chuáº©n hoÃ¡ chÃºng vá» má»™t há»‡ trá»±c chuáº©n.</p>

<p>Káº¿t há»£p hai Ä‘iá»ƒm trÃªn, ta cÃ³ thá»ƒ thu Ä‘Æ°á»£c cÃ¡c vector riÃªng táº¡o thÃ nh má»™t há»‡ trá»±c chuáº©n, chÃ­nh lÃ  ma tráº­n \(\mathbf{U}_K\) trong PCA.</p>

<p><a name="-voi-cac-bai-toan-large-scale"></a></p>

<h3 id="33-vá»›i-cÃ¡c-bÃ i-toÃ¡n-large-scale">3.3. Vá»›i cÃ¡c bÃ i toÃ¡n large-scale</h3>
<p>Trong ráº¥t nhiá»u bÃ i toÃ¡n, cáº£ \(D\) vÃ  \(N\) Ä‘á»u lÃ  cÃ¡c sá»‘ ráº¥t lá»›n, Ä‘á»“ng nghÄ©a vá»›i viá»‡c ta pháº£i tÃ¬m trá»‹ riÃªng cho má»™t ma tráº­n ráº¥t lá»›n. VÃ­ dá»¥, cÃ³ 1 triá»‡u bá»©c áº£nh 1000 \(\times\) 1000 pixel, nhÆ° váº­y \(D = N = 10^6\) lÃ  má»™t sá»‘ ráº¥t lá»›n, viá»‡c trá»±c tiáº¿p tÃ­nh toÃ¡n trá»‹ riÃªng vÃ  vector riÃªng cho ma tráº­n hiá»‡p phÆ°Æ¡ng sai lÃ  khÃ´ng kháº£ thi. Tuy nhiÃªn, cÃ³ má»™t phÆ°Æ¡ng phÃ¡p cho phÃ©p tÃ­nh xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ nÃ y má»™t cÃ¡ch nhanh hÆ¡n. PhÆ°Æ¡ng phÃ¡p Ä‘Ã³ cÃ³ tÃªn lÃ  <a href="http://www.cs.huji.ac.il/~csip/tirgul2.pdf">Power Method</a>.</p>

<p>PhÆ°Æ¡ng phÃ¡p nÃ y nÃ³i ráº±ng, náº¿u thá»±c hiá»‡n quy trÃ¬nh sau, ta sáº½ tÃ¬m Ä‘Æ°á»£c cáº·p trá»‹ riÃªng vÃ  vector Ä‘áº§u tiÃªn cá»§a má»™t ma tráº­n ná»­a xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng:</p>
<hr />

<p><strong>PhÆ°Æ¡ng phÃ¡p Power tÃ¬m trá»‹ riÃªng vÃ  vector riÃªng cá»§a má»™t ma tráº­n ná»­a xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng \(\mathbf{A} \in \mathbb{R}^{n \times n}\)</strong>:</p>
<ol>
  <li>Chá»n má»™t vector \(\mathbf{q}^{(0)} \in \mathbb{R}^n, ||\mathbf{q}^{(0)}||_2 = 1\) báº¥t ká»³.</li>
  <li>Vá»›i \(k = 1, 2, \dots\), tÃ­nh: \(\mathbf{z} = \mathbf{Aq}^{(k-1)}\).</li>
  <li>Chuáº©n hoÃ¡: \(\mathbf{q}^{(k)} = \mathbf{z} / ||\mathbf{z}||_2\).</li>
  <li>Náº¿u \(||\mathbf{q}^{(k)} - \mathbf{q}^{(k-1)}||_2\) Ä‘á»§ nhá» thÃ¬ dá»«ng láº¡i. Náº¿u khÃ´ng, \(k := k + 1\) rá»“i quay láº¡i BÆ°á»›c 2.</li>
  <li>\(\mathbf{q}^{(k)}\) chÃ­nh lÃ  vector riÃªng á»©ng vá»›i trá»‹ riÃªng lá»›n nháº¥t \(\lambda_1 = (\mathbf{q}^{(k)})^T\mathbf{A}\mathbf{q}^{(k)}\).</li>
</ol>
<hr />

<p>Quy trÃ¬nh nÃ y há»™i tá»¥ khÃ¡ nhanh vÃ  Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh <a href="http://www.cs.huji.ac.il/~csip/tirgul2.pdf">táº¡i Ä‘Ã¢y</a>. Pháº§n chá»©ng minh tÆ°Æ¡ng Ä‘á»‘i Ä‘Æ¡n giáº£n vÃ  khÃ´ng mang láº¡i nhiá»u thÃ´ng tin há»¯u Ã­ch, tÃ´i xin Ä‘Æ°á»£c bá» qua.</p>

<p>Äá»ƒ tÃ¬m vector riÃªng vÃ  trá»‹ riÃªng thá»© hai cá»§a ma tráº­n \(\mathbf{A}\), chÃºng ta dá»±a trÃªn Ä‘á»‹nh lÃ½ sau:</p>
<hr />

<p><strong>Äá»‹nh lÃ½:</strong> Náº¿u ma tráº­n ná»­a xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng \(\mathbf{A}\) cÃ³ cÃ¡c trá»‹ riÃªng \(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n ( \geq 0)\) vÃ  cÃ¡c vector riÃªng tÆ°Æ¡ng á»©ng \(\mathbf{v}_1, \dots, \mathbf{v}_n\), hÆ¡n ná»¯a cÃ¡c vector riÃªng nÃ y táº¡o thÃ nh 1 há»‡ trá»±c chuáº©n, thÃ¬ ma tráº­n:
\[
  \mathbf{B} = \mathbf{A} - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T
\]
cÃ³ cÃ¡c trá»‹ riÃªng \(\lambda_2 \geq \lambda_3 \geq \dots \geq \lambda_n \geq 0\) vÃ  cÃ¡c vector riÃªng tÆ°Æ¡ng á»©ng lÃ  \(\mathbf{v}_2, \mathbf{v}_3, \dots, \mathbf{v}_n, \mathbf{v}_1\).</p>
<hr />

<p>Chá»©ng minh:</p>

<p>Vá»›i \(i = 1\):
\[
\begin{eqnarray}
  \mathbf{Bv}_1 &amp;=&amp; (\mathbf{A} - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T) \mathbf{v}
  &amp;= &amp; \mathbf{Av}_1 - \lambda_1 \mathbf{v}_1 = \mathbf{0} \newline
\end{eqnarray}
\]</p>

<p>Vá»›i \(i &gt; 1\):
\[
\begin{eqnarray}
  \mathbf{Bv}_i &amp;=&amp; (\mathbf{A} - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T)\mathbf{v}_i \newline
  &amp;=&amp; \mathbf{Av}_i - \lambda_1 \mathbf{v}_1 (\mathbf{v}_1^T \mathbf{v}_i) \newline
  &amp;=&amp; \mathbf{Av}_i = \lambda_i \mathbf{v}_i
\end{eqnarray}
\]</p>

<p>NhÆ° váº­y Ä‘á»‹nh lÃ½ Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh.</p>

<p>LÃºc nÃ y, \((\lambda_2, \mathbf{v}_2)\) láº¡i trá»Ÿ thÃ nh cáº·p trá»‹ riÃªng-vector riÃªng lá»›n nháº¥t cá»§a \(\mathbf{B}\). CÃ¡ch tÃ¬m hai biáº¿n sá»‘ nÃ y má»™t láº§n ná»¯a Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng PhÆ°Æ¡ng phÃ¡p Power.</p>

<p>Tiáº¿p tá»¥c quy trÃ¬nh nÃ y, ta sáº½ tÃ¬m Ä‘Æ°á»£c (xáº¥p xá»‰) táº¥t cáº£ cÃ¡c trá»‹ riÃªng vÃ  vector riÃªng tÆ°Æ¡ng á»©ng cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai. CÅ©ng xin lÆ°u Ã½ ráº±ng ta chá»‰ cáº§n tÃ¬m tá»›i trá»‹ riÃªng thá»© \(K\) cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai. CÃ¡ch lÃ m nÃ y trÃªn thá»±c táº¿ Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t nhiá»u.</p>

<p>PhÆ°Æ¡ng phÃ¡p Power cÃ²n lÃ  thuáº­t toÃ¡n cÆ¡ báº£n trong <a href="https://en.wikipedia.org/wiki/PageRank">Google PageRank</a> giÃºp sáº¯p xáº¿p cÃ¡c website theo má»©c Ä‘á»™ phá»• biáº¿n giáº£m dáº§n. PageRank chÃ­nh lÃ  ná»n mÃ³ng cá»§a Google; ngÃ y nay, viá»‡c tÃ¬m kiáº¿m trong Google sá»­ dá»¥ng nhiá»u thuáº­t toÃ¡n nÃ¢ng cao hÆ¡n PageRank. TÃ´i sáº½ cÃ³ má»™t bÃ i riÃªng vá» Google PageRank sau khi nÃ³i vá» Chuá»—i Markov vÃ  MÃ´ hÃ¬nh Markov áº©n.</p>

<p><a name="-vi-du-tren-python"></a></p>

<h2 id="4-vÃ­-dá»¥-trÃªn-python">4. VÃ­ dá»¥ trÃªn Python</h2>

<p><a name="-eigenface"></a></p>

<h3 id="41-eigenface">4.1. Eigenface</h3>
<p><a href="https://en.wikipedia.org/wiki/Eigenface">Eigenface</a> lÃ  má»™t trong cÃ¡c phÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t trong bÃ i toÃ¡n nháº­n dáº¡ng khuÃ´n máº·t. Ã tÆ°á»Ÿng cá»§a Eigenface lÃ  Ä‘i tÃ¬m má»™t khÃ´ng gian cÃ³ sá»‘ chiá»u nhá» hÆ¡n Ä‘á»ƒ mÃ´ táº£ má»—i khuÃ´n máº·t, tá»« Ä‘Ã³ sá»­ dá»¥ng vector trong khÃ´ng gian tháº¥p nÃ y nhÆ° lÃ  feature vector cho viá»‡c thá»±c hiá»‡n classification. Äiá»u Ä‘Ã¡ng nÃ³i lÃ  má»™t bá»©c áº£nh khuÃ´n máº·t cÃ³ kÃ­ch thÆ°á»›c khoáº£ng 200 \(\times\) 200 sáº½ cÃ³ sá»‘ chiá»u lÃ  40k - lÃ  má»™t sá»‘ cá»±c lá»›n, trong khi Ä‘Ã³, feature vector thÆ°á»ng chá»‰ cÃ³ sá»‘ chiá»u báº±ng vÃ i trÄƒm.</p>

<p>Eigenface thá»±c ra chÃ­nh lÃ  PCA. CÃ¡c Eigenfaces chÃ­nh lÃ  cÃ¡c eigenvectors á»©ng vá»›i cÃ¡c trá»‹ riÃªng lá»›n nháº¥t cá»§a ma tráº­n hiá»‡p phÆ°Æ¡ng sai.</p>

<p>Trong pháº§n nÃ y, chÃºng ta cÃ¹ng lÃ m má»™t thÃ­ nghiá»‡m nhá» trÃªn cÆ¡ sá»Ÿ dá»¯ liá»‡u <a href="http://vismod.media.mit.edu/vismod/classes/mas622-00/datasets/">Yale face database</a>. CÃ¡c bá»©c áº£nh trong thÃ­ nghiá»‡m nÃ y Ä‘Ã£ Ä‘Æ°á»£c cÄƒn chá»‰nh cho cÃ¹ng vá»›i kÃ­ch thÆ°á»›c vÃ  khuÃ´n máº·t náº±m trá»n váº¹n trong má»™t hÃ¬nh chá»¯ nháº­t cÃ³ kÃ­ch thÆ°á»›c \(116 \times  98\) pixel. CÃ³ táº¥t cáº£ 15 ngÆ°á»i khÃ¡c nhau, má»—i ngÆ°á»i cÃ³ 11 bá»©c áº£nh Ä‘Æ°á»£c chá»¥p á»Ÿ cÃ¡c Ä‘iá»u kiá»‡n Ã¡nh sÃ¡ng vÃ  cáº£m xÃºc khÃ¡c nhau, bao gá»“m: â€˜centerlightâ€™, â€˜glassesâ€™, â€˜happyâ€™, â€˜leftlightâ€™, â€˜noglassesâ€™, â€˜normalâ€™, â€˜rightlightâ€™,â€™sadâ€™, â€˜sleepyâ€™, â€˜surprisedâ€™, vÃ  â€˜winkâ€™.</p>

<p>HÃ¬nh 1 dÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡c bá»©c áº£nh cá»§a ngÆ°á»i cÃ³ id lÃ  10.</p>
<hr />

<div class="imgcap">
<img src="/assets/28_pca2/yaleb_exs.png" align="center" width="800" />
</div>

<div class="thecap" align="left">HÃ¬nh 1: VÃ­ dá»¥ vá» áº£nh cá»§a má»™t ngÆ°á»i trong Yale Face Database. </div>
<hr />

<p>Ta cÃ³ thá»ƒ tháº¥y ráº±ng sá»‘ chiá»u dá»¯ liá»‡u lÃ  \(116 \times 98 = 11368\) lÃ  má»™t sá»‘ khÃ¡ lá»›n. Tuy nhiÃªn, vÃ¬ chá»‰ cÃ³ tá»•ng cá»™ng \(15 \times 11 = 165\) bá»©c áº£nh nÃªn ta cÃ³ thá»ƒ nÃ©n cÃ¡c bá»©c áº£nh nÃ y vá» dá»¯ liá»‡u má»›i cÃ³ chiá»u nhá» hÆ¡n 165. Trong vÃ­ dá»¥ nÃ y, tÃ´i chá»n \(K = 100\).</p>

<p>DÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n code thá»±c hiá»‡n PCA cho toÃ n bá»™ dá»¯ liá»‡u. ChÃº Ã½ ráº±ng tÃ´i sá»­ dá»¥ng thÆ° viá»‡n <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">misc</span>                     <span class="c1"># for loading image
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># filename structure
</span><span class="n">path</span> <span class="o">=</span> <span class="s">'unpadded/'</span> <span class="c1"># path to the database
</span><span class="n">ids</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="c1"># 15 persons
</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s">'centerlight'</span><span class="p">,</span> <span class="s">'glasses'</span><span class="p">,</span> <span class="s">'happy'</span><span class="p">,</span> <span class="s">'leftlight'</span><span class="p">,</span>
          <span class="s">'noglasses'</span><span class="p">,</span> <span class="s">'normal'</span><span class="p">,</span> <span class="s">'rightlight'</span><span class="p">,</span><span class="s">'sad'</span><span class="p">,</span>
          <span class="s">'sleepy'</span><span class="p">,</span> <span class="s">'surprised'</span><span class="p">,</span> <span class="s">'wink'</span> <span class="p">]</span>
<span class="n">prefix</span> <span class="o">=</span> <span class="s">'subject'</span>
<span class="n">surfix</span> <span class="o">=</span> <span class="s">'.pgm'</span>

<span class="c1"># data dimension
</span><span class="n">h</span> <span class="o">=</span> <span class="mi">116</span> <span class="c1"># hight
</span><span class="n">w</span> <span class="o">=</span> <span class="mi">98</span> <span class="c1"># width
</span><span class="n">D</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">*</span><span class="mi">15</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># collect all data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">person_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="n">prefix</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">person_id</span><span class="p">).</span><span class="n">zfill</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.'</span> <span class="o">+</span> <span class="n">state</span> <span class="o">+</span> <span class="n">surfix</span>
        <span class="n">X</span><span class="p">[:,</span> <span class="n">cnt</span><span class="p">]</span> <span class="o">=</span> <span class="n">misc</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="n">fn</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Doing PCA, note that each row is a datapoint
</span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">)</span> <span class="c1"># K = 100
</span><span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># projection matrix
</span><span class="n">U</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">.</span><span class="n">T</span>
</code></pre></div></div>

<p>ChÃº Ã½ ráº±ng cÃ¡c hÃ m cá»§a <code class="language-plaintext highlighter-rouge">sklearn</code> Ä‘á»u chá»n dá»¯ liá»‡u á»Ÿ dáº¡ng hÃ ng. CÃ²n tÃ´i thÆ°á»ng chá»n dá»¯ liá»‡u á»Ÿ dáº¡ng cá»™t cho thuáº­n tiá»‡n trong biá»ƒu diá»…n toÃ¡n há»c. TrÆ°á»›c khi sá»­ dá»¥ng <code class="language-plaintext highlighter-rouge">sklearn</code>, báº¡n Ä‘á»c chÃº Ã½ chuyá»ƒn vá»‹ ma tráº­n dá»¯ liá»‡u.</p>

<p>Trong dÃ²ng <code class="language-plaintext highlighter-rouge">pca = PCA(n_components=K)</code>, náº¿u <code class="language-plaintext highlighter-rouge">n_components</code> lÃ  má»™t sá»‘ thá»±c trong khoáº£ng \((0, 1)\), PCA sáº½ thá»±c hiá»‡n viá»‡c tÃ¬m \(K\) dá»±a trÃªn biá»ƒu thá»©c \((6)\).</p>

<p>HÃ¬nh 2 dÆ°á»›i Ä‘Ã¢y biá»ƒu diá»…n 18 vector riÃªng Ä‘áº§u tiÃªn tÃ¬m Ä‘Æ°á»£c báº±ng PCA. CÃ¡c vector tÃ¬m Ä‘Æ°á»£c á»Ÿ dáº¡ng vector cá»™t, ta cáº§n pháº£i <code class="language-plaintext highlighter-rouge">reshape</code> chÃºng Ä‘á»ƒ cÃ³ thá»ƒ minh hoáº¡ nhÆ° cÃ¡c bá»©c áº£nh.</p>

<hr />

<div class="imgcap">
<img src="/assets/28_pca2/yaleb_eig.png" align="center" width="800" />
</div>

<div class="thecap" align="left">HÃ¬nh 2: CÃ¡c eigenfaces tÃ¬m Ä‘Æ°á»£c báº±ng PCA. </div>
<hr />

<p>CÃ³ má»™t Ä‘iá»u dá»… nháº­n ra lÃ  cÃ¡c áº£nh minh hoáº¡ cÃ¡c vector thu Ä‘Æ°á»£c Ã­t nhiá»u mang thÃ´ng tin cá»§a máº·t ngÆ°á»i. Thá»±c táº¿, má»™t khuÃ´n máº·t gá»‘c sáº½ Ä‘Æ°á»£c xáº¥p xá»‰ nhÆ° tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c <em>khuÃ´n máº·t</em> nÃ y. VÃ¬ cÃ¡c vector riÃªng nÃ y Ä‘Ã³ng vai trÃ² nhÆ° cÆ¡ sá»Ÿ cá»§a khÃ´ng gian má»›i vá»›i Ã­t chiá»u hÆ¡n, chÃºng cÃ²n Ä‘Æ°á»£c gá»i lÃ  <em>khuÃ´n máº·t riÃªng</em>, tá»©c <em>eigenfaces</em>.</p>

<p>Äá»ƒ xem má»©c Ä‘á»™ hiá»‡u quáº£ cá»§a Eigenfaces nhÆ° tháº¿ nÃ o, chÃºng ta thá»­ minh hoáº¡ cÃ¡c bá»©c áº£nh gá»‘c vÃ  cÃ¡c bá»©c áº£nh Ä‘Æ°á»£c xáº¥p xá»‰ báº±ng PCA, káº¿t quáº£ Ä‘Æ°á»£c cho nhÆ° HÃ¬nh 3 dÆ°á»›i Ä‘Ã¢y:
<!-- ============ --></p>
<hr />

<div class="imgcap">
<img src="/assets/28_pca2/yaleb_ori_res.png" align="center" width="800" />
</div>

<div class="thecap" align="left">HÃ¬nh 3: HÃ ng trÃªn: cÃ¡c áº£nh gá»‘c. HÃ ng dÆ°á»›i: cÃ¡c áº£nh Ä‘Æ°á»£c <em>suy ra</em> tá»« eigenfaces. áº¢nh á»Ÿ hÃ ng dÆ°á»›i cÃ³ nhiá»u nhiá»…u nhÆ°ng váº«n mang nhá»¯ng Ä‘áº·c Ä‘iá»ƒm riÃªng mÃ  máº¯t ngÆ°á»i cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c. </div>
<hr />

<p>NhÆ° váº­y, vector vá»›i sá»‘ chiá»u \(K = 100\) trong khÃ´ng gian má»›i mang <em>khÃ¡</em> Ä‘áº§y Ä‘á»§ thÃ´ng tin cá»§a vector cÃ³ sá»‘ chiá»u \(D = 11368\) trong khÃ´ng gian ban Ä‘áº§u.</p>

<p>Pháº§n cÃ²n láº¡i cá»§a source code cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/28_pca2/python/EigenFaces.ipynb">táº¡i Ä‘Ã¢y</a>.</p>

<p><a name="--unsupervised-abnormal-detection"></a></p>

<h3 id="42--unsupervised-abnormal-detection">4.2.  Unsupervised Abnormal Detection</h3>
<p>NgoÃ i cÃ¡c á»©ng dá»¥ng vá» nÃ©n vÃ  classification, PCA cÃ²n Ä‘Æ°á»£c sá»­ dá»¥ng trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau. Abnormal Detection (dÃ² tÃ¬m cÃ¡c hiá»‡n tÆ°á»£ng khÃ´ng bÃ¬nh thÆ°á»ng) lÃ  má»™t trong sá»‘ Ä‘Ã³. ThÃªm ná»¯a, giáº£ sá»­ chÃºng ta khÃ´ng biáº¿t nhÃ£n cá»§a cÃ¡c sá»± kiá»‡n nÃ y, tá»©c ta Ä‘ang lÃ m viá»‡c vá»›i má»™t bÃ i toÃ¡n Unsupervised.</p>

<p>Ã tÆ°á»Ÿng cÆ¡ báº£n lÃ  cÃ¡c sá»± kiá»‡n â€˜normalâ€™ thÆ°á»ng náº±m gáº§n má»™t khÃ´ng gian con nÃ o Ä‘Ã³, trong khi cÃ¡c sá»± kiá»‡n â€˜abnormalâ€™ thÆ°á»ng khÃ¡c biá»‡t vá»›i cÃ¡c sá»± kiá»‡n â€˜normalâ€™, tá»©c náº±m xa khÃ´ng gian con Ä‘Ã³. HÆ¡n ná»¯a, vÃ¬ lÃ  â€˜abnormalâ€™ nÃªn sá»‘ lÆ°á»£ng cÃ¡c sá»± kiá»‡n thuá»™c loáº¡i nÃ y lÃ  ráº¥t nhá» so vá»›i â€˜normalâ€™.</p>

<p>NhÆ° váº­y, chÃºng ta cÃ³ thá»ƒ lÃ m PCA trÃªn toÃ n bá»™ dá»¯ liá»‡u Ä‘á»ƒ tÃ¬m ra cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a dá»¯ liá»‡u, tá»« Ä‘Ã³ suy ra khÃ´ng gian con mÃ  cÃ¡c Ä‘iá»ƒm â€˜normalâ€™ náº±m gáº§n. Viá»‡c xÃ¡c Ä‘á»‹nh má»™t Ä‘iá»ƒm lÃ  â€˜normalâ€™ hay â€˜abnoralâ€™ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch Ä‘o khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm Ä‘Ã³ tá»›i khÃ´ng gian con tÃ¬m Ä‘Æ°á»£c.</p>

<p>HÃ¬nh 4 dÆ°á»›i Ä‘Ã¢y minh hoáº¡ cho viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c sá»± kiá»‡n khÃ´ng bÃ¬nh thÆ°á»ng.</p>

<hr />

<div>
<table width="100%" style="border: 0px solid white">

    <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/28_pca2/abnormal.png" />
         </td>
        <td width="40%" style="border: 0px solid white" align="justify">
        HÃ¬nh 4: PCA cho viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c sá»± kiá»‡n 'abnormal' vá»›i giáº£ sá»­ ráº±ng cÃ¡c sá»± kiá»‡n 'normal' chiáº¿m Ä‘a sá»‘ vÃ  náº±m gáº§n trong má»™t khÃ´ng gian con nÃ o Ä‘Ã³. Khi Ä‘Ã³, náº¿u lÃ m PCA trÃªn toÃ n bá»™ dá»¯ liá»‡u, khÃ´ng gian con thu Ä‘Æ°á»£c gáº§n vá»›i khÃ´ng gian con cá»§a táº­p cÃ¡c sá»± kiá»‡n 'normal'. LÃºc nÃ y, cÃ¡c Ä‘iá»ƒm quÃ¡ xa khÃ´ng gian con nÃ y, trong trÆ°á»ng há»£p nÃ y lÃ  cÃ¡c Ä‘iá»ƒm mÃ u cam, cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  cÃ¡c sá»± kiá»‡n 'abnormal'.
        </td>
    </tr>
</table>
</div>
<hr />

<p>Má»™t á»©ng dá»¥ng cá»§a viá»‡c nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong bÃ i bÃ¡o: <a href="http://www.cs.bu.edu/fac/crovella/paper-archive/sigc04-network-wide-anomalies.pdf">Diagnosing Network-Wide Traffic Anomalies</a>.</p>

<p><a name="-thao-luan"></a></p>

<h2 id="5-tháº£o-luáº­n">5. Tháº£o luáº­n</h2>

<ul>
  <li>
    <p>PCA lÃ  má»™t phÆ°Æ¡ng phÃ¡p Unsupervised. Viá»‡c thá»±c hiá»‡n PCA trÃªn toÃ n bá»™ dá»¯ liá»‡u khÃ´ng phá»¥ thuá»™c vÃ o class(náº¿u cÃ³) cá»§a má»—i dá»¯ liá»‡u. Viá»‡c nÃ y Ä‘Ã´i khi khiáº¿n cho PCA khÃ´ng mang láº¡i hiá»‡u quáº£ cho cÃ¡c bÃ i toÃ¡n classification. Tháº­t váº­y, giáº£ sá»­ trong khÃ´ng gian hai chiá»u, 2 classes phÃ¢n bá»‘ dá»c hai bÃªn cá»§a 1 Ä‘Æ°á»ng tháº³ng. NhÆ° váº­y, PCA nhiá»u kháº£ nÄƒng sáº½ cho chÃºng ta giá»¯ láº¡i thÃ nh pháº§n chÃ­nh chÃ­nh lÃ  Ä‘Æ°á»ng tháº³ng Ä‘Ã³. Khi chiáº¿u dá»¯ liá»‡u lÃªn Ä‘Æ°á»ng tháº³ng nÃ y, cáº£ hai classes bá»‹ trá»™n láº«n vÃ o nhau, khiáº¿n cho viá»‡c classification Ä‘áº¡t káº¿t quáº£ tháº¥p. CÃ³ má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng tá»± nhÆ° PCA giÃºp táº­n dá»¥ng thÃ´ng tin vá» cÃ¡c class Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chiáº¿u theo chiá»u nÃ o, phÆ°Æ¡ng phÃ¡p Ä‘Ã³ cÃ³ tÃªn lÃ  <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis</a>, sáº½ Ä‘Æ°á»£c tháº£o luáº­n trong bÃ i tiáº¿p theo.</p>
  </li>
  <li>
    <p>Vá»›i cÃ¡c bÃ i toÃ¡n Large-scale, Ä‘Ã´i khi viá»‡c tÃ­nh toÃ¡n trÃªn toÃ n bá»™ dá»¯ liá»‡u lÃ  khÃ´ng kháº£ thi vÃ¬ cÃ²n cÃ³ váº¥n Ä‘á» vá» bá»™ nhá»›. Giáº£i phÃ¡p lÃ  thá»±c hiá»‡n PCA láº§n Ä‘áº§u trÃªn má»™t táº­p con dá»¯ liá»‡u vá»«a vá»›i bá»™ nhá»›, sau Ä‘Ã³ láº¥y má»™t táº­p con khÃ¡c Ä‘á»ƒ (incrementally) cáº­p nháº­t nghiá»‡m cá»§a PCA tá»›i khi nÃ o há»™i tá»¥. Ã tÆ°á»Ÿng nÃ y khÃ¡ giá»‘ng vá»›i <a href="http://machinelearningcoban.com/2017/01/16/gradientdescent2/#-mini-batch-gradient-descent">Mini-batch Gradient Descent</a>, vÃ  Ä‘Æ°á»£c gá»i lÃ  <a href="http://cseweb.ucsd.edu/~dasgupta/papers/incremental-pca.pdf">Incremental PCA</a>.</p>
  </li>
  <li>
    <p>NgoÃ i ra, cÃ²n ráº¥t nhiá»u hÆ°á»›ng má»Ÿ rá»™ng cá»§a PCA, báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m kiáº¿m theo tá»« khoÃ¡: Sparse PCA, Kernel PCA, Robust PCA. TÃ´i sáº½ Ä‘á» cáº­p tá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y khi cÃ³ dá»‹p.</p>
  </li>
</ul>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-tÃ i-liá»‡u-tham-kháº£o">6. TÃ i liá»‡u tham kháº£o</h2>
<p>[1] <a href="https://www.youtube.com/watch?v=F-nfsSq42ow">PCA, SVD</a></p>

<p>[2] <a href="https://en.wikipedia.org/wiki/Eigenface">Eigenface</a></p>

<p>[3] <a href="http://cseweb.ucsd.edu/~dasgupta/papers/incremental-pca.pdf">The Fast Convergence of Incremental PCA</a></p>

<p>[4] <a href="http://www.cs.bu.edu/fac/crovella/paper-archive/sigc04-network-wide-anomalies.pdf">Diagnosing Network-Wide Traffic  Anomalies</a></p>

:ET