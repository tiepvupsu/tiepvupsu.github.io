I"ê<p>Trong b√†i n√†y, t√¥i s·∫Ω gi·ªõi thi·ªáu m·ªôt trong nh·ªØng thu·∫≠t to√°n c∆° b·∫£n nh·∫•t (v√† ƒë∆°n gi·∫£n nh·∫•t) c·ªßa Machine Learning. ƒê√¢y l√† m·ªôt thu·∫≠t to√°n <em>Supervised learning</em> c√≥ t√™n <strong>Linear Regression</strong> (H·ªìi Quy Tuy·∫øn T√≠nh). B√†i to√°n n√†y ƒë√¥i khi ƒë∆∞·ª£c g·ªçi l√† <em>Linear Fitting</em> (trong th·ªëng k√™) ho·∫∑c <em>Linear Least Square</em>.
<strong>Trong trang n√†y:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Gi·ªõi thi·ªáu</a></li>
  <li><a href="#-phan-tich-toan-hoc">2. Ph√¢n t√≠ch to√°n h·ªçc</a>
    <ul>
      <li><a href="#-dang-cua-linear-regression">2.1. D·∫°ng c·ªßa Linear Regression</a></li>
      <li><a href="#-sai-so-du-doan">2.2. Sai s·ªë d·ª± ƒëo√°n</a></li>
      <li><a href="#-ham-mat-mat">2.3. H√†m m·∫•t m√°t</a></li>
      <li><a href="#-nghiem-cho-bai-toan-linear-regression">2.4. Nghi·ªám cho b√†i to√°n Linear Regression</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-tren-python">3. V√≠ d·ª• tr√™n Python</a>
    <ul>
      <li><a href="#-bai-toan">3.1. B√†i to√°n</a></li>
      <li><a href="#-hien-thi-du-lieu-tren-do-thi">3.2. Hi·ªÉn th·ªã d·ªØ li·ªáu tr√™n ƒë·ªì th·ªã</a></li>
      <li><a href="#-nghiem-theo-cong-thuc">3.3. Nghi·ªám theo c√¥ng th·ª©c</a></li>
      <li><a href="#-nghiem-theo-thu-vien-scikit-learn">3.4. Nghi·ªám theo th∆∞ vi·ªán scikit-learn</a></li>
    </ul>
  </li>
  <li><a href="#-thao-luan">4. Th·∫£o lu·∫≠n</a>
    <ul>
      <li><a href="#-cac-bai-toan-co-the-giai-bang-linear-regression">4.1. C√°c b√†i to√°n c√≥ th·ªÉ gi·∫£i b·∫±ng Linear Regression</a></li>
      <li><a href="#-han-che-cua-linear-regression">4.2. H·∫°n ch·∫ø c·ªßa Linear Regression</a></li>
      <li><a href="#-cac-phuong-phap-toi-uu">4.3. C√°c ph∆∞∆°ng ph√°p t·ªëi ∆∞u</a></li>
    </ul>
  </li>
  <li><a href="#-tai-lieu-tham-khao">5. T√†i li·ªáu tham kh·∫£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<!-- ========================== New Heading ==================== -->
<p><a name="-gioi-thieu"></a></p>

<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>

<p>Quay l·∫°i <a href="/2016/12/27/categories/#regression-hoi-quy">v√≠ d·ª• ƒë∆°n gi·∫£n ƒë∆∞·ª£c n√™u trong b√†i tr∆∞·ªõc</a>: m·ªôt cƒÉn nh√† r·ªông \(x_1 ~ \text{m}^2\), c√≥ \(x_2\) ph√≤ng ng·ªß v√† c√°ch trung t√¢m th√†nh ph·ªë \(x_3~ \text{km}\) c√≥ gi√° l√† bao nhi√™u. Gi·∫£ s·ª≠ ch√∫ng ta ƒë√£ c√≥ s·ªë li·ªáu th·ªëng k√™ t·ª´ 1000 cƒÉn nh√† trong th√†nh ph·ªë ƒë√≥, li·ªáu r·∫±ng khi c√≥ m·ªôt cƒÉn nh√† m·ªõi v·ªõi c√°c th√¥ng s·ªë v·ªÅ di·ªán t√≠ch, s·ªë ph√≤ng ng·ªß v√† kho·∫£ng c√°ch t·ªõi trung t√¢m, ch√∫ng ta c√≥ th·ªÉ d·ª± ƒëo√°n ƒë∆∞·ª£c gi√° c·ªßa cƒÉn nh√† ƒë√≥ kh√¥ng? N·∫øu c√≥ th√¨ h√†m d·ª± ƒëo√°n \(y = f(\mathbf{x}) \) s·∫Ω c√≥ d·∫°ng nh∆∞ th·∫ø n√†o. ·ªû ƒë√¢y \(\mathbf{x} = [x_1, x_2, x_3] \) l√† m·ªôt vector h√†ng ch·ª©a th√¥ng tin <em>input</em>, \(y\) l√† m·ªôt s·ªë v√¥ h∆∞·ªõng (scalar) bi·ªÉu di·ªÖn <em>output</em> (t·ª©c gi√° c·ªßa cƒÉn nh√† trong v√≠ d·ª• n√†y).</p>

<p><strong>L∆∞u √Ω v·ªÅ k√Ω hi·ªáu to√°n h·ªçc:</strong> <em>trong c√°c b√†i vi·∫øt c·ªßa t√¥i, c√°c s·ªë v√¥ h∆∞·ªõng ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi c√°c ch·ªØ c√°i vi·∫øt ·ªü d·∫°ng kh√¥ng in ƒë·∫≠m, c√≥ th·ªÉ vi·∫øt hoa, v√≠ d·ª• \(x_1, N, y, k\). C√°c vector ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√°c ch·ªØ c√°i th∆∞·ªùng in ƒë·∫≠m, v√≠ d·ª• \(\mathbf{y}, \mathbf{x}_1 \). C√°c ma tr·∫≠n ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi c√°c ch·ªØ vi·∫øt hoa in ƒë·∫≠m, v√≠ d·ª• \(\mathbf{X, Y, W} \).</em></p>

<p>M·ªôt c√°ch ƒë∆°n gi·∫£n nh·∫•t, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng: i) di·ªán t√≠ch nh√† c√†ng l·ªõn th√¨ gi√° nh√† c√†ng cao; ii) s·ªë l∆∞·ª£ng ph√≤ng ng·ªß c√†ng l·ªõn th√¨ gi√° nh√† c√†ng cao; iii) c√†ng xa trung t√¢m th√¨ gi√° nh√† c√†ng gi·∫£m. M·ªôt h√†m s·ªë ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ m√¥ t·∫£ m·ªëi quan h·ªá gi·ªØa gi√° nh√† v√† 3 ƒë·∫°i l∆∞·ª£ng ƒë·∫ßu v√†o l√†:</p>

<!-- \\[ \text{ gi√° nh√† } \approx w_1 (\text{di·ªán t√≠ch}) + w_2 (\text{s·ªë ph√≤ng}) + w_3 (\text{ kho·∫£ng c√°ch}) + w_0 \\]  -->

<p>\[y \approx  f(\mathbf{x}) = \hat{y}\]
\[f(\mathbf{x}) =w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0 ~~~~ (1)\]
trong ƒë√≥, \(w_1, w_2, w_3, w_0\) l√† c√°c h·∫±ng s·ªë,  \(w_0\) c√≤n ƒë∆∞·ª£c g·ªçi l√† bias. M·ªëi quan h·ªá \(y \approx f(\mathbf{x})\) b√™n tr√™n l√† m·ªôt m·ªëi quan h·ªá tuy·∫øn t√≠nh (linear). B√†i to√°n ch√∫ng ta ƒëang l√†m l√† m·ªôt b√†i to√°n thu·ªôc lo·∫°i regression. B√†i to√°n ƒëi t√¨m c√°c h·ªá s·ªë t·ªëi ∆∞u \( \{w_1, w_2, w_3, w_0 \}\) ch√≠nh v√¨ v·∫≠y ƒë∆∞·ª£c g·ªçi l√† b√†i to√°n Linear Regression.</p>

<p><strong>Ch√∫ √Ω 1:</strong> \(y\) l√† gi√° tr·ªã th·ª±c c·ªßa <em>outcome</em> (d·ª±a tr√™n s·ªë li·ªáu th·ªëng k√™ ch√∫ng ta c√≥ trong t·∫≠p <em>training data</em>), trong khi \(\hat{y}\) l√† gi√° tr·ªã m√† m√¥ h√¨nh Linear Regression d·ª± ƒëo√°n ƒë∆∞·ª£c. Nh√¨n chung, \(y\) v√† \(\hat{y}\) l√† hai gi√° tr·ªã kh√°c nhau do c√≥ sai s·ªë m√¥ h√¨nh, tuy nhi√™n, ch√∫ng ta mong mu·ªën r·∫±ng s·ª± kh√°c nhau n√†y r·∫•t nh·ªè.</p>

<p><strong>Ch√∫ √Ω 2:</strong> <em>Linear</em> hay <em>tuy·∫øn t√≠nh</em> hi·ªÉu m·ªôt c√°ch ƒë∆°n gi·∫£n l√† <em>th·∫≥ng, ph·∫≥ng</em>. Trong kh√¥ng gian hai chi·ªÅu, m·ªôt h√†m s·ªë ƒë∆∞·ª£c g·ªçi l√† <em>tuy·∫øn t√≠nh</em> n·∫øu ƒë·ªì th·ªã c·ªßa n√≥ c√≥ d·∫°ng m·ªôt <em>ƒë∆∞·ªùng th·∫≥ng</em>. Trong kh√¥ng gian ba chi·ªÅu, m·ªôt h√†m s·ªë ƒë∆∞·ª£c goi l√† <em>tuy·∫øn t√≠nh</em> n·∫øu ƒë·ªì th·ªã c·ªßa n√≥ c√≥ d·∫°ng m·ªôt <em>m·∫∑t ph·∫≥ng</em>. Trong kh√¥ng gian nhi·ªÅu h∆°n 3 chi·ªÅu, kh√°i ni·ªám <em>m·∫∑t ph·∫≥ng</em> kh√¥ng c√≤n ph√π h·ª£p n·ªØa, thay v√†o ƒë√≥, m·ªôt kh√°i ni·ªám kh√°c ra ƒë·ªùi ƒë∆∞·ª£c g·ªçi l√† <em>si√™u m·∫∑t ph·∫≥ng</em> (<em>hyperplane</em>). C√°c h√†m s·ªë tuy·∫øn t√≠nh l√† c√°c h√†m ƒë∆°n gi·∫£n nh·∫•t, v√¨ ch√∫ng thu·∫≠n ti·ªán trong vi·ªác h√¨nh dung v√† t√≠nh to√°n. Ch√∫ng ta s·∫Ω ƒë∆∞·ª£c th·∫•y trong c√°c b√†i vi·∫øt sau, <em>tuy·∫øn t√≠nh</em> r·∫•t quan tr·ªçng v√† h·ªØu √≠ch trong c√°c b√†i to√°n Machine Learning. Kinh nghi·ªám c√° nh√¢n t√¥i cho th·∫•y, tr∆∞·ªõc khi hi·ªÉu ƒë∆∞·ª£c c√°c thu·∫≠t to√°n <em>phi tuy·∫øn</em> (non-linear, kh√¥ng ph·∫≥ng), ch√∫ng ta c·∫ßn n·∫Øm v·ªØng c√°c k·ªπ thu·∫≠t cho c√°c m√¥ h√¨nh <em>tuy·∫øn t√≠nh</em>.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-phan-tich-toan-hoc"></a></p>

<h2 id="2-ph√¢n-t√≠ch-to√°n-h·ªçc">2. Ph√¢n t√≠ch to√°n h·ªçc</h2>

<!-- ========================== New Heading ==================== -->
<p><a name="-dang-cua-linear-regression"></a></p>

<h3 id="21-d·∫°ng-c·ªßa-linear-regression">2.1. D·∫°ng c·ªßa Linear Regression</h3>

<p>Trong ph∆∞∆°ng tr√¨nh \((1)\) ph√≠a tr√™n, n·∫øu ch√∫ng ta ƒë·∫∑t \(\mathbf{w} = [w_0, w_1, w_2, w_3]^T = \) l√† vector (c·ªôt) h·ªá s·ªë c·∫ßn ph·∫£i t·ªëi ∆∞u v√† \(\mathbf{\bar{x}} = [1, x_1, x_2, x_3]\) (ƒë·ªçc l√† <em>x bar</em> trong ti·∫øng Anh) l√† vector (h√†ng) d·ªØ li·ªáu ƒë·∫ßu v√†o <em>m·ªü r·ªông</em>. S·ªë \(1\) ·ªü ƒë·∫ßu ƒë∆∞·ª£c th√™m v√†o ƒë·ªÉ ph√©p t√≠nh ƒë∆°n gi·∫£n h∆°n v√† thu·∫≠n ti·ªán cho vi·ªác t√≠nh to√°n. Khi ƒë√≥, ph∆∞∆°ng tr√¨nh (1) c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l·∫°i d∆∞·ªõi d·∫°ng:</p>

<p>\[y \approx \mathbf{\bar{x}}\mathbf{w} = \hat{y}\]</p>

<p>Ch√∫ √Ω r·∫±ng \(\mathbf{\bar{x}}\) l√† m·ªôt vector h√†ng. (<a href="/math/#luu-y-ve-ky-hieu">Xem th√™m v·ªÅ k√Ω hi·ªáu vector h√†ng v√† c·ªôt t·∫°i ƒë√¢y</a>)</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-sai-so-du-doan"></a></p>

<h3 id="22-sai-s·ªë-d·ª±-ƒëo√°n">2.2. Sai s·ªë d·ª± ƒëo√°n</h3>

<p>Ch√∫ng ta mong mu·ªën r·∫±ng s·ª± sai kh√°c \(e\) gi·ªØa gi√° tr·ªã th·ª±c \(y\) v√† gi√° tr·ªã d·ª± ƒëo√°n \(\hat{y}\) (ƒë·ªçc l√† <em>y hat</em> trong ti·∫øng Anh) l√† nh·ªè nh·∫•t. N√≥i c√°ch kh√°c, ch√∫ng ta mu·ªën gi√° tr·ªã sau ƒë√¢y c√†ng nh·ªè c√†ng t·ªët:</p>

<p>\[
\frac{1}{2}e^2 = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(y - \mathbf{\bar{x}}\mathbf{w})^2
\]</p>

<p>trong ƒë√≥ h·ªá s·ªë \(\frac{1}{2} \) (<em>l·∫°i</em>) l√† ƒë·ªÉ thu·∫≠n ti·ªán cho vi·ªác t√≠nh to√°n (khi t√≠nh ƒë·∫°o h√†m th√¨ s·ªë \(\frac{1}{2} \) s·∫Ω b·ªã tri·ªát ti√™u). Ch√∫ng ta c·∫ßn \(e^2\) v√¨ \(e = y - \hat{y} \) c√≥ th·ªÉ l√† m·ªôt s·ªë √¢m, vi·ªác n√≥i \(e\) nh·ªè nh·∫•t s·∫Ω kh√¥ng ƒë√∫ng v√¨ khi \(e = - \infty\) l√† r·∫•t nh·ªè nh∆∞ng s·ª± sai l·ªách l√† r·∫•t l·ªõn. B·∫°n ƒë·ªçc c√≥ th·ªÉ t·ª± ƒë·∫∑t c√¢u h·ªèi: <strong>t·∫°i sao kh√¥ng d√πng tr·ªã tuy·ªát ƒë·ªëi \( |e| \) m√† l·∫°i d√πng b√¨nh ph∆∞∆°ng \(e^2\) ·ªü ƒë√¢y?</strong> C√¢u tr·∫£ l·ªùi s·∫Ω c√≥ ·ªü ph·∫ßn sau.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-ham-mat-mat"></a></p>

<h3 id="23-h√†m-m·∫•t-m√°t">2.3. H√†m m·∫•t m√°t</h3>

<p>ƒêi·ªÅu t∆∞∆°ng t·ª± x·∫£y ra v·ªõi t·∫•t c·∫£ c√°c c·∫∑p <em>(input, outcome)</em> \( (\mathbf{x}_i, y_i), i = 1, 2, \dots, N \), v·ªõi \(N\) l√† s·ªë l∆∞·ª£ng d·ªØ li·ªáu quan s√°t ƒë∆∞·ª£c. ƒêi·ªÅu ch√∫ng ta mu·ªën, t·ªïng sai s·ªë l√† nh·ªè nh·∫•t, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác t√¨m \( \mathbf{w} \) ƒë·ªÉ h√†m s·ªë sau ƒë·∫°t gi√° tr·ªã nh·ªè nh·∫•t:</p>

<p>\[ \mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{\bar{x}_i}\mathbf{w})^2 ~~~~~(2) \]</p>

<p>H√†m s·ªë \(\mathcal{L}(\mathbf{w}) \) ƒë∆∞·ª£c g·ªçi l√† <strong>h√†m m·∫•t m√°t</strong> (loss function) c·ªßa b√†i to√°n Linear Regression. Ch√∫ng ta lu√¥n mong mu·ªën r·∫±ng s·ª± m·∫•t m√°t (sai s·ªë) l√† nh·ªè nh·∫•t, ƒëi·ªÅu ƒë√≥ ƒë·ªìng nghƒ©a v·ªõi vi·ªác  t√¨m vector h·ªá s·ªë \( \mathbf{w} \)  sao cho 
gi√° tr·ªã c·ªßa h√†m m·∫•t m√°t n√†y c√†ng nh·ªè c√†ng t·ªët. Gi√° tr·ªã c·ªßa \(\mathbf{w}\) l√†m cho h√†m m·∫•t m√°t ƒë·∫°t gi√° tr·ªã nh·ªè nh·∫•t ƒë∆∞·ª£c g·ªçi l√† <em>ƒëi·ªÉm t·ªëi ∆∞u</em> (optimal point), k√Ω hi·ªáu:</p>

<p>\[ \mathbf{w}^* = \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w})  \]</p>

<p>Tr∆∞·ªõc khi ƒëi t√¨m l·ªùi gi·∫£i, ch√∫ng ta ƒë∆°n gi·∫£n h√≥a ph√©p to√°n trong ph∆∞∆°ng tr√¨nh h√†m m·∫•t m√°t \((2)\). ƒê·∫∑t \(\mathbf{y} = [y_1; y_2; \dots; y_N]\) l√† m·ªôt vector c·ªôt ch·ª©a t·∫•t c·∫£ c√°c <em>output</em> c·ªßa <em>training data</em>; \( \mathbf{\bar{X}} = [\mathbf{\bar{x}}_1; \mathbf{\bar{x}}_2; \dots; \mathbf{\bar{x}}_N ] \) l√† ma tr·∫≠n d·ªØ li·ªáu ƒë·∫ßu v√†o (m·ªü r·ªông) m√† m·ªói h√†ng c·ªßa n√≥ l√† m·ªôt ƒëi·ªÉm d·ªØ li·ªáu. Khi ƒë√≥ h√†m s·ªë m·∫•t m√°t \(\mathcal{L}(\mathbf{w})\) ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng ma tr·∫≠n ƒë∆°n gi·∫£n h∆°n:</p>

<p>\[
\mathcal{L}(\mathbf{w}) 
= \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{\bar{x}}_i\mathbf{w})^2 \]
\[
= \frac{1}{2} \|\mathbf{y} - \mathbf{\bar{X}}\mathbf{w} \|_2^2 ~~~(3) \]</p>

<p>v·ªõi \( \| \mathbf{z} \|_2 \) l√† Euclidean norm (chu·∫©n Euclid, hay kho·∫£ng c√°ch Euclid), n√≥i c√°ch kh√°c \( \| \mathbf{z} \|_2^2 \) l√† t·ªïng c·ªßa b√¨nh ph∆∞∆°ng m·ªói ph·∫ßn t·ª≠ c·ªßa vector \(\mathbf{z}\). T·ªõi ƒë√¢y, ta ƒë√£ c√≥ m·ªôt d·∫°ng ƒë∆°n gi·∫£n c·ªßa h√†m m·∫•t m√°t ƒë∆∞·ª£c vi·∫øt nh∆∞ ph∆∞∆°ng tr√¨nh \((3)\).</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-nghiem-cho-bai-toan-linear-regression"></a></p>

<h3 id="24-nghi·ªám-cho-b√†i-to√°n-linear-regression">2.4. Nghi·ªám cho b√†i to√°n Linear Regression</h3>

<p><strong>C√°ch ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ t√¨m nghi·ªám cho m·ªôt b√†i to√°n t·ªëi ∆∞u (ch√∫ng ta ƒë√£ bi·∫øt t·ª´ khi h·ªçc c·∫•p 3) l√† gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m (gradient) b·∫±ng 0!</strong> T·∫•t nhi√™n ƒë√≥ l√† khi vi·ªác t√≠nh ƒë·∫°o h√†m v√† vi·ªác gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m b·∫±ng 0 kh√¥ng qu√° ph·ª©c t·∫°p. Th·∫≠t may m·∫Øn, v·ªõi c√°c m√¥ h√¨nh tuy·∫øn t√≠nh, hai vi·ªác n√†y l√† kh·∫£ thi.</p>

<p>ƒê·∫°o h√†m theo \(\mathbf{w} \) c·ªßa h√†m m·∫•t m√°t l√†: 
\[
\frac{\partial{\mathcal{L}(\mathbf{w})}}{\partial{\mathbf{w}}} 
= \mathbf{\bar{X}}^T(\mathbf{\bar{X}}\mathbf{w} - \mathbf{y}) 
\]</p>

<p>C√°c b·∫°n c√≥ th·ªÉ tham kh·∫£o b·∫£ng ƒë·∫°o h√†m theo vector ho·∫∑c ma tr·∫≠n c·ªßa m·ªôt h√†m s·ªë trong <a href="https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf">m·ª•c D.2 c·ªßa t√†i li·ªáu n√†y</a>. <em>ƒê·∫øn ƒë√¢y t√¥i xin quay l·∫°i c√¢u h·ªèi ·ªü ph·∫ßn <a href="#sai so du doan">Sai s·ªë d·ª± ƒëo√°n</a> ph√≠a tr√™n v·ªÅ vi·ªác t·∫°i sao kh√¥ng d√πng tr·ªã tuy·ªát ƒë·ªëi m√† l·∫°i d√πng b√¨nh ph∆∞∆°ng. C√¢u tr·∫£ l·ªùi l√† h√†m b√¨nh ph∆∞∆°ng c√≥ ƒë·∫°o h√†m t·∫°i m·ªçi n∆°i, trong khi h√†m tr·ªã tuy·ªát ƒë·ªëi th√¨ kh√¥ng (ƒë·∫°o h√†m kh√¥ng x√°c ƒë·ªãnh t·∫°i 0)</em>.</p>

<p>Ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m b·∫±ng 0 t∆∞∆°ng ƒë∆∞∆°ng v·ªõi: 
\[
\mathbf{\bar{X}}^T\mathbf{\bar{X}}\mathbf{w} = \mathbf{\bar{X}}^T\mathbf{y} \triangleq \mathbf{b}
~~~ (4)
\]
(k√Ω hi·ªáu \(\mathbf{\bar{X}}^T\mathbf{y} \triangleq \mathbf{b} \) nghƒ©a l√† <em>ƒë·∫∑t</em> \(\mathbf{\bar{X}}^T\mathbf{y}\) <em>b·∫±ng</em> \(\mathbf{b}\) ).</p>

<p>N·∫øu ma tr·∫≠n vu√¥ng \( \mathbf{A} \triangleq \mathbf{\bar{X}}^T\mathbf{\bar{X}}\) kh·∫£ ngh·ªãch (non-singular hay invertible) th√¨ ph∆∞∆°ng tr√¨nh \((4)\) c√≥ nghi·ªám duy nh·∫•t: \( \mathbf{w} = \mathbf{A}^{-1}\mathbf{b}  \).</p>

<p>V·∫≠y n·∫øu ma tr·∫≠n \(\mathbf{A} \) kh√¥ng kh·∫£ ngh·ªãch (c√≥ ƒë·ªãnh th·ª©c b·∫±ng 0) th√¨ sao? N·∫øu c√°c b·∫°n v·∫´n nh·ªõ c√°c ki·∫øn th·ª©c v·ªÅ h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh, trong tr∆∞·ªùng h·ª£p n√†y th√¨ ho·∫∑c ph∆∞∆°ng trinh \( (4) \) v√¥ nghi·ªám, ho·∫∑c l√† n√≥ c√≥ v√¥ s·ªë nghi·ªám. Khi ƒë√≥, ch√∫ng ta s·ª≠ d·ª•ng kh√°i ni·ªám <a href="https://vi.wikipedia.org/wiki/Gi·∫£_ngh·ªãch_ƒë·∫£o_Moore‚ÄìPenrose"><em>gi·∫£ ngh·ªãch ƒë·∫£o</em></a> \( \mathbf{A}^{\dagger} \) (ƒë·ªçc l√† <em>A dagger</em> trong ti·∫øng Anh). (<em>Gi·∫£ ngh·ªãch ƒë·∫£o (pseudo inverse) l√† tr∆∞·ªùng h·ª£p t·ªïng qu√°t c·ªßa ngh·ªãch ƒë·∫£o khi ma tr·∫≠n kh√¥ng kh·∫£ ngh·ªãch ho·∫∑c th·∫≠m ch√≠ kh√¥ng vu√¥ng. Trong khu√¥n kh·ªï b√†i vi·∫øt n√†y, t√¥i xin ph√©p ƒë∆∞·ª£c l∆∞·ª£c b·ªè ph·∫ßn n√†y, n·∫øu c√°c b·∫°n th·ª±c s·ª± quan t√¢m, t√¥i s·∫Ω vi·∫øt m·ªôt b√†i kh√°c ch·ªâ n√≥i v·ªÅ gi·∫£ ngh·ªãch ƒë·∫£o. Xem th√™m: <a href="http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf">Least Squares, Pseudo-Inverses, PCA &amp; SVD</a>.</em>)</p>

<p>V·ªõi kh√°i ni·ªám gi·∫£ ngh·ªãch ƒë·∫£o, ƒëi·ªÉm t·ªëi ∆∞u c·ªßa b√†i to√°n Linear Regression c√≥ d·∫°ng:</p>

<p>\[
\mathbf{w} = \mathbf{A}^{\dagger}\mathbf{b} = (\mathbf{\bar{X}}^T\mathbf{\bar{X}})^{\dagger} \mathbf{\bar{X}}^T\mathbf{y}
~~~ (5)
\]</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-vi-du-tren-python"></a></p>

<h2 id="3-v√≠-d·ª•-tr√™n-python">3. V√≠ d·ª• tr√™n Python</h2>

<!-- ========================== New Heading ==================== -->
<p><a name="-bai-toan"></a></p>

<h3 id="31-b√†i-to√°n">3.1. B√†i to√°n</h3>

<p>Trong ph·∫ßn n√†y, t√¥i s·∫Ω ch·ªçn m·ªôt v√≠ d·ª• ƒë∆°n gi·∫£n v·ªÅ vi·ªác gi·∫£i b√†i to√°n Linear Regression trong Python. T√¥i c≈©ng s·∫Ω so s√°nh nghi·ªám c·ªßa b√†i to√°n khi gi·∫£i theo ph∆∞∆°ng tr√¨nh \((5) \) v√† nghi·ªám t√¨m ƒë∆∞·ª£c khi d√πng th∆∞ vi·ªán <a href="http://scikit-learn.org/stable/">scikit-learn</a> c·ªßa Python. (<em>ƒê√¢y l√† th∆∞ vi·ªán Machine Learning ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong Python</em>). Trong v√≠ d·ª• n√†y, d·ªØ li·ªáu ƒë·∫ßu v√†o ch·ªâ c√≥ 1 gi√° tr·ªã (1 chi·ªÅu) ƒë·ªÉ thu·∫≠n ti·ªán cho vi·ªác minh ho·∫° trong m·∫∑t ph·∫≥ng.</p>

<p>Ch√∫ng ta c√≥ 1 b·∫£ng d·ªØ li·ªáu v·ªÅ chi·ªÅu cao v√† c√¢n n·∫∑ng c·ªßa 15 ng∆∞·ªùi nh∆∞ d∆∞·ªõi ƒë√¢y:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Chi·ªÅu cao (cm)</th>
      <th style="text-align: center">C√¢n n·∫∑ng (kg)</th>
      <th style="text-align: center">Chi·ªÅu cao (cm)</th>
      <th style="text-align: center">C√¢n n·∫∑ng (kg)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">147</td>
      <td style="text-align: center">49</td>
      <td style="text-align: center">168</td>
      <td style="text-align: center">60</td>
    </tr>
    <tr>
      <td style="text-align: center">150</td>
      <td style="text-align: center">50</td>
      <td style="text-align: center">170</td>
      <td style="text-align: center">72</td>
    </tr>
    <tr>
      <td style="text-align: center">153</td>
      <td style="text-align: center">51</td>
      <td style="text-align: center">173</td>
      <td style="text-align: center">63</td>
    </tr>
    <tr>
      <td style="text-align: center">155</td>
      <td style="text-align: center">52</td>
      <td style="text-align: center">175</td>
      <td style="text-align: center">64</td>
    </tr>
    <tr>
      <td style="text-align: center">158</td>
      <td style="text-align: center">54</td>
      <td style="text-align: center">178</td>
      <td style="text-align: center">66</td>
    </tr>
    <tr>
      <td style="text-align: center">160</td>
      <td style="text-align: center">56</td>
      <td style="text-align: center">180</td>
      <td style="text-align: center">67</td>
    </tr>
    <tr>
      <td style="text-align: center">163</td>
      <td style="text-align: center">58</td>
      <td style="text-align: center">183</td>
      <td style="text-align: center">68</td>
    </tr>
    <tr>
      <td style="text-align: center">165</td>
      <td style="text-align: center">59</td>
      <td style="text-align: center">¬†</td>
      <td style="text-align: center">¬†</td>
    </tr>
  </tbody>
</table>

<p>B√†i to√°n ƒë·∫∑t ra l√†: li·ªáu c√≥ th·ªÉ d·ª± ƒëo√°n c√¢n n·∫∑ng c·ªßa m·ªôt ng∆∞·ªùi d·ª±a v√†o chi·ªÅu cao c·ªßa h·ªç kh√¥ng? (<em>Tr√™n th·ª±c t·∫ø, t·∫•t nhi√™n l√† kh√¥ng, v√¨ c√¢n n·∫∑ng c√≤n ph·ª• thu·ªôc v√†o nhi·ªÅu y·∫øu t·ªë kh√°c n·ªØa, th·ªÉ t√≠ch ch·∫≥ng h·∫°n</em>). V√¨ blog n√†y n√≥i v·ªÅ c√°c thu·∫≠t to√°n Machine Learning ƒë∆°n gi·∫£n n√™n t√¥i s·∫Ω gi·∫£ s·ª≠ r·∫±ng ch√∫ng ta c√≥ th·ªÉ d·ª± ƒëo√°n ƒë∆∞·ª£c.</p>

<p>Ch√∫ng ta c√≥ th·ªÉ th·∫•y l√† c√¢n n·∫∑ng s·∫Ω t·ªâ l·ªá thu·∫≠n v·ªõi chi·ªÅu cao (c√†ng cao c√†ng n·∫∑ng), n√™n c√≥ th·ªÉ s·ª≠ d·ª•ng Linear Regression model cho vi·ªác d·ª± ƒëo√°n n√†y. ƒê·ªÉ ki·ªÉm tra ƒë·ªô ch√≠nh x√°c c·ªßa model t√¨m ƒë∆∞·ª£c, ch√∫ng ta s·∫Ω gi·ªØ l·∫°i c·ªôt 155 v√† 160 cm ƒë·ªÉ ki·ªÉm th·ª≠, c√°c c·ªôt c√≤n l·∫°i ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán (train) model.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-hien-thi-du-lieu-tren-do-thi"></a></p>

<h3 id="32-hi·ªÉn-th·ªã-d·ªØ-li·ªáu-tr√™n-ƒë·ªì-th·ªã">3.2. Hi·ªÉn th·ªã d·ªØ li·ªáu tr√™n ƒë·ªì th·ªã</h3>
<p>Tr∆∞·ªõc ti√™n, ch√∫ng ta c·∫ßn c√≥ hai th∆∞ vi·ªán <a href="http://www.numpy.org/">numpy</a> cho ƒë·∫°i s·ªë tuy·∫øn t√≠nh v√† <a href="http://matplotlib.org/">matplotlib</a> cho vi·ªác v·∫Ω h√¨nh.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To support both python 2 and python 3
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>Ti·∫øp theo, ch√∫ng ta khai b√°o v√† bi·ªÉu di·ªÖn d·ªØ li·ªáu tr√™n m·ªôt ƒë·ªì th·ªã.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># height (cm)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">147</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">153</span><span class="p">,</span> <span class="mi">158</span><span class="p">,</span> <span class="mi">163</span><span class="p">,</span> <span class="mi">165</span><span class="p">,</span> <span class="mi">168</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">173</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">178</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">183</span><span class="p">]]).</span><span class="n">T</span>
<span class="c1"># weight (kg)
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span>  <span class="mi">54</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">68</span><span class="p">]]).</span><span class="n">T</span>
<span class="c1"># Visualize data 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">140</span><span class="p">,</span> <span class="mi">190</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">75</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Height (cm)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Weight (kg)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<!-- ![png](/assets/LR/output_3_0.png) -->
<div class="imgcap">
<img src="/assets/LR/output_3_0.png" align="center" />
</div>

<p>T·ª´ ƒë·ªì th·ªã n√†y ta th·∫•y r·∫±ng d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp g·∫ßn nh∆∞ theo 1 ƒë∆∞·ªùng th·∫≥ng, v·∫≠y m√¥ h√¨nh Linear Regression nhi·ªÅu kh·∫£ nƒÉng s·∫Ω cho k·∫øt qu·∫£ t·ªët:</p>

<p>(c√¢n n·∫∑ng) = <code class="language-plaintext highlighter-rouge">w_1</code>*(chi·ªÅu cao) + <code class="language-plaintext highlighter-rouge">w_0</code></p>

<!-- ========================== New Heading ==================== -->
<p><a name="-nghiem-theo-cong-thuc"></a></p>

<h3 id="33-nghi·ªám-theo-c√¥ng-th·ª©c">3.3. Nghi·ªám theo c√¥ng th·ª©c</h3>

<p>Ti·∫øp theo, ch√∫ng ta s·∫Ω t√≠nh to√°n c√°c h·ªá s·ªë <code class="language-plaintext highlighter-rouge">w_1</code> v√† <code class="language-plaintext highlighter-rouge">w_0</code> d·ª±a v√†o c√¥ng th·ª©c \((5)\). Ch√∫ √Ω: gi·∫£ ngh·ªãch ƒë·∫£o c·ªßa m·ªôt ma tr·∫≠n <code class="language-plaintext highlighter-rouge">A</code> trong Python s·∫Ω ƒë∆∞·ª£c t√≠nh b·∫±ng <code class="language-plaintext highlighter-rouge">numpy.linalg.pinv(A)</code>, <code class="language-plaintext highlighter-rouge">pinv</code> l√† t·ª´ vi·∫øt t·∫Øt c·ªßa <em>pseudo inverse</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Building Xbar 
</span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">Xbar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">one</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Calculating weights of the fitting line 
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xbar</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Xbar</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xbar</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'w = '</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="c1"># Preparing the fitting line 
</span><span class="n">w_0</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w_1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">145</span><span class="p">,</span> <span class="mi">185</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">w_0</span> <span class="o">+</span> <span class="n">w_1</span><span class="o">*</span><span class="n">x0</span>

<span class="c1"># Drawing the fitting line 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">)</span>     <span class="c1"># data 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span>               <span class="c1"># the fitting line
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">140</span><span class="p">,</span> <span class="mi">190</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">75</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Height (cm)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Weight (kg)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w =  [[-33.73541021]
 [  0.55920496]]
</code></pre></div></div>

<!-- ![pnet/asset/LR/output_5_1.png) -->
<div class="imgcap">
<img src="/assets/LR/output_5_1.png" align="center" />
</div>

<p>T·ª´ ƒë·ªì th·ªã b√™n tr√™n ta th·∫•y r·∫±ng c√°c ƒëi·ªÉm d·ªØ li·ªáu m√†u ƒë·ªè n·∫±m kh√° g·∫ßn ƒë∆∞·ªùng th·∫≥ng d·ª± ƒëo√°n m√†u xanh. V·∫≠y m√¥ h√¨nh Linear Regression ho·∫°t ƒë·ªông t·ªët v·ªõi t·∫≠p d·ªØ li·ªáu <em>training</em>. B√¢y gi·ªù, ch√∫ng ta s·ª≠ d·ª•ng m√¥ h√¨nh n√†y ƒë·ªÉ d·ª± ƒëo√°n c√¢n n·∫∑ng c·ªßa hai ng∆∞·ªùi c√≥ chi·ªÅu cao 155 v√† 160 cm m√† ch√∫ng ta ƒë√£ kh√¥ng d√πng khi t√≠nh to√°n nghi·ªám.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y1</span> <span class="o">=</span> <span class="n">w_1</span><span class="o">*</span><span class="mi">155</span> <span class="o">+</span> <span class="n">w_0</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">w_1</span><span class="o">*</span><span class="mi">160</span> <span class="o">+</span> <span class="n">w_0</span>

<span class="k">print</span><span class="p">(</span> <span class="sa">u</span><span class="s">'Predict weight of person with height 155 cm: %.2f (kg), real number: 52 (kg)'</span>  <span class="o">%</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span> <span class="p">)</span>
<span class="k">print</span><span class="p">(</span> <span class="sa">u</span><span class="s">'Predict weight of person with height 160 cm: %.2f (kg), real number: 56 (kg)'</span>  <span class="o">%</span><span class="p">(</span><span class="n">y2</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predict weight of person with height 155 cm: 52.94 (kg), real number: 52 (kg)
Predict weight of person with height 160 cm: 55.74 (kg), real number: 56 (kg)
</code></pre></div></div>

<p>Ch√∫ng ta th·∫•y r·∫±ng k·∫øt qu·∫£ d·ª± ƒëo√°n kh√° g·∫ßn v·ªõi s·ªë li·ªáu th·ª±c t·∫ø.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-nghiem-theo-thu-vien-scikit-learn"></a></p>

<h3 id="34-nghi·ªám-theo-th∆∞-vi·ªán-scikit-learn">3.4. Nghi·ªám theo th∆∞ vi·ªán scikit-learn</h3>

<p>Ti·∫øp theo, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng th∆∞ vi·ªán scikit-learn c·ªßa Python ƒë·ªÉ t√¨m nghi·ªám.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">linear_model</span>

<span class="c1"># fit the model by Linear Regression
</span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># fit_intercept = False for calculating the bias
</span><span class="n">regr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xbar</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Compare two results
</span><span class="k">print</span><span class="p">(</span> <span class="s">'Solution found by scikit-learn  : '</span><span class="p">,</span> <span class="n">regr</span><span class="p">.</span><span class="n">coef_</span> <span class="p">)</span>
<span class="k">print</span><span class="p">(</span> <span class="s">'Solution found by (5): '</span><span class="p">,</span> <span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Solution found by scikit-learn  :  [[  -33.73541021 0.55920496]]
    Solution found by (5):  [[  -33.73541021 0.55920496 ]]
</code></pre></div></div>

<p>Ch√∫ng ta th·∫•y r·∫±ng hai k·∫øt qu·∫£ thu ƒë∆∞·ª£c nh∆∞ nhau! (<em>Nghƒ©a l√† t√¥i ƒë√£ kh√¥ng m·∫Øc l·ªói n√†o trong c√°ch t√¨m nghi·ªám ·ªü ph·∫ßn tr√™n</em>)</p>

<p><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/LR/LR.ipynb">Source code Jupyter Notebook cho b√†i n√†y.</a></p>

<!-- ========================== New Heading ==================== -->
<p><a name="-thao-luan"></a></p>

<h2 id="4-th·∫£o-lu·∫≠n">4. Th·∫£o lu·∫≠n</h2>

<!-- ========================== New Heading ==================== -->
<p><a name="-cac-bai-toan-co-the-giai-bang-linear-regression"></a></p>

<h3 id="41-c√°c-b√†i-to√°n-c√≥-th·ªÉ-gi·∫£i-b·∫±ng-linear-regression">4.1. C√°c b√†i to√°n c√≥ th·ªÉ gi·∫£i b·∫±ng Linear Regression</h3>
<p>H√†m s·ªë \(y \approx f(\mathbf{x})= \mathbf{w}^T\mathbf{x}\) l√† m·ªôt h√†m tuy·∫øn t√≠nh theo c·∫£ \( \mathbf{w}\) v√† \(\mathbf{x}\). Tr√™n th·ª±c t·∫ø, Linear Regression c√≥ th·ªÉ √°p d·ª•ng cho c√°c m√¥ h√¨nh ch·ªâ c·∫ßn tuy·∫øn t√≠nh theo \(\mathbf{w}\). V√≠ d·ª•:
\[
y \approx w_1 x_1 + w_2 x_2 + w_3 x_1^2 + 
\]
\[
+w_4 \sin(x_2) + w_5 x_1x_2 + w_0
\]
l√† m·ªôt h√†m tuy·∫øn t√≠nh theo \(\mathbf{w}\) v√† v√¨ v·∫≠y c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i b·∫±ng Linear Regression. V·ªõi m·ªói d·ªØ li·ªáu ƒë·∫ßu v√†o \(\mathbf{x}=[x_1; x_2] \), ch√∫ng ta t√≠nh to√°n d·ªØ li·ªáu m·ªõi \(\tilde{\mathbf{x}} = [x_1, x_2, x_1^2, \sin(x_2), x_1x_2]\) (ƒë·ªçc l√† <em>x tilde</em> trong ti·∫øng Anh) r·ªìi √°p d·ª•ng Linear Regression v·ªõi d·ªØ li·ªáu m·ªõi n√†y.</p>

<p>Xem th√™m v√≠ d·ª• v·ªÅ <a href="http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression">Quadratic Regression</a> (H·ªìi Quy B·∫≠c Hai).</p>
<div class="imgcap">
<div>
    <img src="http://www.varsitytutors.com/assets/vt-hotmath-legacy/hotmath_help/topics/quadratic-regression/f-qr-1-1.gif" width="300" />&lt;/a&gt;
</div>
<div class="thecap"> Quadratic Regression (Ngu·ªìn: <a href="http://www.varsitytutors.com/hotmath/hotmath_help/topics/quadratic-regression"> Quadratic Regression</a>) <br /></div>
</div>

<!-- ========================== New Heading ==================== -->
<p><a name="-han-che-cua-linear-regression"></a></p>

<h3 id="42-h·∫°n-ch·∫ø-c·ªßa-linear-regression">4.2. H·∫°n ch·∫ø c·ªßa Linear Regression</h3>

<p>H·∫°n ch·∫ø ƒë·∫ßu ti√™n c·ªßa Linear Regression l√† n√≥ r·∫•t <strong>nh·∫°y c·∫£m v·ªõi nhi·ªÖu</strong> (sensitive to noise). Trong v√≠ d·ª• v·ªÅ m·ªëi quan h·ªá gi·ªØa chi·ªÅu cao v√† c√¢n n·∫∑ng b√™n tr√™n, n·∫øu c√≥ ch·ªâ
m·ªôt c·∫∑p d·ªØ li·ªáu <em>nhi·ªÖu</em> (150 cm, 90kg) th√¨ k·∫øt qu·∫£ s·∫Ω sai kh√°c ƒëi r·∫•t nhi·ªÅu. Xem h√¨nh d∆∞·ªõi ƒë√¢y:</p>
<div class="imgcap">
<img src="/assets/LR/output_13_1.png" align="center" />
</div>

<p>V√¨ v·∫≠y, tr∆∞·ªõc khi th·ª±c hi·ªán Linear Regression, c√°c nhi·ªÖu (<em>outlier</em>) c·∫ßn
 ph·∫£i ƒë∆∞·ª£c lo·∫°i b·ªè. B∆∞·ªõc n√†y ƒë∆∞·ª£c g·ªçi l√† ti·ªÅn x·ª≠ l√Ω (pre-processing).</p>

<p>H·∫°n ch·∫ø th·ª© hai c·ªßa Linear Regression l√† n√≥ <strong>kh√¥ng bi·ªÖu di·ªÖn ƒë∆∞·ª£c c√°c m√¥ h√¨nh ph·ª©c t·∫°p</strong>. M·∫∑c d√π trong ph·∫ßn tr√™n, ch√∫ng ta th·∫•y r·∫±ng ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ ƒë∆∞·ª£c √°p d·ª•ng n·∫øu quan h·ªá gi·ªØa <em>outcome</em> v√† <em>input</em> kh√¥ng nh·∫•t thi·∫øt ph·∫£i l√† tuy·∫øn t√≠nh, nh∆∞ng m·ªëi quan h·ªá n√†y v·∫´n ƒë∆°n gi·∫£n nhi·ªÅu so v·ªõi c√°c m√¥ h√¨nh th·ª±c t·∫ø. H∆°n n·ªØa, ch√∫ng ta s·∫Ω t·ª± h·ªèi: l√†m th·∫ø n√†o ƒë·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c h√†m \(x_1^2, \sin(x_2), x_1x_2\) nh∆∞ ·ªü tr√™n?!</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-cac-phuong-phap-toi-uu"></a></p>

<h3 id="43-c√°c-ph∆∞∆°ng-ph√°p-t·ªëi-∆∞u">4.3. C√°c ph∆∞∆°ng ph√°p t·ªëi ∆∞u</h3>
<p>Linear Regression l√† m·ªôt m√¥ h√¨nh ƒë∆°n gi·∫£n, l·ªùi gi·∫£i cho ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m b·∫±ng 0 c≈©ng kh√° ƒë∆°n gi·∫£n. <em>Trong h·∫ßu h·∫øt c√°c tr∆∞·ªùng h·ª£p, ch√∫ng ta kh√¥ng th·ªÉ gi·∫£i ƒë∆∞·ª£c ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m b·∫±ng 0.</em></p>

<p>Nh∆∞ng c√≥ m·ªôt ƒëi·ªÅu ch√∫ng ta n√™n nh·ªõ, <strong>c√≤n t√≠nh ƒë∆∞·ª£c ƒë·∫°o h√†m l√† c√≤n c√≥ hy v·ªçng</strong>.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="5-t√†i-li·ªáu-tham-kh·∫£o">5. T√†i li·ªáu tham kh·∫£o</h2>

<ol>
  <li><a href="https://en.wikipedia.org/wiki/Linear_regression">Linear Regression - Wikipedia</a></li>
  <li><a href="http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/">Simple Linear Regression Tutorial for Machine Learning</a></li>
  <li><a href="http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf">Least Squares, Pseudo-Inverses, PCA &amp; SVD</a></li>
</ol>

:ET